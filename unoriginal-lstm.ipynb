{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not original at all!!\n",
    "Data Munging: https://www.kaggle.com/mayer79/rnn-starter-for-huge-time-series (Please Upvote!)\n",
    "LSTM: https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru (Please Upvote!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RNN is based on 24 features\n"
     ]
    }
   ],
   "source": [
    "# BASIC IDEA OF THE KERNEL\n",
    "\n",
    "# The data consists of a one dimensional time series x with 600 Mio data points. \n",
    "# At test time, we will see a time series of length 150'000 to predict the next earthquake.\n",
    "# The idea of this kernel is to randomly sample chunks of length 150'000 from x, derive some\n",
    "# features and use them to update weights of a recurrent neural net with 150'000 / 1000 = 150\n",
    "# time steps. \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, TimeDistributed, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "# Import\n",
    "float_data = pd.read_csv(\"../input/train/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values\n",
    "\n",
    "# Helper function for the data generator. Extracts mean, standard deviation, and quantiles per time step.\n",
    "# Can easily be extended. Expects a two dimensional array.\n",
    "def extract_features(z):\n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  #np.median(z),\n",
    "                  #np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n",
    "                  np.sqrt(np.abs(z)).mean(axis=1),\n",
    "                  np.sqrt(np.abs(z)).std(axis=1)\n",
    "                 ]\n",
    "\n",
    "# For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# From each piece, a set features are extracted. This results in a feature matrix \n",
    "# of dimension (150 time steps x features).  \n",
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:]),\n",
    "                 extract_features(temp[:, -step_length // 300:])]\n",
    "\n",
    "# Query \"create_X\" to figure out the number of features\n",
    "n_features = create_X(float_data[0:150000]).shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)\n",
    "    \n",
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets\n",
    "        \n",
    "batch_size = 32\n",
    "\n",
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "float_data[second_earthquake, 1]\n",
    "\n",
    "# Initialize generators\n",
    "train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "# train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "\n",
    "# The LSTM architecture\n",
    "model = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=(None,n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "model.add(CuDNNLSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "# The output layer\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile and fit model\n",
    "model.compile(optimizer='rmsprop', loss='mae')\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=40,\n",
    "                              verbose=2,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200\n",
    "                             )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies simply mean max min std 32 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies  simply mean max min std and sqrt mean 32 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies  simply mean max min std 40 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies simply mean max min std and sqrt mean 40 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies simply mean max min std and sqrt std 40 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies simply mean max min std and sqrt mean 80 epoch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "perf_plot(history) # Extra Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission file\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "x = None\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values[:]\n",
    "    #print(x.shape)\n",
    "    submission.time_to_failure[i] = (model.predict(np.expand_dims(create_X(x), 0)))\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission-ltsm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df = pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original features \n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1)]\n",
    "Epoch 1/32\n",
    " - 66s - loss: 2.3928 - val_loss: 1.9370\n",
    "Epoch 2/32\n",
    " - 63s - loss: 2.2244 - val_loss: 1.7033\n",
    "Epoch 3/32\n",
    " - 62s - loss: 2.1842 - val_loss: 1.9598\n",
    "Epoch 4/32\n",
    " - 62s - loss: 2.1599 - val_loss: 1.9348\n",
    "Epoch 5/32\n",
    " - 64s - loss: 2.1407 - val_loss: 1.7172\n",
    "Epoch 6/32\n",
    " - 64s - loss: 2.1230 - val_loss: 1.7050\n",
    "Epoch 7/32\n",
    " - 63s - loss: 2.1201 - val_loss: 1.9158\n",
    "Epoch 8/32\n",
    " - 62s - loss: 2.1078 - val_loss: 1.7558\n",
    "Epoch 9/32\n",
    " - 63s - loss: 2.1064 - val_loss: 1.7702\n",
    "Epoch 10/32\n",
    " - 63s - loss: 2.1122 - val_loss: 1.6435\n",
    "Epoch 11/32\n",
    " - 63s - loss: 2.0944 - val_loss: 1.6754\n",
    "Epoch 12/32\n",
    " - 63s - loss: 2.0927 - val_loss: 1.6267\n",
    "Epoch 13/32\n",
    " - 63s - loss: 2.0813 - val_loss: 1.7408\n",
    "Epoch 14/32\n",
    " - 63s - loss: 2.0837 - val_loss: 1.8019\n",
    "Epoch 15/32\n",
    " - 64s - loss: 2.0842 - val_loss: 1.6420\n",
    "Epoch 16/32\n",
    " - 63s - loss: 2.0797 - val_loss: 1.7668\n",
    "Epoch 17/32\n",
    " - 64s - loss: 2.0844 - val_loss: 1.7888\n",
    "Epoch 18/32\n",
    " - 64s - loss: 2.0684 - val_loss: 1.7200\n",
    "Epoch 19/32\n",
    " - 65s - loss: 2.0914 - val_loss: 1.8434\n",
    "Epoch 20/32\n",
    " - 64s - loss: 2.0600 - val_loss: 1.9241\n",
    "Epoch 21/32\n",
    " - 64s - loss: 2.0642 - val_loss: 1.7292\n",
    "Epoch 22/32\n",
    " - 63s - loss: 2.0589 - val_loss: 1.8935\n",
    "Epoch 23/32\n",
    " - 61s - loss: 2.0733 - val_loss: 1.7342\n",
    "Epoch 24/32\n",
    " - 63s - loss: 2.0645 - val_loss: 1.5960\n",
    "Epoch 25/32\n",
    " - 63s - loss: 2.0541 - val_loss: 1.7033\n",
    "Epoch 26/32\n",
    " - 64s - loss: 2.0526 - val_loss: 1.6880\n",
    "Epoch 27/32\n",
    " - 64s - loss: 2.0662 - val_loss: 1.7038\n",
    "Epoch 28/32\n",
    " - 63s - loss: 2.0614 - val_loss: 1.7706\n",
    "Epoch 29/32\n",
    " - 63s - loss: 2.0617 - val_loss: 1.6943\n",
    "Epoch 30/32\n",
    " - 61s - loss: 2.0569 - val_loss: 1.7090\n",
    "Epoch 31/32\n",
    " - 62s - loss: 2.0566 - val_loss: 1.6816\n",
    "Epoch 32/32\n",
    " - 62s - loss: 2.0511 - val_loss: 1.6311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  np.sqrt(np.abs(z)).mean(axis=1)\n",
    "\n",
    "Epoch 1/32\n",
    " - 154s - loss: 2.3595 - val_loss: 1.9728\n",
    "Epoch 2/32\n",
    " - 157s - loss: 2.2042 - val_loss: 1.9447\n",
    "Epoch 3/32\n",
    " - 158s - loss: 2.1524 - val_loss: 1.7406\n",
    "Epoch 4/32\n",
    " - 159s - loss: 2.1361 - val_loss: 1.5513\n",
    "Epoch 5/32\n",
    " - 157s - loss: 2.1304 - val_loss: 1.5908\n",
    "Epoch 6/32\n",
    " - 153s - loss: 2.1078 - val_loss: 1.6882\n",
    "Epoch 7/32\n",
    " - 153s - loss: 2.0924 - val_loss: 1.6414\n",
    "Epoch 8/32\n",
    " - 152s - loss: 2.0977 - val_loss: 1.6917\n",
    "Epoch 9/32\n",
    " - 152s - loss: 2.0910 - val_loss: 1.9564\n",
    "Epoch 10/32\n",
    " - 152s - loss: 2.0816 - val_loss: 1.7140\n",
    "Epoch 11/32\n",
    " - 153s - loss: 2.0708 - val_loss: 1.7071\n",
    "Epoch 12/32\n",
    " - 154s - loss: 2.0761 - val_loss: 1.8837\n",
    "Epoch 13/32\n",
    " - 159s - loss: 2.0632 - val_loss: 1.7577\n",
    "Epoch 14/32\n",
    " - 157s - loss: 2.0771 - val_loss: 1.6824\n",
    "Epoch 15/32\n",
    " - 158s - loss: 2.0725 - val_loss: 1.6208\n",
    "Epoch 16/32\n",
    " - 158s - loss: 2.0668 - val_loss: 1.5576\n",
    "Epoch 17/32\n",
    " - 157s - loss: 2.0586 - val_loss: 1.8591\n",
    "Epoch 18/32\n",
    " - 158s - loss: 2.0547 - val_loss: 1.8409\n",
    "Epoch 19/32\n",
    " - 154s - loss: 2.0550 - val_loss: 1.4596\n",
    "Epoch 20/32\n",
    " - 157s - loss: 2.0596 - val_loss: 1.5710\n",
    "Epoch 21/32\n",
    " - 152s - loss: 2.0535 - val_loss: 1.8322\n",
    "Epoch 22/32\n",
    " - 152s - loss: 2.0464 - val_loss: 1.7149\n",
    "Epoch 23/32\n",
    " - 156s - loss: 2.0430 - val_loss: 1.5419\n",
    "Epoch 24/32\n",
    " - 155s - loss: 2.0358 - val_loss: 1.6811\n",
    "Epoch 25/32\n",
    " - 157s - loss: 2.0327 - val_loss: 1.7708\n",
    "Epoch 26/32\n",
    " - 158s - loss: 2.0374 - val_loss: 1.7572\n",
    "Epoch 27/32\n",
    " - 153s - loss: 2.0326 - val_loss: 1.5652\n",
    "Epoch 28/32\n",
    " - 153s - loss: 2.0308 - val_loss: 1.5978\n",
    "Epoch 29/32\n",
    " - 155s - loss: 2.0450 - val_loss: 1.7402\n",
    "Epoch 30/32\n",
    " - 156s - loss: 2.0325 - val_loss: 1.7172\n",
    "Epoch 31/32\n",
    " - 154s - loss: 2.0342 - val_loss: 1.6428\n",
    "Epoch 32/32\n",
    " - 154s - loss: 2.0261 - val_loss: 1.6561\n",
    "CPU times: user 1h 49min 6s, sys: 3min, total: 1h 52min 7s\n",
    "Wall time: 1h 22min 49s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  #np.sqrt(np.abs(z)).mean(axis=1)\n",
    "Epoch 1/40\n",
    " - 65s - loss: 2.3928 - val_loss: 1.9375\n",
    "Epoch 2/40\n",
    " - 64s - loss: 2.2232 - val_loss: 1.6909\n",
    "Epoch 3/40\n",
    " - 64s - loss: 2.1922 - val_loss: 2.1060\n",
    "Epoch 4/40\n",
    " - 64s - loss: 2.1631 - val_loss: 1.9303\n",
    "Epoch 5/40\n",
    " - 64s - loss: 2.1422 - val_loss: 1.7295\n",
    "Epoch 6/40\n",
    " - 64s - loss: 2.1202 - val_loss: 1.6751\n",
    "Epoch 7/40\n",
    " - 64s - loss: 2.1185 - val_loss: 1.7106\n",
    "Epoch 8/40\n",
    " - 64s - loss: 2.1116 - val_loss: 1.9057\n",
    "Epoch 9/40\n",
    " - 64s - loss: 2.1101 - val_loss: 1.5614\n",
    "Epoch 10/40\n",
    " - 64s - loss: 2.1172 - val_loss: 1.6790\n",
    "Epoch 11/40\n",
    " - 64s - loss: 2.0888 - val_loss: 1.6710\n",
    "Epoch 12/40\n",
    " - 65s - loss: 2.0919 - val_loss: 1.5902\n",
    "Epoch 13/40\n",
    " - 64s - loss: 2.0823 - val_loss: 1.6984\n",
    "Epoch 14/40\n",
    " - 65s - loss: 2.0856 - val_loss: 1.7742\n",
    "Epoch 15/40\n",
    " - 64s - loss: 2.0857 - val_loss: 1.7923\n",
    "Epoch 16/40\n",
    " - 64s - loss: 2.0792 - val_loss: 1.7989\n",
    "Epoch 17/40\n",
    " - 63s - loss: 2.0865 - val_loss: 1.8651\n",
    "Epoch 18/40\n",
    " - 62s - loss: 2.0748 - val_loss: 1.7109\n",
    "Epoch 19/40\n",
    " - 63s - loss: 2.0905 - val_loss: 1.7494\n",
    "Epoch 20/40\n",
    " - 63s - loss: 2.0589 - val_loss: 1.7050\n",
    "Epoch 21/40\n",
    " - 62s - loss: 2.0642 - val_loss: 1.5661\n",
    "Epoch 22/40\n",
    " - 62s - loss: 2.0519 - val_loss: 1.7448\n",
    "Epoch 23/40\n",
    " - 62s - loss: 2.0674 - val_loss: 1.7525\n",
    "Epoch 24/40\n",
    " - 62s - loss: 2.0720 - val_loss: 1.6188\n",
    "Epoch 25/40\n",
    " - 62s - loss: 2.0571 - val_loss: 1.8109\n",
    "Epoch 26/40\n",
    " - 61s - loss: 2.0515 - val_loss: 1.6783\n",
    "Epoch 27/40\n",
    " - 62s - loss: 2.0624 - val_loss: 1.8582\n",
    "Epoch 28/40\n",
    " - 63s - loss: 2.0618 - val_loss: 1.7183\n",
    "Epoch 29/40\n",
    " - 64s - loss: 2.0642 - val_loss: 1.7743\n",
    "Epoch 30/40\n",
    " - 63s - loss: 2.0533 - val_loss: 1.7152\n",
    "Epoch 31/40\n",
    " - 62s - loss: 2.0643 - val_loss: 1.8029\n",
    "Epoch 32/40\n",
    " - 62s - loss: 2.0507 - val_loss: 1.5906\n",
    "Epoch 33/40\n",
    " - 62s - loss: 2.0491 - val_loss: 1.7614\n",
    "Epoch 34/40\n",
    " - 62s - loss: 2.0578 - val_loss: 1.5521\n",
    "Epoch 35/40\n",
    " - 61s - loss: 2.0408 - val_loss: 1.7914\n",
    "Epoch 36/40\n",
    " - 64s - loss: 2.0512 - val_loss: 1.7751\n",
    "Epoch 37/40\n",
    " - 63s - loss: 2.0403 - val_loss: 1.6504\n",
    "Epoch 38/40\n",
    " - 64s - loss: 2.0427 - val_loss: 1.8024\n",
    "Epoch 39/40\n",
    " - 63s - loss: 2.0462 - val_loss: 1.6763\n",
    "Epoch 40/40\n",
    " - 63s - loss: 2.0490 - val_loss: 1.6260\n",
    "CPU times: user 1h 11min 48s, sys: 3min 23s, total: 1h 15min 11s\n",
    "Wall time: 42min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  np.sqrt(np.abs(z)).mean(axis=1)\n",
    "\n",
    "Epoch 1/40\n",
    " - 81s - loss: 2.3920 - val_loss: 2.0196\n",
    "Epoch 2/40\n",
    " - 80s - loss: 2.2248 - val_loss: 1.7401\n",
    "Epoch 3/40\n",
    " - 78s - loss: 2.1981 - val_loss: 1.9153\n",
    "Epoch 4/40\n",
    " - 78s - loss: 2.1820 - val_loss: 1.8003\n",
    "Epoch 5/40\n",
    " - 79s - loss: 2.1465 - val_loss: 1.6427\n",
    "Epoch 6/40\n",
    " - 78s - loss: 2.1354 - val_loss: 1.7191\n",
    "Epoch 7/40\n",
    " - 78s - loss: 2.1312 - val_loss: 1.8521\n",
    "Epoch 8/40\n",
    " - 78s - loss: 2.1288 - val_loss: 1.6283\n",
    "Epoch 9/40\n",
    " - 78s - loss: 2.1173 - val_loss: 1.7793\n",
    "Epoch 10/40\n",
    " - 78s - loss: 2.1200 - val_loss: 1.7659\n",
    "Epoch 11/40\n",
    " - 82s - loss: 2.1033 - val_loss: 1.6827\n",
    "Epoch 12/40\n",
    " - 81s - loss: 2.0991 - val_loss: 1.7370\n",
    "Epoch 13/40\n",
    " - 80s - loss: 2.0918 - val_loss: 1.7541\n",
    "Epoch 14/40\n",
    " - 80s - loss: 2.0945 - val_loss: 1.7652\n",
    "Epoch 15/40\n",
    " - 82s - loss: 2.0921 - val_loss: 1.5560\n",
    "Epoch 16/40\n",
    " - 82s - loss: 2.0881 - val_loss: 1.7586\n",
    "Epoch 17/40\n",
    " - 82s - loss: 2.0883 - val_loss: 1.6854\n",
    "Epoch 18/40\n",
    " - 82s - loss: 2.0814 - val_loss: 1.7114\n",
    "Epoch 19/40\n",
    " - 82s - loss: 2.0999 - val_loss: 1.8670\n",
    "Epoch 20/40\n",
    " - 82s - loss: 2.0672 - val_loss: 1.7266\n",
    "Epoch 21/40\n",
    " - 82s - loss: 2.0704 - val_loss: 1.8502\n",
    "Epoch 22/40\n",
    " - 82s - loss: 2.0609 - val_loss: 2.0490\n",
    "Epoch 23/40\n",
    " - 82s - loss: 2.0799 - val_loss: 1.7238\n",
    "Epoch 24/40\n",
    " - 82s - loss: 2.0702 - val_loss: 1.8085\n",
    "Epoch 25/40\n",
    " - 82s - loss: 2.0608 - val_loss: 1.6307\n",
    "Epoch 26/40\n",
    " - 82s - loss: 2.0554 - val_loss: 1.7432\n",
    "Epoch 27/40\n",
    " - 82s - loss: 2.0680 - val_loss: 1.7395\n",
    "Epoch 28/40\n",
    " - 82s - loss: 2.0645 - val_loss: 1.7326\n",
    "Epoch 29/40\n",
    " - 83s - loss: 2.0655 - val_loss: 1.8699\n",
    "Epoch 30/40\n",
    " - 83s - loss: 2.0550 - val_loss: 1.7695\n",
    "Epoch 31/40\n",
    " - 82s - loss: 2.0706 - val_loss: 1.7876\n",
    "Epoch 32/40\n",
    " - 82s - loss: 2.0552 - val_loss: 1.5393\n",
    "Epoch 33/40\n",
    " - 83s - loss: 2.0514 - val_loss: 1.7154\n",
    "Epoch 34/40\n",
    " - 82s - loss: 2.0528 - val_loss: 1.5252\n",
    "Epoch 35/40\n",
    " - 82s - loss: 2.0446 - val_loss: 1.7102\n",
    "Epoch 36/40\n",
    " - 82s - loss: 2.0560 - val_loss: 1.6044\n",
    "Epoch 37/40\n",
    " - 82s - loss: 2.0458 - val_loss: 1.7797\n",
    "Epoch 38/40\n",
    " - 82s - loss: 2.0429 - val_loss: 1.8065\n",
    "Epoch 39/40\n",
    " - 82s - loss: 2.0535 - val_loss: 1.6717\n",
    "Epoch 40/40\n",
    " - 82s - loss: 2.0459 - val_loss: 1.5742\n",
    "CPU times: user 1h 26min 2s, sys: 3min 31s, total: 1h 29min 33s\n",
    "Wall time: 54min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  #np.sqrt(np.abs(z)).mean(axis=1),\n",
    "                  np.sqrt(np.abs(z)).std(axis=1)\n",
    "                 ]\n",
    "Epoch 1/40\n",
    " - 93s - loss: 2.4005 - val_loss: 1.9946\n",
    "Epoch 2/40\n",
    " - 93s - loss: 2.2334 - val_loss: 1.8570\n",
    "Epoch 3/40\n",
    " - 92s - loss: 2.2080 - val_loss: 1.7356\n",
    "Epoch 4/40\n",
    " - 93s - loss: 2.1833 - val_loss: 1.7988\n",
    "Epoch 5/40\n",
    " - 92s - loss: 2.1430 - val_loss: 1.6780\n",
    "Epoch 6/40\n",
    " - 92s - loss: 2.1329 - val_loss: 1.6404\n",
    "Epoch 7/40\n",
    " - 91s - loss: 2.1278 - val_loss: 1.8292\n",
    "Epoch 8/40\n",
    " - 92s - loss: 2.1261 - val_loss: 1.6718\n",
    "Epoch 9/40\n",
    " - 92s - loss: 2.1168 - val_loss: 1.8089\n",
    "Epoch 10/40\n",
    " - 93s - loss: 2.1255 - val_loss: 1.8600\n",
    "Epoch 11/40\n",
    " - 93s - loss: 2.0972 - val_loss: 1.6522\n",
    "Epoch 12/40\n",
    " - 92s - loss: 2.0992 - val_loss: 1.6430\n",
    "Epoch 13/40\n",
    " - 92s - loss: 2.0946 - val_loss: 1.7748\n",
    "Epoch 14/40\n",
    " - 92s - loss: 2.0960 - val_loss: 1.7877\n",
    "Epoch 15/40\n",
    " - 93s - loss: 2.0943 - val_loss: 1.7881\n",
    "Epoch 16/40\n",
    " - 92s - loss: 2.0889 - val_loss: 1.7832\n",
    "Epoch 17/40\n",
    " - 92s - loss: 2.0959 - val_loss: 1.7639\n",
    "Epoch 18/40\n",
    " - 93s - loss: 2.0829 - val_loss: 1.6759\n",
    "Epoch 19/40\n",
    " - 92s - loss: 2.1061 - val_loss: 1.7114\n",
    "Epoch 20/40\n",
    " - 94s - loss: 2.0677 - val_loss: 1.7818\n",
    "Epoch 21/40\n",
    " - 95s - loss: 2.0712 - val_loss: 1.6394\n",
    "Epoch 22/40\n",
    " - 93s - loss: 2.0623 - val_loss: 1.7075\n",
    "Epoch 23/40\n",
    " - 93s - loss: 2.0761 - val_loss: 1.8154\n",
    "Epoch 24/40\n",
    " - 93s - loss: 2.0747 - val_loss: 1.5792\n",
    "Epoch 25/40\n",
    " - 93s - loss: 2.0627 - val_loss: 1.7405\n",
    "Epoch 26/40\n",
    " - 94s - loss: 2.0548 - val_loss: 1.6842\n",
    "Epoch 27/40\n",
    " - 95s - loss: 2.0660 - val_loss: 1.8134\n",
    "Epoch 28/40\n",
    " - 95s - loss: 2.0645 - val_loss: 1.7107\n",
    "Epoch 29/40\n",
    " - 94s - loss: 2.0645 - val_loss: 1.7149\n",
    "Epoch 30/40\n",
    " - 93s - loss: 2.0582 - val_loss: 1.6315\n",
    "Epoch 31/40\n",
    " - 92s - loss: 2.0634 - val_loss: 1.7321\n",
    "Epoch 32/40\n",
    " - 92s - loss: 2.0578 - val_loss: 1.6383\n",
    "Epoch 33/40\n",
    " - 93s - loss: 2.0539 - val_loss: 1.7767\n",
    "Epoch 34/40\n",
    " - 91s - loss: 2.0546 - val_loss: 1.5094\n",
    "Epoch 35/40\n",
    " - 94s - loss: 2.0388 - val_loss: 1.7988\n",
    "Epoch 36/40\n",
    " - 95s - loss: 2.0541 - val_loss: 1.7876\n",
    "Epoch 37/40\n",
    " - 93s - loss: 2.0431 - val_loss: 1.6930\n",
    "Epoch 38/40\n",
    " - 95s - loss: 2.0412 - val_loss: 1.8164\n",
    "Epoch 39/40\n",
    " - 92s - loss: 2.0475 - val_loss: 1.6933\n",
    "Epoch 40/40\n",
    " - 93s - loss: 2.0496 - val_loss: 1.6468\n",
    "CPU times: user 1h 33min 4s, sys: 3min 27s, total: 1h 36min 31s\n",
    "Wall time: 1h 1min 59s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1),\n",
    "                  #np.median(z),\n",
    "                  np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n",
    "                  np.sqrt(np.abs(z)).mean(axis=1)\n",
    "                  #np.sqrt(np.abs(z)).std(axis=1)\n",
    "Epoch 1/80\n",
    " - 275s - loss: 2.4420 - val_loss: 2.0530\n",
    "Epoch 2/80\n",
    " - 271s - loss: 2.2485 - val_loss: 1.6975\n",
    "Epoch 3/80\n",
    " - 274s - loss: 2.2377 - val_loss: 1.8995\n",
    "Epoch 4/80\n",
    " - 280s - loss: 2.2032 - val_loss: 1.9955\n",
    "Epoch 5/80\n",
    " - 272s - loss: 2.1795 - val_loss: 1.7917\n",
    "Epoch 6/80\n",
    " - 272s - loss: 2.1598 - val_loss: 1.9248\n",
    "Epoch 7/80\n",
    " - 273s - loss: 2.1371 - val_loss: 1.8859\n",
    "Epoch 8/80\n",
    " - 271s - loss: 2.1348 - val_loss: 1.7393\n",
    "Epoch 9/80\n",
    " - 274s - loss: 2.1232 - val_loss: 1.7991\n",
    "Epoch 10/80\n",
    " - 272s - loss: 2.1259 - val_loss: 1.8371\n",
    "Epoch 11/80\n",
    " - 270s - loss: 2.1127 - val_loss: 1.6605\n",
    "Epoch 12/80\n",
    " - 270s - loss: 2.1033 - val_loss: 1.7167\n",
    "Epoch 13/80\n",
    " - 270s - loss: 2.0987 - val_loss: 1.7000\n",
    "Epoch 14/80\n",
    " - 271s - loss: 2.0965 - val_loss: 1.8395\n",
    "Epoch 15/80\n",
    " - 270s - loss: 2.0941 - val_loss: 1.6249\n",
    "Epoch 16/80\n",
    " - 270s - loss: 2.0866 - val_loss: 1.7929\n",
    "Epoch 17/80\n",
    " - 269s - loss: 2.0915 - val_loss: 1.7493\n",
    "Epoch 18/80\n",
    " - 271s - loss: 2.0837 - val_loss: 1.6512\n",
    "Epoch 19/80\n",
    " - 272s - loss: 2.0933 - val_loss: 1.8019\n",
    "Epoch 20/80\n",
    " - 280s - loss: 2.0643 - val_loss: 1.7801\n",
    "Epoch 21/80\n",
    " - 275s - loss: 2.0724 - val_loss: 1.7586\n",
    "Epoch 22/80\n",
    " - 277s - loss: 2.0600 - val_loss: 1.8950\n",
    "Epoch 23/80\n",
    " - 278s - loss: 2.0789 - val_loss: 1.7047\n",
    "Epoch 24/80\n",
    " - 278s - loss: 2.0738 - val_loss: 1.6130\n",
    "Epoch 25/80\n",
    " - 278s - loss: 2.0614 - val_loss: 1.7254\n",
    "Epoch 26/80\n",
    " - 279s - loss: 2.0568 - val_loss: 1.6731\n",
    "Epoch 27/80\n",
    " - 273s - loss: 2.0701 - val_loss: 1.6532\n",
    "Epoch 28/80\n",
    " - 274s - loss: 2.0681 - val_loss: 1.8125\n",
    "Epoch 29/80\n",
    " - 279s - loss: 2.0643 - val_loss: 1.7123\n",
    "Epoch 30/80\n",
    " - 271s - loss: 2.0588 - val_loss: 1.7218\n",
    "Epoch 31/80\n",
    " - 272s - loss: 2.0610 - val_loss: 1.6583\n",
    "Epoch 32/80\n",
    " - 273s - loss: 2.0515 - val_loss: 1.5537\n",
    "Epoch 33/80\n",
    " - 274s - loss: 2.0492 - val_loss: 1.7274\n",
    "Epoch 34/80\n",
    " - 274s - loss: 2.0550 - val_loss: 1.5911\n",
    "Epoch 35/80\n",
    " - 273s - loss: 2.0342 - val_loss: 1.6914\n",
    "Epoch 36/80\n",
    " - 278s - loss: 2.0532 - val_loss: 1.6069\n",
    "Epoch 37/80\n",
    " - 277s - loss: 2.0480 - val_loss: 1.6758\n",
    "Epoch 38/80\n",
    " - 281s - loss: 2.0436 - val_loss: 1.6805\n",
    "Epoch 39/80\n",
    " - 274s - loss: 2.0514 - val_loss: 1.7445\n",
    "Epoch 40/80\n",
    " - 278s - loss: 2.0446 - val_loss: 1.5711\n",
    "Epoch 41/80\n",
    " - 269s - loss: 2.0459 - val_loss: 1.7405\n",
    "Epoch 42/80\n",
    " - 271s - loss: 2.0530 - val_loss: 1.6143\n",
    "Epoch 43/80\n",
    " - 268s - loss: 2.0355 - val_loss: 1.6658\n",
    "Epoch 44/80\n",
    " - 270s - loss: 2.0475 - val_loss: 1.7299\n",
    "Epoch 45/80\n",
    " - 273s - loss: 2.0311 - val_loss: 1.6779\n",
    "Epoch 46/80\n",
    " - 276s - loss: 2.0267 - val_loss: 1.5801\n",
    "Epoch 47/80\n",
    " - 270s - loss: 2.0130 - val_loss: 1.6804\n",
    "Epoch 48/80\n",
    " - 271s - loss: 2.0446 - val_loss: 1.6024\n",
    "Epoch 49/80\n",
    " - 274s - loss: 2.0306 - val_loss: 1.7541\n",
    "Epoch 50/80\n",
    " - 279s - loss: 2.0236 - val_loss: 1.7320\n",
    "Epoch 51/80\n",
    " - 278s - loss: 2.0386 - val_loss: 1.5982\n",
    "Epoch 52/80\n",
    " - 279s - loss: 2.0289 - val_loss: 1.6217\n",
    "Epoch 53/80\n",
    " - 279s - loss: 2.0206 - val_loss: 1.8180\n",
    "Epoch 54/80\n",
    " - 276s - loss: 2.0273 - val_loss: 1.6369\n",
    "Epoch 55/80\n",
    " - 272s - loss: 2.0260 - val_loss: 1.6120\n",
    "Epoch 56/80\n",
    " - 272s - loss: 2.0200 - val_loss: 1.6388\n",
    "Epoch 57/80\n",
    " - 271s - loss: 2.0403 - val_loss: 1.6474\n",
    "Epoch 58/80\n",
    " - 275s - loss: 2.0137 - val_loss: 1.7147\n",
    "Epoch 59/80\n",
    " - 269s - loss: 2.0213 - val_loss: 1.6805\n",
    "Epoch 60/80\n",
    " - 268s - loss: 2.0259 - val_loss: 1.6119\n",
    "Epoch 61/80\n",
    " - 271s - loss: 2.0210 - val_loss: 1.4685\n",
    "Epoch 62/80\n",
    " - 271s - loss: 2.0194 - val_loss: 1.6699\n",
    "Epoch 63/80\n",
    " - 268s - loss: 2.0120 - val_loss: 1.6508\n",
    "Epoch 64/80\n",
    " - 269s - loss: 2.0140 - val_loss: 1.7159\n",
    "Epoch 65/80\n",
    " - 268s - loss: 2.0157 - val_loss: 1.8277\n",
    "Epoch 66/80\n",
    " - 274s - loss: 2.0018 - val_loss: 1.6347\n",
    "Epoch 67/80\n",
    " - 268s - loss: 2.0126 - val_loss: 1.7237\n",
    "Epoch 68/80\n",
    " - 269s - loss: 1.9904 - val_loss: 1.6448\n",
    "Epoch 69/80\n",
    " - 270s - loss: 1.9926 - val_loss: 1.6572\n",
    "Epoch 70/80\n",
    " - 268s - loss: 2.0086 - val_loss: 1.5817\n",
    "Epoch 71/80\n",
    " - 271s - loss: 1.9957 - val_loss: 1.6470\n",
    "Epoch 72/80\n",
    " - 270s - loss: 1.9928 - val_loss: 1.6598\n",
    "Epoch 73/80\n",
    " - 269s - loss: 2.0029 - val_loss: 1.7102\n",
    "Epoch 74/80\n",
    " - 272s - loss: 1.9949 - val_loss: 1.6547\n",
    "Epoch 75/80\n",
    " - 270s - loss: 1.9831 - val_loss: 1.5767\n",
    "Epoch 76/80\n",
    " - 269s - loss: 1.9996 - val_loss: 1.7222\n",
    "Epoch 77/80\n",
    " - 275s - loss: 1.9796 - val_loss: 1.5692\n",
    "Epoch 78/80\n",
    " - 279s - loss: 1.9908 - val_loss: 1.6256\n",
    "Epoch 79/80\n",
    " - 279s - loss: 1.9838 - val_loss: 1.6479\n",
    "Epoch 80/80\n",
    " - 279s - loss: 1.9672 - val_loss: 1.6011\n",
    "CPU times: user 7h 11min 50s, sys: 7min 10s, total: 7h 19min\n",
    "Wall time: 6h 4min 18s\n",
    "\n",
    "# Visualiz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
