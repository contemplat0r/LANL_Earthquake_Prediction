{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ad951d6352a465a126da5de93e1c5016386c4b4"
   },
   "source": [
    "# Introduction\n",
    "I constructed this Kernel from [MichaelMayer](https://www.kaggle.com/mayer79)'s great [RNN Starter Kernel](https://www.kaggle.com/mayer79/rnn-starter/code) code. Thanks for that! I've been using a similar Kernel with a Notebook but it's got plenty of other stuff in there which makes it more than just bare bones. Since this is not really my work, I figured I would share this Kernel with the community. It's meant to be MichaelMayer's starter code converted to a Notebook and nothing more. I had to make some changes, which I've indicated in the code.\n",
    "\n",
    "# Basic idea of the Kernel\n",
    "At test time, we will see a time series x of length 150 000 to predict the next earthquake. The idea of this kernel is to split x into contiguous chunks of size 1000 and feed summary statistics calculated from these chunks into a current neural net with 150 000 / 1000 = 150 time steps. \n",
    "\n",
    "# Validation\n",
    "Validation is a very complex and crucial element of this competition. Here, the validation generator is fed with data until the second earthquake, while the training generator solely picks chunks after the second earthquake. In order to reach a better LB score, you will need to change this so that the training generator has access to the full data.\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import hann\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import convolve\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a79066c13602706f8f5650c26df4543343751a24"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7603a75fef6a3699a80c83f1a022db846ec4b62"
   },
   "outputs": [],
   "source": [
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "float_data = pd.read_csv(\"../input/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 150000\n",
    "segments = int(np.floor(train_df.shape[0] / rows))\n",
    "print(\"Number of segments: \", segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Helper functions\n",
    "## Helper function for data generator\n",
    "Extracts mean, standard deviation, and quantiles per time step.\n",
    "Can easily be extended. Expects a two dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f613168286f30f0b726a5a87b19496b886e5799d"
   },
   "outputs": [],
   "source": [
    "def extract_features(z):\n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n",
    "                  z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5e008338780fbabe1bbca8db240321eb2d43ac4"
   },
   "source": [
    "## Create features\n",
    "For a given ending position \"last_index\", we split the last 150 000 values of *x* into 150 pieces of length 1000 each. So *n_steps * step_length* should equal 150 000.\n",
    "From each piece, a set features are extracted. This results in a feature matrix of dimension *(150 time steps x features)*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44b656fdd10ab9f20d07cb9796fb368077aa91ba"
   },
   "outputs": [],
   "source": [
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n",
    "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    # MY CHANGE: This doesn't fix things, I get the same errors\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1).astype(np.float32) - 5 ) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c822f8139814beef638d67db525d4947ac50b50"
   },
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff24351374ff15e299019aa74cb8a58434cf5d04"
   },
   "outputs": [],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "n_features = create_X(float_data[0:150000]).shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)\n",
    "    \n",
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets\n",
    "        \n",
    "batch_size = 32\n",
    "\n",
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "float_data[second_earthquake, 1]\n",
    "\n",
    "# Initialize generators\n",
    "# train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "'''\n",
    "train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    sta = np.cumsum(x ** 2)\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "    return sta / lta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "train_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n",
    "total_mean = train_df['acoustic_data'].mean()\n",
    "total_std = train_df['acoustic_data'].std()\n",
    "total_max = train_df['acoustic_data'].max()\n",
    "total_min = train_df['acoustic_data'].min()\n",
    "total_sum = train_df['acoustic_data'].sum()\n",
    "total_abs_sum = np.abs(train_df['acoustic_data']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(seg_id, seg, X):\n",
    "    xc = pd.Series(seg['acoustic_data'].values)\n",
    "    zc = np.fft.fft(xc)\n",
    "    \n",
    "    X.loc[seg_id, 'mean'] = xc.mean()\n",
    "    X.loc[seg_id, 'std'] = xc.std()\n",
    "    X.loc[seg_id, 'max'] = xc.max()\n",
    "    X.loc[seg_id, 'min'] = xc.min()\n",
    "    \n",
    "    #FFT transform values\n",
    "    realFFT = np.real(zc)\n",
    "    imagFFT = np.imag(zc)\n",
    "    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n",
    "    X.loc[seg_id, 'Rstd'] = realFFT.std()\n",
    "    X.loc[seg_id, 'Rmax'] = realFFT.max()\n",
    "    X.loc[seg_id, 'Rmin'] = realFFT.min()\n",
    "    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n",
    "    X.loc[seg_id, 'Istd'] = imagFFT.std()\n",
    "    X.loc[seg_id, 'Imax'] = imagFFT.max()\n",
    "    X.loc[seg_id, 'Imin'] = imagFFT.min()\n",
    "    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n",
    "    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n",
    "    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n",
    "    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n",
    "    X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) / xc[:-1]))[0])\n",
    "    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n",
    "    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n",
    "    \n",
    "    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n",
    "    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n",
    "    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n",
    "    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n",
    "    \n",
    "    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n",
    "    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n",
    "    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n",
    "    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n",
    "    \n",
    "    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n",
    "    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n",
    "    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n",
    "    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n",
    "    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n",
    "    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n",
    "    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n",
    "    \n",
    "    X.loc[seg_id, 'max_to_min'] = xc.max() / np.abs(xc.min())\n",
    "    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n",
    "    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n",
    "    X.loc[seg_id, 'sum'] = xc.sum()\n",
    "    \n",
    "    X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) / xc[:50000][:-1]))[0])\n",
    "    X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) / xc[-50000:][:-1]))[0])\n",
    "    X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) / xc[:10000][:-1]))[0])\n",
    "    X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) / xc[-10000:][:-1]))[0])\n",
    "    \n",
    "    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n",
    "    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n",
    "    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n",
    "    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n",
    "    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n",
    "    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n",
    "    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'trend'] = add_trend_feature(xc)\n",
    "    X.loc[seg_id, 'abs_trend'] = add_trend_feature(xc, abs_values=True)\n",
    "    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n",
    "    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n",
    "    \n",
    "    X.loc[seg_id, 'mad'] = xc.mad()\n",
    "    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n",
    "    X.loc[seg_id, 'skew'] = xc.skew()\n",
    "    X.loc[seg_id, 'med'] = xc.median()\n",
    "    \n",
    "    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n",
    "    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta(xc, 500, 10000).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta(xc, 5000, 100000).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta(xc, 3333, 6666).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta(xc, 10000, 25000).mean()\n",
    "    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n",
    "    ewma = pd.Series.ewm\n",
    "    X.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n",
    "    no_of_std = 2\n",
    "    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n",
    "    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n",
    "    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n",
    "    \n",
    "    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n",
    "    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n",
    "    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n",
    "    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n",
    "    \n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id in tqdm_notebook(range(segments)):\n",
    "    seg = train_df.iloc[seg_id*rows:seg_id*rows+rows]\n",
    "    create_features(seg_id, seg, train_X)\n",
    "    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "scaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "test_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id in tqdm_notebook(test_X.index):\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    create_features(seg_id, seg, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04a3f10117cf8a343a14b543a89da29e17e37b57"
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cb0f322b75ff0b99cfa387c4cdf4e0ba1b6bd57"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ad308c89964016bdd68f4e47c54c6cdb4dcdd99"
   },
   "source": [
    "# Compile and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "205c34084b16a4f12d02670732948fa2885ea1e8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f018417dfa11a7949f0d6ee60360c8086d652ee8"
   },
   "source": [
    "# Visualize accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5955f00b93ddff2c34ac06c34e956d16f01de620"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "690bac2b88759da70329c0a2ff5d37a2976db7e6"
   },
   "source": [
    "# Load submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80137e94051a31311cf02ef720cf4ce5ecc404fc"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d2ebcc27d44ef5fabf4edf6657cf3fb1f1c3973"
   },
   "source": [
    "## Prepare submission data\n",
    "Load each test data, create the feature matrix, get numeric prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a18c660aa93a7aefa19a304b1cbc72a52c337bab"
   },
   "outputs": [],
   "source": [
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ef3dfcb47c3dc06e936e859a7f9e519a52e8085"
   },
   "source": [
    "## Save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3806f69a539d7d9eeadeb5691ab6f243d53db5f6"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
