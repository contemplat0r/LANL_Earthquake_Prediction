{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow import keras\n",
    "#from gplearn.genetic import SymbolicRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "#import numpy as np \n",
    "#import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, TimeDistributed, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "#seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "#set_random_seed(5944)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with simple derived features\n",
    "\n",
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1]).values\n",
    "    change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return np.mean(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(df, sort_field_name='acoustic_data', num_of_extremals=12):\n",
    "    sorted_df = df.sort_values(sort_field_name)\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_df.index[i]\n",
    "        idx_max = sorted_df.index[-i - 1]\n",
    "        min_v = df.iloc[idx_min][sort_field_name]\n",
    "        max_v = df.iloc[idx_max][sort_field_name]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        min_v = series.iloc[idx_min]\n",
    "        max_v = series.iloc[idx_max]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremals(series, num_of_extremals=6):\n",
    "\n",
    "    sorted_series = series.sort_values()\n",
    "    extremals_indexes = set()\n",
    "    extremals = []\n",
    "    \n",
    "    i = 0\n",
    "    min_idx_idx = 0\n",
    "    max_idx_idx = 0\n",
    "    extremals_coutner = 0\n",
    "    while (extremals_coutner < num_of_extremals):\n",
    "\n",
    "        idx_min = sorted_series.index[min_idx_idx]\n",
    "        idx_min_not_proceed = not idx_min in extremals_indexes\n",
    "\n",
    "        idx_max = sorted_series.index[-max_idx_idx - 1]\n",
    "        idx_max_not_proceed = not idx_max in extremals_indexes\n",
    "\n",
    "        if idx_min_not_proceed and idx_max_not_proceed:\n",
    "            if idx_max < idx_min:               \n",
    "                idx_min, idx_max = idx_max, idx_min\n",
    "            extremals_indexes = extremals_indexes.union(set(range(idx_min, idx_max + 1)))\n",
    "            extremals.append(series.iloc[idx_min:idx_max])\n",
    "            min_idx_idx += 1\n",
    "            max_idx_idx += 1\n",
    "            extremals_coutner += 1\n",
    "        else:\n",
    "            if not idx_min_not_proceed:\n",
    "                min_idx_idx += 1\n",
    "            if not idx_max_not_proceed:\n",
    "                max_idx_idx += 1\n",
    "\n",
    "    return extremals, series.loc[set(series.index).difference(extremals_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremals(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremals_indexes = set()\n",
    "    extremals = []    \n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        if idx_max < idx_min:               \n",
    "            idx_min, idx_max = idx_max, idx_min\n",
    "        extremals_indexes = extremals_indexes.union(set(range(idx_min, idx_max + 1)))\n",
    "        extremals.append(series.iloc[idx_min:idx_max])\n",
    "        \n",
    "    return extremals, series.loc[set(series.index).difference(extremals_indexes)]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dtype = dtype\n",
    "        self.filename = None\n",
    "        self.n_jobs = n_jobs\n",
    "        self.test_files = []\n",
    "        if self.dtype == 'train':\n",
    "            self.filename = '../input/train/train.csv'\n",
    "            self.total_data = int(629145481 / self.chunk_size)\n",
    "            #print(\"Feature Generator __init__, self.total_data:\", self.total_data)\n",
    "        else:\n",
    "            submission = pd.read_csv('../input/sample_submission.csv')\n",
    "            for seg_id in submission.seg_id.values:\n",
    "                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n",
    "            #print(\"Feature Generator __init__, int(len(submission)):\", int(len(submission)))\n",
    "            self.total_data = int(len(submission))\n",
    "\n",
    "    def read_chunks(self):\n",
    "        if self.dtype == 'train':\n",
    "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
    "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
    "            for counter, df in enumerate(iter_df):\n",
    "                x = df.acoustic_data.values\n",
    "                y = df.time_to_failure.values[-1]\n",
    "                seg_id = 'train_' + str(counter)\n",
    "                del df\n",
    "                yield seg_id, x, y\n",
    "        else:\n",
    "            for seg_id, f in self.test_files:\n",
    "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
    "                x = df.acoustic_data.values[-self.chunk_size:]\n",
    "                del df\n",
    "                yield seg_id, x, -999\n",
    "    \n",
    "    def get_features(self, x, y, seg_id):\n",
    "        \"\"\"\n",
    "        Gets three groups of features: from original data and from reald and imaginary parts of FFT.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = pd.Series(x)\n",
    "        \n",
    "        '''\n",
    "        zc = np.fft.fft(x)\n",
    "        realFFT = pd.Series(np.real(zc))\n",
    "        imagFFT = pd.Series(np.imag(zc))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        main_dict = self.features(x, y, seg_id)\n",
    "        \n",
    "        '''\n",
    "        r_dict = self.features(realFFT, y, seg_id)\n",
    "        i_dict = self.features(imagFFT, y, seg_id)\n",
    "        \n",
    "        for k, v in r_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'fftr_{k}'] = v\n",
    "                \n",
    "        for k, v in i_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'ffti_{k}'] = v\n",
    "        '''\n",
    "        return main_dict\n",
    "        \n",
    "    \n",
    "    def features(self, x, y, seg_id):\n",
    "        feature_dict = dict()\n",
    "        feature_dict['target'] = y\n",
    "        feature_dict['seg_id'] = seg_id\n",
    "\n",
    "        # create features here\n",
    "\n",
    "        # lists with parameters to iterate over them\n",
    "        #percentiles = [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]\n",
    "        percentiles = [10, 20]\n",
    "        hann_windows = [50, 150, 1500, 15000]\n",
    "        spans = [300, 3000, 30000, 50000]\n",
    "        windows = [10, 50, 100, 500, 1000, 10000]\n",
    "        borders = list(range(-4000, 4001, 1000))\n",
    "        #peaks = [10, 20, 50, 100]\n",
    "        peaks = [10]\n",
    "        coefs = [1, 5, 10, 50, 100]\n",
    "        lags = [10, 100, 1000, 10000]\n",
    "        #autocorr_lags = [5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "        autocorr_lags = [5]\n",
    "        \n",
    "        # basic stats\n",
    "        feature_dict['mean'] = x.mean()\n",
    "        feature_dict['std'] = x.std()\n",
    "        feature_dict['max'] = x.max()\n",
    "        feature_dict['min'] = x.min()\n",
    "        \n",
    "        extremals, not_extremals = get_extremals(x, num_of_extremals=5)\n",
    "        \n",
    "        \n",
    "        for i, extremal in enumerate(extremals):\n",
    "            feature_dict[f'extr_accel_{i}'] = np.abs((extremal.max() - extremal.min()) / len(extremal))\n",
    "            feature_dict[f'extr_mean_{i}'] = extremal.mean()\n",
    "            feature_dict[f'extr_std_{i}'] = extremal.std()\n",
    "            \n",
    "            '''\n",
    "            print()\n",
    "            extr_fft = np.fft.fft(extremal)\n",
    "            print(\"len(extr_fft):\", len(extr_fft))\n",
    "            extr_fft = extr_fft[:3]\n",
    "            print(\"len(extr_fft[:3]):\", len(extr_fft))\n",
    "            print(\"extr_fft.shape\", extr_fft.shape)\n",
    "            print(\"extr_fft:\\n\", extr_fft)\n",
    "            for j, extr in enumerate(extr_fft):\n",
    "                feature_dict[f'extr_fft_re_{i}_{j}'] = np.real(extr)\n",
    "                print(\"extr_fft_re_{}_{}:\".format(i, j), feature_dict[f'extr_fft_re_{i}_{j}'])\n",
    "                feature_dict[f'extr_fft_im_{i}_{j}'] = np.imag(extr)\n",
    "                print(\"extr_fft_im_{}_{}:\".format(i, j), feature_dict[f'extr_fft_im_{i}_{j}'])\n",
    "            print() \n",
    "            #for k, extr in enumerate(np.imag(extr_fft)):\n",
    "            #    feature_dict[f'extr_fft_im_{i}_{k}'] = extr\n",
    "            #    print(\"extr_fft_im_{}_{}:\\n\".format(i, k), feature_dict[f'extr_fft_im_{i}_{k}'])\n",
    "                \n",
    "            #print()\n",
    "        '''\n",
    "        min_points_num = 5\n",
    "        #print(\"len(x):\", len(x))\n",
    "        filtered_x = x[np.abs(x) > 150]\n",
    "        for threshold in range(130, 30, 20):\n",
    "            if len(filtered_x) < min_points_num:\n",
    "                filtered_x = x[np.abc(x) > threshold]\n",
    "                break\n",
    "\n",
    "        if len(filtered_x) < min_points_num:\n",
    "            filtered_x = x\n",
    "        #print(\"len(filtered_x):\", len(filtered_x))\n",
    "        fft_filtered_x = np.fft.fft(filtered_x)[:min_points_num]\n",
    "        for i, f_coef in enumerate(fft_filtered_x):\n",
    "            feature_dict[f'v_fft_re_{i}'] = np.real(f_coef)\n",
    "            feature_dict[f'v_fft_im_{i}'] = np.imag(f_coef)\n",
    "            \n",
    "        for i, item in enumerate(x.value_counts().iloc[:5].items()):\n",
    "            feature_dict[f'rel_freq_{i}'] = item[0] / item[1]\n",
    "\n",
    "        \n",
    "        whole_fft_points_num = 4\n",
    "        whole_fft_result = np.fft.fft(x)[:whole_fft_points_num]\n",
    "        for i, f_coef in enumerate(whole_fft_result):\n",
    "            feature_dict[f'w_fft_re_{i}'] = np.real(f_coef)\n",
    "            feature_dict[f'w_fft_im_{i}'] = np.imag(f_coef)\n",
    "        #feature_dict[f'fft'] = acoustic_data[np.abs(acoustic_data) > 150]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        feature_dict[f'not_extr_mean'] = not_extremals.mean()\n",
    "        feature_dict[f'not_extr_std'] = not_extremals.std()\n",
    "        feature_dict[f'not_extr_min'] = not_extremals.min()\n",
    "        feature_dict[f'not_extr_max'] = not_extremals.max()\n",
    "        '''\n",
    "            \n",
    "        '''\n",
    "        # basic stats on absolute values\n",
    "        feature_dict['mean_change_abs'] = np.mean(np.diff(x))\n",
    "        feature_dict['abs_max'] = np.abs(x).max()\n",
    "        feature_dict['abs_mean'] = np.abs(x).mean()\n",
    "        feature_dict['abs_std'] = np.abs(x).std()\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # geometric and harminic means\n",
    "        '''\n",
    "        feature_dict['hmean'] = stats.hmean(np.abs(x[np.nonzero(x)[0]]))\n",
    "        feature_dict['gmean'] = stats.gmean(np.abs(x[np.nonzero(x)[0]])) \n",
    "\n",
    "        # k-statistic and moments\n",
    "        for i in range(1, 5):\n",
    "            feature_dict[f'kstat_{i}'] = stats.kstat(x, i)\n",
    "            feature_dict[f'moment_{i}'] = stats.moment(x, i)\n",
    "\n",
    "        for i in [1, 2]:\n",
    "            feature_dict[f'kstatvar_{i}'] = stats.kstatvar(x, i)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # aggregations on various slices of data\n",
    "        for agg_type, slice_length, direction in product(['std', 'min', 'max', 'mean'], [1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[:slice_length].agg(agg_type)\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[-slice_length:].agg(agg_type)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''\n",
    "        feature_dict['max_to_min'] = x.max() / np.abs(x.min())\n",
    "        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "        feature_dict['count_big'] = len(x[np.abs(x) > 500])\n",
    "        feature_dict['sum'] = x.sum()\n",
    "\n",
    "        feature_dict['mean_change_rate'] = calc_change_rate(x)\n",
    "        # calc_change_rate on slices of data\n",
    "        for slice_length, direction in product([1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[:slice_length])\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[-slice_length:])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # percentiles on original and absolute values\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'percentile_{p}'] = np.percentile(x, p)\n",
    "            feature_dict[f'abs_percentile_{p}'] = np.percentile(np.abs(x), p)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        feature_dict['trend'] = add_trend_feature(x)\n",
    "        feature_dict['abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "        '''\n",
    "\n",
    "        #feature_dict['mad'] = x.mad()\n",
    "        feature_dict['kurt'] = x.kurtosis()\n",
    "        feature_dict['skew'] = x.skew()\n",
    "        #feature_dict['med'] = x.median()\n",
    "\n",
    "        '''\n",
    "        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "\n",
    "        for hw in hann_windows:\n",
    "            feature_dict[f'Hann_window_mean_{hw}'] = (convolve(x, hann(hw), mode='same') / sum(hann(hw))).mean()\n",
    "\n",
    "        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
    "        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
    "        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
    "        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # exponential rolling statistics\n",
    "        ewma = pd.Series.ewm\n",
    "        for s in spans:\n",
    "            feature_dict[f'exp_Moving_average_{s}_mean'] = (ewma(x, span=s).mean(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_average_{s}_std'] = (ewma(x, span=s).mean(skipna=True)).std(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_mean'] = (ewma(x, span=s).std(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_std'] = (ewma(x, span=s).std(skipna=True)).std(skipna=True)\n",
    "\n",
    "        feature_dict['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "        feature_dict['iqr1'] = np.subtract(*np.percentile(x, [95, 5]))\n",
    "        feature_dict['ave10'] = stats.trim_mean(x, 0.1)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for slice_length, threshold in product([50000, 100000, 150000],\n",
    "                                                     [5, 10, 20, 50, 100]):\n",
    "            feature_dict[f'count_big_{slice_length}_threshold_{threshold}'] = (np.abs(x[-slice_length:]) > threshold).sum()\n",
    "            feature_dict[f'count_big_{slice_length}_less_threshold_{threshold}'] = (np.abs(x[-slice_length:]) < threshold).sum()\n",
    "\n",
    "        # tfresh features take too long to calculate, so I comment them for now\n",
    "\n",
    "#         feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
    "#         feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
    "#         feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
    "#         feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
    "#         feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
    "#         feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
    "#         feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
    "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
    "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for i, j in zip(borders, borders[1:]):\n",
    "            feature_dict[f'range_{i}_{j}'] = feature_calculators.range_count(x, i, j)\n",
    "        '''\n",
    "\n",
    "#         feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
    "#         feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
    "#         feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
    "#         feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
    "#         feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
    "\n",
    "#         for lag in lags:\n",
    "#             feature_dict[f'time_rev_asym_stat_{lag}'] = feature_calculators.time_reversal_asymmetry_statistic(x, lag)\n",
    "        ## for autocorr_lag in autocorr_lags:\n",
    "        ##    feature_dict[f'autocorrelation_{autocorr_lag}'] = feature_calculators.autocorrelation(x, autocorr_lag)\n",
    "        ##    #feature_dict[f'c3_{autocorr_lag}'] = feature_calculators.c3(x, autocorr_lag)\n",
    "\n",
    "#         for coeff, attr in product([1, 2, 3, 4, 5], ['real', 'imag', 'angle']):\n",
    "#             feature_dict[f'fft_{coeff}_{attr}'] = list(feature_calculators.fft_coefficient(x, [{'coeff': coeff, 'attr': attr}]))[0][1]\n",
    "\n",
    "#         feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
    "#         feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
    "#         feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
    "#         feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
    "        \n",
    "    \n",
    "        '''\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'binned_entropy_{p}'] = feature_calculators.binned_entropy(x, p)\n",
    "\n",
    "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
    "        '''\n",
    "        \n",
    "        ## for peak in peaks:\n",
    "        ##    feature_dict[f'num_peaks_{peak}'] = feature_calculators.number_peaks(x, peak)\n",
    "        \n",
    "        '''\n",
    "        for c in coefs:\n",
    "            feature_dict[f'spkt_welch_density_{c}'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': c}]))[0][1]\n",
    "            feature_dict[f'time_rev_asym_stat_{c}'] = feature_calculators.time_reversal_asymmetry_statistic(x, c)  \n",
    "        '''\n",
    "        \n",
    "        # statistics on rolling windows of various sizes\n",
    "        for w in windows:\n",
    "            break\n",
    "            #pass\n",
    "            ## x_roll_std = x.rolling(w).std().dropna().values\n",
    "            ## x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "            \n",
    "            \n",
    "            #feature_dict[f'ave_roll_std_{w}'] = x_roll_std.mean()\n",
    "            #feature_dict[f'std_roll_std_{w}'] = x_roll_std.std()\n",
    "            #feature_dict[f'max_roll_std_{w}'] = x_roll_std.max()\n",
    "            \n",
    "            ## feature_dict[f'min_roll_std_{w}'] = x_roll_std.min()\n",
    "            \n",
    "\n",
    "            ## for p in percentiles:\n",
    "            ##    feature_dict[f'percentile_roll_std_{p}_window_{w}'] = np.percentile(x_roll_std, p)\n",
    "            \n",
    "            '''\n",
    "            feature_dict[f'av_change_abs_roll_std_{w}'] = np.mean(np.diff(x_roll_std))\n",
    "            feature_dict[f'av_change_rate_roll_std_{w}'] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_std_{w}'] = np.abs(x_roll_std).max()\n",
    "\n",
    "            feature_dict[f'ave_roll_mean_{w}'] = x_roll_mean.mean()\n",
    "            feature_dict[f'std_roll_mean_{w}'] = x_roll_mean.std()\n",
    "            feature_dict[f'max_roll_mean_{w}'] = x_roll_mean.max()\n",
    "            feature_dict[f'min_roll_mean_{w}'] = x_roll_mean.min()\n",
    "            \n",
    "            for p in percentiles:\n",
    "                feature_dict[f'percentile_roll_mean_{p}_window_{w}'] = np.percentile(x_roll_mean, p)\n",
    "\n",
    "            feature_dict[f'av_change_abs_roll_mean_{w}'] = np.mean(np.diff(x_roll_mean))\n",
    "            feature_dict[f'av_change_rate_roll_mean_{w}'] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_mean_{w}'] = np.abs(x_roll_mean).max()    \n",
    "            '''\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def generate(self):\n",
    "        feature_list = []\n",
    "        res = Parallel(n_jobs=self.n_jobs,\n",
    "                       backend='threading')(delayed(self.get_features)(x, y, s)\n",
    "                                            for s, x, y in tqdm_notebook(self.read_chunks(), total=self.total_data))\n",
    "        #print(\"FeatureGenerator, generate, type(res)\", type(res))\n",
    "        #print(\"FeatureGenerator, generate, len(res)\", len(res))\n",
    "        for r in res:\n",
    "            feature_list.append(r)\n",
    "        #print(\"FeatureGenerator, generate, len(feature_list)\", len(feature_list))\n",
    "        return pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, selection_list=[], condition=False):\n",
    "    if condition:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col in selection_list]\n",
    "    else:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col not in selection_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d13fd44ab58455e8a0c5d4746065d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a521e25a84663913b4416b55aba6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_fg = FeatureGenerator(dtype='train', n_jobs=20, chunk_size=30000)\n",
    "\n",
    "\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=20, chunk_size=30000)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target\n",
    "train_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extr_accel_0</th>\n",
       "      <th>extr_accel_1</th>\n",
       "      <th>extr_accel_2</th>\n",
       "      <th>extr_accel_3</th>\n",
       "      <th>extr_accel_4</th>\n",
       "      <th>extr_mean_0</th>\n",
       "      <th>extr_mean_1</th>\n",
       "      <th>extr_mean_2</th>\n",
       "      <th>extr_mean_3</th>\n",
       "      <th>extr_mean_4</th>\n",
       "      <th>...</th>\n",
       "      <th>v_fft_re_3</th>\n",
       "      <th>v_fft_re_4</th>\n",
       "      <th>w_fft_im_0</th>\n",
       "      <th>w_fft_im_1</th>\n",
       "      <th>w_fft_im_2</th>\n",
       "      <th>w_fft_im_3</th>\n",
       "      <th>w_fft_re_0</th>\n",
       "      <th>w_fft_re_1</th>\n",
       "      <th>w_fft_re_2</th>\n",
       "      <th>w_fft_re_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.777778</td>\n",
       "      <td>18.363636</td>\n",
       "      <td>24.428571</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>1.887850</td>\n",
       "      <td>13.555556</td>\n",
       "      <td>11.454545</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>-22.111111</td>\n",
       "      <td>5.429907</td>\n",
       "      <td>...</td>\n",
       "      <td>-1060.173040</td>\n",
       "      <td>1196.064201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1622.527147</td>\n",
       "      <td>-561.422514</td>\n",
       "      <td>304.065221</td>\n",
       "      <td>150351.0</td>\n",
       "      <td>1225.730939</td>\n",
       "      <td>-530.481270</td>\n",
       "      <td>-1060.173040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.750000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>8.625000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>4.917457</td>\n",
       "      <td>4.881322</td>\n",
       "      <td>4.889489</td>\n",
       "      <td>...</td>\n",
       "      <td>828.138234</td>\n",
       "      <td>862.173978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1359.143098</td>\n",
       "      <td>-546.991370</td>\n",
       "      <td>-1234.171350</td>\n",
       "      <td>145401.0</td>\n",
       "      <td>592.156306</td>\n",
       "      <td>599.695132</td>\n",
       "      <td>828.138234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.555556</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>5.189255</td>\n",
       "      <td>5.190115</td>\n",
       "      <td>5.184966</td>\n",
       "      <td>5.188974</td>\n",
       "      <td>...</td>\n",
       "      <td>-1605.145368</td>\n",
       "      <td>-542.283274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.041983</td>\n",
       "      <td>-1463.691504</td>\n",
       "      <td>-228.928989</td>\n",
       "      <td>153963.0</td>\n",
       "      <td>-1309.822126</td>\n",
       "      <td>-1146.789123</td>\n",
       "      <td>-1605.145368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>6.653846</td>\n",
       "      <td>3.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>-278.429639</td>\n",
       "      <td>-114.140034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2739.059192</td>\n",
       "      <td>-1781.741144</td>\n",
       "      <td>-254.503859</td>\n",
       "      <td>147660.0</td>\n",
       "      <td>1846.501449</td>\n",
       "      <td>90.501396</td>\n",
       "      <td>-278.429639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.070764</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>5.120567</td>\n",
       "      <td>4.750466</td>\n",
       "      <td>4.401866</td>\n",
       "      <td>5.602273</td>\n",
       "      <td>4.308824</td>\n",
       "      <td>...</td>\n",
       "      <td>-239.102702</td>\n",
       "      <td>960.019851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.405687</td>\n",
       "      <td>1990.014630</td>\n",
       "      <td>1443.411576</td>\n",
       "      <td>135242.0</td>\n",
       "      <td>3308.135813</td>\n",
       "      <td>652.787426</td>\n",
       "      <td>-239.102702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extr_accel_0  extr_accel_1  extr_accel_2  extr_accel_3  extr_accel_4  \\\n",
       "0     21.777778     18.363636     24.428571     18.222222      1.887850   \n",
       "1      8.750000      7.500000      0.004669      0.007467      0.007361   \n",
       "2     11.555556      0.004831      0.004381      0.004831      0.004917   \n",
       "3      6.800000      7.875000      2.880000      2.769231      2.666667   \n",
       "4      0.297872      0.070764      0.002079      0.465909      0.573529   \n",
       "\n",
       "   extr_mean_0  extr_mean_1  extr_mean_2  extr_mean_3  extr_mean_4  \\\n",
       "0    13.555556    11.454545    15.714286   -22.111111     5.429907   \n",
       "1     8.625000     7.200000     4.917457     4.881322     4.889489   \n",
       "2     2.777778     5.189255     5.190115     5.184966     5.188974   \n",
       "3     9.800000    10.750000     2.920000     6.653846     3.148148   \n",
       "4     5.120567     4.750466     4.401866     5.602273     4.308824   \n",
       "\n",
       "      ...        v_fft_re_3   v_fft_re_4  w_fft_im_0   w_fft_im_1  \\\n",
       "0     ...      -1060.173040  1196.064201         0.0 -1622.527147   \n",
       "1     ...        828.138234   862.173978         0.0 -1359.143098   \n",
       "2     ...      -1605.145368  -542.283274         0.0   -37.041983   \n",
       "3     ...       -278.429639  -114.140034         0.0 -2739.059192   \n",
       "4     ...       -239.102702   960.019851         0.0    45.405687   \n",
       "\n",
       "    w_fft_im_2   w_fft_im_3  w_fft_re_0   w_fft_re_1   w_fft_re_2   w_fft_re_3  \n",
       "0  -561.422514   304.065221    150351.0  1225.730939  -530.481270 -1060.173040  \n",
       "1  -546.991370 -1234.171350    145401.0   592.156306   599.695132   828.138234  \n",
       "2 -1463.691504  -228.928989    153963.0 -1309.822126 -1146.789123 -1605.145368  \n",
       "3 -1781.741144  -254.503859    147660.0  1846.501449    90.501396  -278.429639  \n",
       "4  1990.014630  1443.411576    135242.0  3308.135813   652.787426  -239.102702  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 44)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X.loc[X[col] != -np.inf, col].mean()\n",
    "        X.loc[X[col] == -np.inf, col] = mean_value\n",
    "        X[col] = X[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_col = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_columns = X.columns\n",
    "#scaler.fit_transform(X[train_columns])\n",
    "X[train_columns] = scaler.fit_transform(X[train_columns])\n",
    "X_test[train_columns] = scaler.transform(X_test[train_columns])\n",
    "test_X = X_test\n",
    "\n",
    "#print(type(X))\n",
    "#print(type(X_test))\n",
    "#print(type(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extr_accel_0</th>\n",
       "      <th>extr_accel_1</th>\n",
       "      <th>extr_accel_2</th>\n",
       "      <th>extr_accel_3</th>\n",
       "      <th>extr_accel_4</th>\n",
       "      <th>extr_mean_0</th>\n",
       "      <th>extr_mean_1</th>\n",
       "      <th>extr_mean_2</th>\n",
       "      <th>extr_mean_3</th>\n",
       "      <th>extr_mean_4</th>\n",
       "      <th>...</th>\n",
       "      <th>v_fft_re_3</th>\n",
       "      <th>v_fft_re_4</th>\n",
       "      <th>w_fft_im_0</th>\n",
       "      <th>w_fft_im_1</th>\n",
       "      <th>w_fft_im_2</th>\n",
       "      <th>w_fft_im_3</th>\n",
       "      <th>w_fft_re_0</th>\n",
       "      <th>w_fft_re_1</th>\n",
       "      <th>w_fft_re_2</th>\n",
       "      <th>w_fft_re_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988970</td>\n",
       "      <td>0.718051</td>\n",
       "      <td>1.273306</td>\n",
       "      <td>0.937130</td>\n",
       "      <td>-0.321056</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.453977</td>\n",
       "      <td>0.648854</td>\n",
       "      <td>-1.578266</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913482</td>\n",
       "      <td>1.124335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.872933</td>\n",
       "      <td>-0.442006</td>\n",
       "      <td>0.285811</td>\n",
       "      <td>1.729588</td>\n",
       "      <td>0.796560</td>\n",
       "      <td>-0.462348</td>\n",
       "      <td>-1.082829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005752</td>\n",
       "      <td>-0.065023</td>\n",
       "      <td>-0.571063</td>\n",
       "      <td>-0.521407</td>\n",
       "      <td>-0.483163</td>\n",
       "      <td>0.259964</td>\n",
       "      <td>0.139379</td>\n",
       "      <td>-0.022595</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700045</td>\n",
       "      <td>0.808901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.731934</td>\n",
       "      <td>-0.430883</td>\n",
       "      <td>-1.167695</td>\n",
       "      <td>1.149940</td>\n",
       "      <td>0.379223</td>\n",
       "      <td>0.533539</td>\n",
       "      <td>0.849084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217490</td>\n",
       "      <td>-0.605291</td>\n",
       "      <td>-0.571085</td>\n",
       "      <td>-0.521618</td>\n",
       "      <td>-0.483374</td>\n",
       "      <td>-0.218862</td>\n",
       "      <td>-0.009304</td>\n",
       "      <td>-0.005639</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.379151</td>\n",
       "      <td>-0.517923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024167</td>\n",
       "      <td>-1.137414</td>\n",
       "      <td>-0.217824</td>\n",
       "      <td>2.152555</td>\n",
       "      <td>-0.873616</td>\n",
       "      <td>-1.005425</td>\n",
       "      <td>-1.640385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.141416</td>\n",
       "      <td>-0.037993</td>\n",
       "      <td>-0.353933</td>\n",
       "      <td>-0.300260</td>\n",
       "      <td>-0.253919</td>\n",
       "      <td>0.356185</td>\n",
       "      <td>0.401880</td>\n",
       "      <td>-0.146816</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245497</td>\n",
       "      <td>-0.113446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.470652</td>\n",
       "      <td>-1.382545</td>\n",
       "      <td>-0.241990</td>\n",
       "      <td>1.414470</td>\n",
       "      <td>1.205464</td>\n",
       "      <td>0.084848</td>\n",
       "      <td>-0.283035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.632138</td>\n",
       "      <td>-0.600538</td>\n",
       "      <td>-0.571258</td>\n",
       "      <td>-0.484697</td>\n",
       "      <td>-0.434357</td>\n",
       "      <td>-0.027012</td>\n",
       "      <td>-0.041749</td>\n",
       "      <td>-0.054660</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>-0.042602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211892</td>\n",
       "      <td>0.901338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>1.524469</td>\n",
       "      <td>1.362399</td>\n",
       "      <td>-0.039685</td>\n",
       "      <td>2.168247</td>\n",
       "      <td>0.580323</td>\n",
       "      <td>-0.242800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extr_accel_0  extr_accel_1  extr_accel_2  extr_accel_3  extr_accel_4  \\\n",
       "0      0.988970      0.718051      1.273306      0.937130     -0.321056   \n",
       "1      0.005752     -0.065023     -0.571063     -0.521407     -0.483163   \n",
       "2      0.217490     -0.605291     -0.571085     -0.521618     -0.483374   \n",
       "3     -0.141416     -0.037993     -0.353933     -0.300260     -0.253919   \n",
       "4     -0.632138     -0.600538     -0.571258     -0.484697     -0.434357   \n",
       "\n",
       "   extr_mean_0  extr_mean_1  extr_mean_2  extr_mean_3  extr_mean_4  \\\n",
       "0     0.663726     0.453977     0.648854    -1.578266     0.032600   \n",
       "1     0.259964     0.139379    -0.022595    -0.012324    -0.003651   \n",
       "2    -0.218862    -0.009304    -0.005639     0.005291     0.016438   \n",
       "3     0.356185     0.401880    -0.146816     0.090507    -0.120460   \n",
       "4    -0.027012    -0.041749    -0.054660     0.029501    -0.042602   \n",
       "\n",
       "      ...      v_fft_re_3  v_fft_re_4  w_fft_im_0  w_fft_im_1  w_fft_im_2  \\\n",
       "0     ...       -0.913482    1.124335         0.0   -0.872933   -0.442006   \n",
       "1     ...        0.700045    0.808901         0.0   -0.731934   -0.430883   \n",
       "2     ...       -1.379151   -0.517923         0.0   -0.024167   -1.137414   \n",
       "3     ...       -0.245497   -0.113446         0.0   -1.470652   -1.382545   \n",
       "4     ...       -0.211892    0.901338         0.0    0.019970    1.524469   \n",
       "\n",
       "   w_fft_im_3  w_fft_re_0  w_fft_re_1  w_fft_re_2  w_fft_re_3  \n",
       "0    0.285811    1.729588    0.796560   -0.462348   -1.082829  \n",
       "1   -1.167695    1.149940    0.379223    0.533539    0.849084  \n",
       "2   -0.217824    2.152555   -0.873616   -1.005425   -1.640385  \n",
       "3   -0.241990    1.414470    1.205464    0.084848   -0.283035  \n",
       "4    1.362399   -0.039685    2.168247    0.580323   -0.242800  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_columns(X, selection_list=['extr_accel_%i' % i for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, holdout_idx, _, _ = train_test_split(\n",
    "    X.index,\n",
    "    y.index,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X.iloc[train_idx]\n",
    "holdout_X = X.iloc[holdout_idx]\n",
    "train_y = y.iloc[train_idx]\n",
    "holdout_y = y.iloc[holdout_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_idx[:5])\n",
    "#print(train_y_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    model = None\n",
    "    \n",
    "    train_stack_predict = np.array([])\n",
    "    test_stack_predict = np.array([])\n",
    "    \n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            #model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model = lgb.LGBMRegressor(**params, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=600)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            print(\"LGB, y_pred_valid.shape:\", y_pred_valid.shape)\n",
    "            print(\"LGB, y_pred.shape:\", y_pred.shape)\n",
    "            print(\"LGB, train_stack_predict.shape:\", train_stack_predict.shape)\n",
    "            if train_stack_predict.shape[0] == 0:\n",
    "                train_stack_predict = y_pred_valid\n",
    "            else:\n",
    "                train_stack_predict = np.hstack([train_stack_predict, y_pred_valid])            \n",
    "            if test_stack_predict.shape[0] == 0:\n",
    "                test_stack_predict = y_pred\n",
    "            else:\n",
    "                test_stack_predict = np.hstack([test_stack_predict, y_pred])\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            print(\"XGB, y_pred_valid.shape:\", y_pred_valid.shape)\n",
    "            print(\"XGB, y_pred.shape:\", y_pred.shape)\n",
    "            print(\"XGB, train_stack_predict.shape:\", train_stack_predict.shape)\n",
    "            if train_stack_predict.shape[0] == 0:\n",
    "                train_stack_predict = y_pred_valid\n",
    "            else:\n",
    "                train_stack_predict = np.hstack([train_stack_predict, y_pred_valid])            \n",
    "            if test_stack_predict.shape[0] == 0:\n",
    "                test_stack_predict = y_pred\n",
    "            else:\n",
    "                test_stack_predict = np.hstack([test_stack_predict, y_pred])         \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance, model, train_stack_predict, test_stack_predict\n",
    "        return oof, prediction, scores, model, train_stack_predict, test_stack_predict  \n",
    "    else:\n",
    "        return oof, prediction, scores, model, train_stack_predict, test_stack_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_filtered_columns = []\n",
    "xgb_filtered_columns = ['extr_accel_%i' % i for i in range(5)] + ['extr_mean_%i' % i for i in range(5)] + ['extr_std_%i' % i for i in range(5)] + ['v_fft_re_%i' % i for i in range(5)] + ['v_fft_im_%i' % i for i in range(5)] + ['w_fft_im_%i' % i for i in range(4)] + ['w_fft_re_%i' % i for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(train_X)\n",
    "#type(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = lgb.LGBMRegressor( num_leaves=10, min_child_samples=9, objective='gamma', max_depth= 4, learning_rate= 0.001, boosting_type= \"gbdt\", metric= 'mae', reg_alpha=1, reg_lambda =1, verbosity= -1, random_state= 11, n_estimators = 50_000, n_jobs = -1)\n",
    "\n",
    "#model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='mae', verbose=10000, early_stopping_rounds=500, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Jun  3 13:29:33 2019\n",
      "[0]\ttrain-mae:5.19776\tvalid_data-mae:5.16611\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:3.50796\tvalid_data-mae:3.51062\n",
      "[1000]\ttrain-mae:2.67755\tvalid_data-mae:2.72067\n",
      "[1500]\ttrain-mae:2.30911\tvalid_data-mae:2.39919\n",
      "[2000]\ttrain-mae:2.16956\tvalid_data-mae:2.29346\n",
      "[2500]\ttrain-mae:2.11828\tvalid_data-mae:2.26553\n",
      "[3000]\ttrain-mae:2.09351\tvalid_data-mae:2.2612\n",
      "[3500]\ttrain-mae:2.07676\tvalid_data-mae:2.26299\n",
      "Stopping. Best iteration:\n",
      "[2966]\ttrain-mae:2.09489\tvalid_data-mae:2.26113\n",
      "\n",
      "XGB, y_pred_valid.shape: (3356,)\n",
      "XGB, y_pred.shape: (2624,)\n",
      "XGB, train_stack_predict.shape: (0,)\n",
      "Fold 1 started at Mon Jun  3 13:29:48 2019\n",
      "[0]\ttrain-mae:5.16722\tvalid_data-mae:5.28836\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:3.49937\tvalid_data-mae:3.58189\n",
      "[1000]\ttrain-mae:2.67759\tvalid_data-mae:2.75145\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_params = {\n",
    "    'eta': 0.03,\n",
    "    'num_leaves': 10,\n",
    "    'max_depth': 6,\n",
    "    #'min_child_samples': 9,\n",
    "    'subsample': 0.85,\n",
    "    #'boosting_type': 'reg:linear',\n",
    "    'boosting_type': 'gbdt', \n",
    "    #'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.001,\n",
    "    #'objective': 'gamma',\n",
    "    'eval_metric': 'mae',\n",
    "    #'red_alpha': 1,\n",
    "    #'red_lambda': 1, \n",
    "    'silent': True,\n",
    "    'nthread': 10,\n",
    "    'n_estimators': 50000\n",
    "}\n",
    "oof_xgb, prediction_xgb, scores, xgb_model, xgb_train_stack_predict, xgb_test_stack_predict = train_model(\n",
    "    train_X[train_X.columns.drop(xgb_filtered_columns)],\n",
    "    test_X[test_X.columns.drop(xgb_filtered_columns)],\n",
    "    train_y,\n",
    "    params=xgb_params,\n",
    "    model_type='xgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(test_X))\n",
    "#print(type(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_filtered_columns = ['extr_accel_%i' % i for i in range(5)] + ['extr_mean_%i' % i for i in range(5)] + ['extr_std_%i' % i for i in range(5)]  + ['v_fft_re_%i' % i for i in range(5)] + ['v_fft_im_%i' % i for i in range(5)] + ['w_fft_im_%i' % i for i in range(4)] + ['w_fft_re_%i' % i for i in range(4)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Jun  3 11:22:43 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "[10000]\ttraining's l1: 2.14287\tvalid_1's l1: 2.28656\n",
      "Early stopping, best iteration is:\n",
      "[17074]\ttraining's l1: 2.08917\tvalid_1's l1: 2.28284\n",
      "LGB, y_pred.shape: (2624,)\n",
      "Fold 1 started at Mon Jun  3 11:23:06 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "[10000]\ttraining's l1: 2.14906\tvalid_1's l1: 2.25429\n",
      "Early stopping, best iteration is:\n",
      "[14447]\ttraining's l1: 2.11266\tvalid_1's l1: 2.25186\n",
      "LGB, y_pred.shape: (2624,)\n",
      "Fold 2 started at Mon Jun  3 11:23:25 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "[10000]\ttraining's l1: 2.143\tvalid_1's l1: 2.28842\n",
      "Early stopping, best iteration is:\n",
      "[13758]\ttraining's l1: 2.11154\tvalid_1's l1: 2.2864\n",
      "LGB, y_pred.shape: (2624,)\n",
      "Fold 3 started at Mon Jun  3 11:23:43 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "[10000]\ttraining's l1: 2.15119\tvalid_1's l1: 2.25315\n",
      "Early stopping, best iteration is:\n",
      "[18056]\ttraining's l1: 2.09038\tvalid_1's l1: 2.24729\n",
      "LGB, y_pred.shape: (2624,)\n",
      "Fold 4 started at Mon Jun  3 11:24:07 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "[10000]\ttraining's l1: 2.13414\tvalid_1's l1: 2.31542\n",
      "Early stopping, best iteration is:\n",
      "[10344]\ttraining's l1: 2.13087\tvalid_1's l1: 2.31518\n",
      "LGB, y_pred.shape: (2624,)\n",
      "CV mean score: 2.2767, std: 0.0249.\n",
      "CPU times: user 15min 38s, sys: 8.74 s, total: 15min 46s\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'num_leaves': 128,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': 6,\n",
    "          'learning_rate': 0.001,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.2,\n",
    "          'n_estimators': 50000\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance, lgb_model, lgb_train_stack_predict, lgb_test_stack_predict = train_model(\n",
    "    train_X[train_X.columns.drop(lgb_filtered_columns)],\n",
    "    test_X[test_X.columns.drop(lgb_filtered_columns)],    \n",
    "    train_y,\n",
    "    params=params,\n",
    "    model_type='lgb',\n",
    "    plot_feature_importance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 50\n",
    "call_ES = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=patience,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    #restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "def create_model(input_dim=10):\n",
    "\n",
    "    # The LSTM architecture\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with Dropout regularisation\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=(None, input_dim)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Second LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Fourth LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    # The output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "nn_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "#def train_nn(train_X, test_X, train_columns):\n",
    "def train_nn(train_X, test_X, train_y):\n",
    "    nn_predictions = np.zeros(len(test_X))\n",
    "    num_of_features = train_X.shape[-1]\n",
    "    model = None\n",
    "    train_stack_predict = np.array([])\n",
    "    test_stack_predict = np.array([])\n",
    "    \n",
    "    #print(train_X.shape)\n",
    "    #print(train_y.shape)\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n",
    "        strLog = \"fold {}\".format(fold_)\n",
    "        print(strLog)\n",
    "        fold_idxs.append(val_idx)\n",
    "    \n",
    "        #X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "        X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "        #X_tr = (X_tr.values).reshape(len(X_tr), 1, num_of_features)\n",
    "        X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "        #print(\"X_tr:\\n\", X_tr[2, :])\n",
    "        #X_val = (X_val.values).reshape(len(X_val), 1, num_of_features)\n",
    "        X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "        #print(\"X_val:\\n\", X_val[2, :])\n",
    "        y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "        #print(\"y_tr:\\n\", y_tr[2])\n",
    "        #print(\"y_val:\\n\", y_val[2])\n",
    "        model = create_model(num_of_features)\n",
    "        model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=2, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "        nn_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "        y_pred_valid = model.predict(X_val)\n",
    "        #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "        #test_X = (test_X.values).reshape(len(test_X), 1, num_of_features)\n",
    "        test_X = test_X.reshape(len(test_X), 1, num_of_features)\n",
    "        y_pred = model.predict(test_X)[:,0] / folds.n_splits\n",
    "        print(\"NN, y_pred_valid.shape:\", y_pred_valid.shape)\n",
    "        print(\"NN, y_pred.shape:\", y_pred.shape)\n",
    "        print(\"NN, train_stack_predict.shape:\", train_stack_predict.shape)\n",
    "        if train_stack_predict.shape[0] == 0:\n",
    "            train_stack_predict = y_pred_valid\n",
    "        else:\n",
    "            train_stack_predict = np.hstack([train_stack_predict, y_pred_valid])            \n",
    "        if test_stack_predict.shape[0] == 0:\n",
    "            test_stack_predict = y_pred\n",
    "        else:\n",
    "            test_stack_predict = np.hstack([test_stack_predict, y_pred])\n",
    "        \n",
    "        #nn_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "        nn_predictions += p_tmp\n",
    "        history = model.history.history\n",
    "        tr_loss = history[\"loss\"]\n",
    "        val_loss = history[\"val_loss\"]\n",
    "        print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "        train_score.append(tr_loss[-patience])\n",
    "        #     break\n",
    "    \n",
    "        cv_score = mean_absolute_error(train_y, nn_oof)\n",
    "        print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n",
    "    return model, nn_predictions, train_stack_predict, test_stack_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13421 samples, validate on 3356 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7356 - val_loss: 2.3652\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3815 - val_loss: 2.3463\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3593 - val_loss: 2.3347\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3431 - val_loss: 2.3194\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3334 - val_loss: 2.3145\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3288 - val_loss: 2.3239\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3140 - val_loss: 2.3305\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3107 - val_loss: 2.2942\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3006 - val_loss: 2.2942\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2926 - val_loss: 2.2883\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2922 - val_loss: 2.2950\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2848 - val_loss: 2.2882\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2767 - val_loss: 2.2831\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2769 - val_loss: 2.2716\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2652 - val_loss: 2.2745\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2522 - val_loss: 2.2737\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2578 - val_loss: 2.2727\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2555 - val_loss: 2.2680\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2484 - val_loss: 2.2672\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2574 - val_loss: 2.2630\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2472 - val_loss: 2.2927\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2396 - val_loss: 2.2637\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2370 - val_loss: 2.2644\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2249 - val_loss: 2.2760\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2294 - val_loss: 2.2767\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2296 - val_loss: 2.2846\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2282 - val_loss: 2.2576\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2280 - val_loss: 2.2672\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2216 - val_loss: 2.2753\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2217 - val_loss: 2.2674\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2030 - val_loss: 2.2684\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2175 - val_loss: 2.2790\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2201 - val_loss: 2.2624\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2120 - val_loss: 2.2726\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2080 - val_loss: 2.2691\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2073 - val_loss: 2.2589\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2065 - val_loss: 2.2889\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2033 - val_loss: 2.2720\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.1984 - val_loss: 2.2752\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.1974 - val_loss: 2.2695\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.1918 - val_loss: 2.2734\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.1918 - val_loss: 2.2689\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.1887 - val_loss: 2.2805\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.1958 - val_loss: 2.2716\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.1893 - val_loss: 2.2863\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1842 - val_loss: 2.2723\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1753 - val_loss: 2.2747\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1844 - val_loss: 2.2794\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1775 - val_loss: 2.2784\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1777 - val_loss: 2.2742\n",
      "p_tmp.shape: (2624,)\n",
      "loss: 2.736 | val_loss: 2.365 | diff: -0.370\n",
      "After 5 test_CV = 4.997 | train_CV = 2.736 | 2.262 fold 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13421 samples, validate on 3356 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 2.7691 - val_loss: 2.3208\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3833 - val_loss: 2.3031\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3563 - val_loss: 2.2970\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3484 - val_loss: 2.2934\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3432 - val_loss: 2.3042\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3240 - val_loss: 2.2788\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3224 - val_loss: 2.2752\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3103 - val_loss: 2.2630\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3041 - val_loss: 2.2690\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2974 - val_loss: 2.2616\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2851 - val_loss: 2.2624\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2793 - val_loss: 2.2566\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2812 - val_loss: 2.2589\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2702 - val_loss: 2.2642\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2754 - val_loss: 2.2580\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2596 - val_loss: 2.2626\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2595 - val_loss: 2.2545\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2573 - val_loss: 2.2551\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2535 - val_loss: 2.2478\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2461 - val_loss: 2.2586\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2521 - val_loss: 2.2461\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2448 - val_loss: 2.2663\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2478 - val_loss: 2.2544\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2354 - val_loss: 2.2442\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2360 - val_loss: 2.2526\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2367 - val_loss: 2.2599\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2311 - val_loss: 2.2473\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2266 - val_loss: 2.2766\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2194 - val_loss: 2.2669\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2252 - val_loss: 2.2581\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2144 - val_loss: 2.2514\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2112 - val_loss: 2.2622\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2149 - val_loss: 2.2518\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2150 - val_loss: 2.2489\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2076 - val_loss: 2.2569\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2111 - val_loss: 2.2683\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2004 - val_loss: 2.2594\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2056 - val_loss: 2.2594\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2034 - val_loss: 2.2571\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.1953 - val_loss: 2.2561\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.1929 - val_loss: 2.2539\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.1964 - val_loss: 2.2524\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2004 - val_loss: 2.2546\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.1982 - val_loss: 2.2645\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.1876 - val_loss: 2.2661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      " - 5s - loss: 2.1872 - val_loss: 2.2604\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1901 - val_loss: 2.2637\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1708 - val_loss: 2.2754\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1740 - val_loss: 2.2959\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1830 - val_loss: 2.2561\n",
      "p_tmp.shape: (2624,)\n",
      "loss: 2.769 | val_loss: 2.321 | diff: -0.448\n",
      "After 5 test_CV = 4.294 | train_CV = 2.752 | 1.541 fold 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_11 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_12 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 2.7460 - val_loss: 2.3820\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3769 - val_loss: 2.3499\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3580 - val_loss: 2.3332\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3404 - val_loss: 2.3218\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3264 - val_loss: 2.3178\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3214 - val_loss: 2.3179\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3159 - val_loss: 2.3118\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.2964 - val_loss: 2.3376\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.2850 - val_loss: 2.2883\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2876 - val_loss: 2.3126\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2837 - val_loss: 2.2883\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2709 - val_loss: 2.2826\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2678 - val_loss: 2.3189\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2748 - val_loss: 2.2741\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2616 - val_loss: 2.2940\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2451 - val_loss: 2.2771\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2522 - val_loss: 2.2672\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2500 - val_loss: 2.2865\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2426 - val_loss: 2.2719\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2442 - val_loss: 2.2796\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2290 - val_loss: 2.2662\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2302 - val_loss: 2.2603\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2269 - val_loss: 2.2670\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2294 - val_loss: 2.3021\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2124 - val_loss: 2.2694\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2200 - val_loss: 2.2803\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2176 - val_loss: 2.2687\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2181 - val_loss: 2.2844\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2170 - val_loss: 2.2651\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2160 - val_loss: 2.2702\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2123 - val_loss: 2.2988\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2043 - val_loss: 2.3331\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2027 - val_loss: 2.2781\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2006 - val_loss: 2.2843\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2006 - val_loss: 2.2767\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2076 - val_loss: 2.2721\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.1914 - val_loss: 2.2847\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.1914 - val_loss: 2.2847\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.1923 - val_loss: 2.2818\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.1871 - val_loss: 2.3075\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.1949 - val_loss: 2.2818\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.1833 - val_loss: 2.3020\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.1845 - val_loss: 2.2859\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.1828 - val_loss: 2.2906\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.1735 - val_loss: 2.2915\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1701 - val_loss: 2.3028\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1713 - val_loss: 2.2921\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1782 - val_loss: 2.2819\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1641 - val_loss: 2.2782\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1587 - val_loss: 2.2913\n",
      "p_tmp.shape: (2624,)\n",
      "loss: 2.746 | val_loss: 2.382 | diff: -0.364\n",
      "After 5 test_CV = 3.629 | train_CV = 2.750 | 0.879 fold 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_13 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_14 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.8037 - val_loss: 2.3005\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3883 - val_loss: 2.2926\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3675 - val_loss: 2.3161\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3535 - val_loss: 2.2778\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3441 - val_loss: 2.2732\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3349 - val_loss: 2.2736\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3283 - val_loss: 2.2730\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3155 - val_loss: 2.2655\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3068 - val_loss: 2.2491\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2981 - val_loss: 2.2505\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2961 - val_loss: 2.2675\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2858 - val_loss: 2.2507\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2858 - val_loss: 2.2412\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2758 - val_loss: 2.2402\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2821 - val_loss: 2.2398\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2690 - val_loss: 2.2370\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2711 - val_loss: 2.2423\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2661 - val_loss: 2.2347\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2589 - val_loss: 2.2393\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2522 - val_loss: 2.2674\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2534 - val_loss: 2.2341\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2503 - val_loss: 2.2276\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2525 - val_loss: 2.2401\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2458 - val_loss: 2.2307\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2407 - val_loss: 2.2263\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2456 - val_loss: 2.2324\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2389 - val_loss: 2.2296\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2258 - val_loss: 2.2464\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2295 - val_loss: 2.2315\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2173 - val_loss: 2.2290\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2314 - val_loss: 2.2349\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2281 - val_loss: 2.2398\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2122 - val_loss: 2.2301\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2248 - val_loss: 2.2275\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2189 - val_loss: 2.2370\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2023 - val_loss: 2.2381\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2149 - val_loss: 2.2371\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2072 - val_loss: 2.2357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      " - 5s - loss: 2.2044 - val_loss: 2.2339\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2042 - val_loss: 2.2411\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.2101 - val_loss: 2.2331\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2042 - val_loss: 2.2730\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.1947 - val_loss: 2.2397\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.1969 - val_loss: 2.2318\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.2030 - val_loss: 2.2414\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1947 - val_loss: 2.2392\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1991 - val_loss: 2.2294\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1846 - val_loss: 2.2514\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1931 - val_loss: 2.2538\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1843 - val_loss: 2.2450\n",
      "p_tmp.shape: (2624,)\n",
      "loss: 2.804 | val_loss: 2.300 | diff: -0.503\n",
      "After 5 test_CV = 2.946 | train_CV = 2.764 | 0.183 fold 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7453 - val_loss: 2.3592\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3824 - val_loss: 2.3555\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3514 - val_loss: 2.3383\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3397 - val_loss: 2.3429\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3319 - val_loss: 2.3340\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3264 - val_loss: 2.3244\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3175 - val_loss: 2.3140\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3017 - val_loss: 2.3157\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3015 - val_loss: 2.3058\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2854 - val_loss: 2.3017\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2815 - val_loss: 2.3040\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2718 - val_loss: 2.2974\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2760 - val_loss: 2.3040\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2707 - val_loss: 2.2865\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2607 - val_loss: 2.2932\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2571 - val_loss: 2.2831\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2555 - val_loss: 2.2944\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2464 - val_loss: 2.2804\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2510 - val_loss: 2.2828\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2466 - val_loss: 2.2900\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2426 - val_loss: 2.2807\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2392 - val_loss: 2.2816\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2316 - val_loss: 2.2883\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2333 - val_loss: 2.2795\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2334 - val_loss: 2.2761\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2314 - val_loss: 2.2753\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2292 - val_loss: 2.2784\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2181 - val_loss: 2.2885\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2250 - val_loss: 2.2812\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2257 - val_loss: 2.2746\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2247 - val_loss: 2.2739\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2139 - val_loss: 2.2760\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2227 - val_loss: 2.2656\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2190 - val_loss: 2.2767\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2049 - val_loss: 2.2793\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2034 - val_loss: 2.2723\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2061 - val_loss: 2.2760\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.1979 - val_loss: 2.2824\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.1966 - val_loss: 2.2844\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2005 - val_loss: 2.2893\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.1966 - val_loss: 2.2764\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.1904 - val_loss: 2.2879\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.1909 - val_loss: 2.2778\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.1865 - val_loss: 2.2753\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.1818 - val_loss: 2.2860\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1852 - val_loss: 2.2754\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1807 - val_loss: 2.2798\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1778 - val_loss: 2.2814\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1746 - val_loss: 2.2802\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1836 - val_loss: 2.2796\n",
      "p_tmp.shape: (2624,)\n",
      "loss: 2.745 | val_loss: 2.359 | diff: -0.386\n",
      "After 5 test_CV = 2.269 | train_CV = 2.760 | -0.491 "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stack_predicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-927624ab7133>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(train_X, test_X, train_y)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mcv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_predicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stack_predicts' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "NN_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "NN_predictions = np.zeros(len(test_X))\n",
    "\n",
    "num_of_features = train_X.shape[-1]\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X.values, train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "    \n",
    "    ## X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "    X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "    X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "    y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "    model = create_model(num_of_features)\n",
    "    model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=2, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "    NN_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "    \n",
    "    #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "    test_X = (test_X.values).reshape(len(test_X), 1, num_of_features)\n",
    "    NN_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "    history = model.history.history\n",
    "    tr_loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "    train_score.append(tr_loss[-patience])\n",
    "#     break\n",
    "    \n",
    "cv_score = mean_absolute_error(train_y, NN_oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n",
    "'''\n",
    "\n",
    "nn_filtered_columns = ['extr_mean_%i' % i for i in range(5)] + ['extr_std_%i' % i for i in range(5)] + ['w_fft_im_%i' % i for i in range(4)] + ['w_fft_re_%i' % i for i in range(4)]\n",
    "nn_filtered_columns_np = select_columns(X, nn_filtered_columns)\n",
    "\n",
    "#nn_model = train_nn(train_X, test_X, train_X.columns.drop(nn_filtered_columns))\n",
    "nn_model, nn_predictions, nn_train_stack_predict, nn_test_stack_predict = train_nn(\n",
    "    (train_X.values)[:, nn_filtered_columns_np],\n",
    "    (test_X.values)[:, nn_filtered_columns_np],\n",
    "    train_y.values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d40abaf5f6e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlgb_holdout_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgb_filtered_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(lgb_holdout_pred.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m nn_holdout_pred = nn_model.predict(\n\u001b[0m\u001b[1;32m      6\u001b[0m     (holdout_X[holdout_X.columns.drop(nn_filtered_columns)].values).reshape(len(holdout_X), 1, holdout_X[holdout_X.columns.drop(nn_filtered_columns)].shape[-1])).reshape((holdout_X.shape[0]))\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(nn_holdout_pred.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_holdout_pred = xgb_model.predict(xgb.DMatrix(holdout_X[holdout_X.columns.drop(xgb_filtered_columns)].values, feature_names=holdout_X.columns.drop(xgb_filtered_columns)), ntree_limit=xgb_model.best_ntree_limit)\n",
    "#print(xgb_holdout_pred.shape)\n",
    "lgb_holdout_pred = lgb_model.predict(holdout_X[holdout_X.columns.drop(lgb_filtered_columns)].values)\n",
    "#print(lgb_holdout_pred.shape)\n",
    "nn_holdout_pred = nn_model.predict(\n",
    "    (holdout_X[holdout_X.columns.drop(nn_filtered_columns)].values).reshape(len(holdout_X), 1, holdout_X[holdout_X.columns.drop(nn_filtered_columns)].shape[-1])).reshape((holdout_X.shape[0]))\n",
    "#print(nn_holdout_pred.shape)\n",
    "print(\"xgb holdout prediction MAE:\", mean_absolute_error(holdout_y, xgb_holdout_pred))\n",
    "print(\"lgb holdout prediction MAE:\", mean_absolute_error(holdout_y, lgb_holdout_pred))\n",
    "print(\"nn holdout prediction MAE:\", mean_absolute_error(holdout_y, nn_holdout_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = (xgb_holdout_pred + lgb_holdout_pred + nn_holdout_pred) / 3\n",
    "#print(holdout_prediction.shape)\n",
    "print(\"total holdout prediction MAE:\", mean_absolute_error(holdout_y, holdout_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = (prediction_xgb + prediction_lgb + nn_predictions) / 3\n",
    "submission.to_csv(f'xgb_lgb_nn_{i}_{today}_submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn_stack_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
