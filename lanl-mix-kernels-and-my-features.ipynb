{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow import keras\n",
    "#from gplearn.genetic import SymbolicRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "#import numpy as np \n",
    "#import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, TimeDistributed, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "#seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "#set_random_seed(5944)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with simple derived features\n",
    "\n",
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1]).values\n",
    "    change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return np.mean(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(df, sort_field_name='acoustic_data', num_of_extremals=12):\n",
    "    sorted_df = df.sort_values(sort_field_name)\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_df.index[i]\n",
    "        idx_max = sorted_df.index[-i - 1]\n",
    "        min_v = df.iloc[idx_min][sort_field_name]\n",
    "        max_v = df.iloc[idx_max][sort_field_name]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        min_v = series.iloc[idx_min]\n",
    "        max_v = series.iloc[idx_max]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremals(series, num_of_extremals=6):\n",
    "\n",
    "    sorted_series = series.sort_values()\n",
    "    extremals_indexes = set()\n",
    "    extremals = []\n",
    "    \n",
    "    i = 0\n",
    "    min_idx_idx = 0\n",
    "    max_idx_idx = 0\n",
    "    extremals_coutner = 0\n",
    "    while (extremals_coutner < num_of_extremals):\n",
    "\n",
    "        idx_min = sorted_series.index[min_idx_idx]\n",
    "        idx_min_not_proceed = not idx_min in extremals_indexes\n",
    "\n",
    "        idx_max = sorted_series.index[-max_idx_idx - 1]\n",
    "        idx_max_not_proceed = not idx_max in extremals_indexes\n",
    "\n",
    "        if idx_min_not_proceed and idx_max_not_proceed:\n",
    "            if idx_max < idx_min:               \n",
    "                idx_min, idx_max = idx_max, idx_min\n",
    "            extremals_indexes = extremals_indexes.union(set(range(idx_min, idx_max + 1)))\n",
    "            extremals.append(series.iloc[idx_min:idx_max])\n",
    "            min_idx_idx += 1\n",
    "            max_idx_idx += 1\n",
    "            extremals_coutner += 1\n",
    "        else:\n",
    "            if not idx_min_not_proceed:\n",
    "                min_idx_idx += 1\n",
    "            if not idx_max_not_proceed:\n",
    "                max_idx_idx += 1\n",
    "\n",
    "    return extremals, series.loc[set(series.index).difference(extremals_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremals(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremals_indexes = set()\n",
    "    extremals = []    \n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        if idx_max < idx_min:               \n",
    "            idx_min, idx_max = idx_max, idx_min\n",
    "        extremals_indexes = extremals_indexes.union(set(range(idx_min, idx_max + 1)))\n",
    "        extremals.append(series.iloc[idx_min:idx_max])\n",
    "        \n",
    "    return extremals, series.loc[set(series.index).difference(extremals_indexes)]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dtype = dtype\n",
    "        self.filename = None\n",
    "        self.n_jobs = n_jobs\n",
    "        self.test_files = []\n",
    "        if self.dtype == 'train':\n",
    "            self.filename = '../input/train/train.csv'\n",
    "            self.total_data = int(629145481 / self.chunk_size)\n",
    "            #print(\"Feature Generator __init__, self.total_data:\", self.total_data)\n",
    "        else:\n",
    "            submission = pd.read_csv('../input/sample_submission.csv')\n",
    "            for seg_id in submission.seg_id.values:\n",
    "                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n",
    "            #print(\"Feature Generator __init__, int(len(submission)):\", int(len(submission)))\n",
    "            self.total_data = int(len(submission))\n",
    "\n",
    "    def read_chunks(self):\n",
    "        if self.dtype == 'train':\n",
    "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
    "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
    "            for counter, df in enumerate(iter_df):\n",
    "                x = df.acoustic_data.values\n",
    "                y = df.time_to_failure.values[-1]\n",
    "                seg_id = 'train_' + str(counter)\n",
    "                del df\n",
    "                yield seg_id, x, y\n",
    "        else:\n",
    "            for seg_id, f in self.test_files:\n",
    "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
    "                x = df.acoustic_data.values[-self.chunk_size:]\n",
    "                del df\n",
    "                yield seg_id, x, -999\n",
    "    \n",
    "    def get_features(self, x, y, seg_id):\n",
    "        \"\"\"\n",
    "        Gets three groups of features: from original data and from reald and imaginary parts of FFT.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = pd.Series(x)\n",
    "        \n",
    "        '''\n",
    "        zc = np.fft.fft(x)\n",
    "        realFFT = pd.Series(np.real(zc))\n",
    "        imagFFT = pd.Series(np.imag(zc))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        main_dict = self.features(x, y, seg_id)\n",
    "        \n",
    "        '''\n",
    "        r_dict = self.features(realFFT, y, seg_id)\n",
    "        i_dict = self.features(imagFFT, y, seg_id)\n",
    "        \n",
    "        for k, v in r_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'fftr_{k}'] = v\n",
    "                \n",
    "        for k, v in i_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'ffti_{k}'] = v\n",
    "        '''\n",
    "        return main_dict\n",
    "        \n",
    "    \n",
    "    def features(self, x, y, seg_id):\n",
    "        feature_dict = dict()\n",
    "        feature_dict['target'] = y\n",
    "        feature_dict['seg_id'] = seg_id\n",
    "\n",
    "        # create features here\n",
    "\n",
    "        # lists with parameters to iterate over them\n",
    "        #percentiles = [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]\n",
    "        percentiles = [10, 20]\n",
    "        hann_windows = [50, 150, 1500, 15000]\n",
    "        spans = [300, 3000, 30000, 50000]\n",
    "        windows = [10, 50, 100, 500, 1000, 10000]\n",
    "        borders = list(range(-4000, 4001, 1000))\n",
    "        #peaks = [10, 20, 50, 100]\n",
    "        peaks = [10]\n",
    "        coefs = [1, 5, 10, 50, 100]\n",
    "        lags = [10, 100, 1000, 10000]\n",
    "        #autocorr_lags = [5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "        autocorr_lags = [5]\n",
    "        \n",
    "        # basic stats\n",
    "        feature_dict['mean'] = x.mean()\n",
    "        feature_dict['std'] = x.std()\n",
    "        feature_dict['max'] = x.max()\n",
    "        feature_dict['min'] = x.min()\n",
    "        \n",
    "        extremals, not_extremals = get_extremals(x, num_of_extremals=5)\n",
    "        \n",
    "        \n",
    "        for i, extremal in enumerate(extremals):\n",
    "            feature_dict[f'extr_accel_{i}'] = np.abs((extremal.max() - extremal.min()) / len(extremal))\n",
    "            #feature_dict[f'extr_mean_{i}'] = extremal.mean()\n",
    "            #feature_dict[f'extr_std_{i}'] = extremal.std()\n",
    "         \n",
    "        for i, item in enumerate(x.value_counts().iloc[:5].items()):\n",
    "            feature_dict[f'rel_freq_{i}'] = item[0] / item[1]\n",
    "\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        feature_dict[f'not_extr_mean'] = not_extremals.mean()\n",
    "        feature_dict[f'not_extr_std'] = not_extremals.std()\n",
    "        feature_dict[f'not_extr_min'] = not_extremals.min()\n",
    "        feature_dict[f'not_extr_max'] = not_extremals.max()\n",
    "        '''\n",
    "            \n",
    "        '''\n",
    "        # basic stats on absolute values\n",
    "        feature_dict['mean_change_abs'] = np.mean(np.diff(x))\n",
    "        feature_dict['abs_max'] = np.abs(x).max()\n",
    "        feature_dict['abs_mean'] = np.abs(x).mean()\n",
    "        feature_dict['abs_std'] = np.abs(x).std()\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # geometric and harminic means\n",
    "        '''\n",
    "        feature_dict['hmean'] = stats.hmean(np.abs(x[np.nonzero(x)[0]]))\n",
    "        feature_dict['gmean'] = stats.gmean(np.abs(x[np.nonzero(x)[0]])) \n",
    "\n",
    "        # k-statistic and moments\n",
    "        for i in range(1, 5):\n",
    "            feature_dict[f'kstat_{i}'] = stats.kstat(x, i)\n",
    "            feature_dict[f'moment_{i}'] = stats.moment(x, i)\n",
    "\n",
    "        for i in [1, 2]:\n",
    "            feature_dict[f'kstatvar_{i}'] = stats.kstatvar(x, i)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # aggregations on various slices of data\n",
    "        for agg_type, slice_length, direction in product(['std', 'min', 'max', 'mean'], [1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[:slice_length].agg(agg_type)\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[-slice_length:].agg(agg_type)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''\n",
    "        feature_dict['max_to_min'] = x.max() / np.abs(x.min())\n",
    "        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "        feature_dict['count_big'] = len(x[np.abs(x) > 500])\n",
    "        feature_dict['sum'] = x.sum()\n",
    "\n",
    "        feature_dict['mean_change_rate'] = calc_change_rate(x)\n",
    "        # calc_change_rate on slices of data\n",
    "        for slice_length, direction in product([1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[:slice_length])\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[-slice_length:])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # percentiles on original and absolute values\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'percentile_{p}'] = np.percentile(x, p)\n",
    "            feature_dict[f'abs_percentile_{p}'] = np.percentile(np.abs(x), p)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        feature_dict['trend'] = add_trend_feature(x)\n",
    "        feature_dict['abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "        '''\n",
    "\n",
    "        #feature_dict['mad'] = x.mad()\n",
    "        feature_dict['kurt'] = x.kurtosis()\n",
    "        feature_dict['skew'] = x.skew()\n",
    "        #feature_dict['med'] = x.median()\n",
    "\n",
    "        '''\n",
    "        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "\n",
    "        for hw in hann_windows:\n",
    "            feature_dict[f'Hann_window_mean_{hw}'] = (convolve(x, hann(hw), mode='same') / sum(hann(hw))).mean()\n",
    "\n",
    "        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
    "        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
    "        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
    "        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # exponential rolling statistics\n",
    "        ewma = pd.Series.ewm\n",
    "        for s in spans:\n",
    "            feature_dict[f'exp_Moving_average_{s}_mean'] = (ewma(x, span=s).mean(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_average_{s}_std'] = (ewma(x, span=s).mean(skipna=True)).std(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_mean'] = (ewma(x, span=s).std(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_std'] = (ewma(x, span=s).std(skipna=True)).std(skipna=True)\n",
    "\n",
    "        feature_dict['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "        feature_dict['iqr1'] = np.subtract(*np.percentile(x, [95, 5]))\n",
    "        feature_dict['ave10'] = stats.trim_mean(x, 0.1)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for slice_length, threshold in product([50000, 100000, 150000],\n",
    "                                                     [5, 10, 20, 50, 100]):\n",
    "            feature_dict[f'count_big_{slice_length}_threshold_{threshold}'] = (np.abs(x[-slice_length:]) > threshold).sum()\n",
    "            feature_dict[f'count_big_{slice_length}_less_threshold_{threshold}'] = (np.abs(x[-slice_length:]) < threshold).sum()\n",
    "\n",
    "        # tfresh features take too long to calculate, so I comment them for now\n",
    "\n",
    "#         feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
    "#         feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
    "#         feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
    "#         feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
    "#         feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
    "#         feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
    "#         feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
    "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
    "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for i, j in zip(borders, borders[1:]):\n",
    "            feature_dict[f'range_{i}_{j}'] = feature_calculators.range_count(x, i, j)\n",
    "        '''\n",
    "\n",
    "#         feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
    "#         feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
    "#         feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
    "#         feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
    "#         feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
    "\n",
    "#         for lag in lags:\n",
    "#             feature_dict[f'time_rev_asym_stat_{lag}'] = feature_calculators.time_reversal_asymmetry_statistic(x, lag)\n",
    "        ## for autocorr_lag in autocorr_lags:\n",
    "        ##    feature_dict[f'autocorrelation_{autocorr_lag}'] = feature_calculators.autocorrelation(x, autocorr_lag)\n",
    "        ##    #feature_dict[f'c3_{autocorr_lag}'] = feature_calculators.c3(x, autocorr_lag)\n",
    "\n",
    "#         for coeff, attr in product([1, 2, 3, 4, 5], ['real', 'imag', 'angle']):\n",
    "#             feature_dict[f'fft_{coeff}_{attr}'] = list(feature_calculators.fft_coefficient(x, [{'coeff': coeff, 'attr': attr}]))[0][1]\n",
    "\n",
    "#         feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
    "#         feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
    "#         feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
    "#         feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
    "        \n",
    "    \n",
    "        '''\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'binned_entropy_{p}'] = feature_calculators.binned_entropy(x, p)\n",
    "\n",
    "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
    "        '''\n",
    "        \n",
    "        ## for peak in peaks:\n",
    "        ##    feature_dict[f'num_peaks_{peak}'] = feature_calculators.number_peaks(x, peak)\n",
    "        \n",
    "        '''\n",
    "        for c in coefs:\n",
    "            feature_dict[f'spkt_welch_density_{c}'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': c}]))[0][1]\n",
    "            feature_dict[f'time_rev_asym_stat_{c}'] = feature_calculators.time_reversal_asymmetry_statistic(x, c)  \n",
    "        '''\n",
    "        \n",
    "        # statistics on rolling windows of various sizes\n",
    "        for w in windows:\n",
    "            break\n",
    "            #pass\n",
    "            ## x_roll_std = x.rolling(w).std().dropna().values\n",
    "            ## x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "            \n",
    "            \n",
    "            #feature_dict[f'ave_roll_std_{w}'] = x_roll_std.mean()\n",
    "            #feature_dict[f'std_roll_std_{w}'] = x_roll_std.std()\n",
    "            #feature_dict[f'max_roll_std_{w}'] = x_roll_std.max()\n",
    "            \n",
    "            ## feature_dict[f'min_roll_std_{w}'] = x_roll_std.min()\n",
    "            \n",
    "\n",
    "            ## for p in percentiles:\n",
    "            ##    feature_dict[f'percentile_roll_std_{p}_window_{w}'] = np.percentile(x_roll_std, p)\n",
    "            \n",
    "            '''\n",
    "            feature_dict[f'av_change_abs_roll_std_{w}'] = np.mean(np.diff(x_roll_std))\n",
    "            feature_dict[f'av_change_rate_roll_std_{w}'] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_std_{w}'] = np.abs(x_roll_std).max()\n",
    "\n",
    "            feature_dict[f'ave_roll_mean_{w}'] = x_roll_mean.mean()\n",
    "            feature_dict[f'std_roll_mean_{w}'] = x_roll_mean.std()\n",
    "            feature_dict[f'max_roll_mean_{w}'] = x_roll_mean.max()\n",
    "            feature_dict[f'min_roll_mean_{w}'] = x_roll_mean.min()\n",
    "            \n",
    "            for p in percentiles:\n",
    "                feature_dict[f'percentile_roll_mean_{p}_window_{w}'] = np.percentile(x_roll_mean, p)\n",
    "\n",
    "            feature_dict[f'av_change_abs_roll_mean_{w}'] = np.mean(np.diff(x_roll_mean))\n",
    "            feature_dict[f'av_change_rate_roll_mean_{w}'] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_mean_{w}'] = np.abs(x_roll_mean).max()    \n",
    "            '''\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def generate(self):\n",
    "        feature_list = []\n",
    "        res = Parallel(n_jobs=self.n_jobs,\n",
    "                       backend='threading')(delayed(self.get_features)(x, y, s)\n",
    "                                            for s, x, y in tqdm_notebook(self.read_chunks(), total=self.total_data))\n",
    "        #print(\"FeatureGenerator, generate, type(res)\", type(res))\n",
    "        #print(\"FeatureGenerator, generate, len(res)\", len(res))\n",
    "        for r in res:\n",
    "            feature_list.append(r)\n",
    "        #print(\"FeatureGenerator, generate, len(feature_list)\", len(feature_list))\n",
    "        return pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, selection_list=[], condition=False):\n",
    "    if condition:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col in  pd.DatetimeIndex(selection_list)]\n",
    "    else:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col not in pd.DatetimeIndex(selection_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, selection_list=[], condition=False):\n",
    "    if condition:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col in selection_list]\n",
    "    else:\n",
    "        return [df.columns.get_loc(col) for col in df.columns if col not in selection_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383a75d700db4d388eab0d7c2d0ddc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abc6aa0e5ff464fbf5ca29bc3839e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_fg = FeatureGenerator(dtype='train', n_jobs=20, chunk_size=30000)\n",
    "\n",
    "\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=20, chunk_size=30000)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target\n",
    "train_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extr_accel_0</th>\n",
       "      <th>extr_accel_1</th>\n",
       "      <th>extr_accel_2</th>\n",
       "      <th>extr_accel_3</th>\n",
       "      <th>extr_accel_4</th>\n",
       "      <th>extr_mean_0</th>\n",
       "      <th>extr_mean_1</th>\n",
       "      <th>extr_mean_2</th>\n",
       "      <th>extr_mean_3</th>\n",
       "      <th>extr_mean_4</th>\n",
       "      <th>...</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>rel_freq_0</th>\n",
       "      <th>rel_freq_1</th>\n",
       "      <th>rel_freq_2</th>\n",
       "      <th>rel_freq_3</th>\n",
       "      <th>rel_freq_4</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.777778</td>\n",
       "      <td>18.363636</td>\n",
       "      <td>24.428571</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>1.887850</td>\n",
       "      <td>13.555556</td>\n",
       "      <td>11.454545</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>-22.111111</td>\n",
       "      <td>5.429907</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.011700</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.129510</td>\n",
       "      <td>7.367779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.750000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>8.625000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>4.917457</td>\n",
       "      <td>4.881322</td>\n",
       "      <td>4.889489</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.846700</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.070504</td>\n",
       "      <td>4.630081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.555556</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>5.189255</td>\n",
       "      <td>5.190115</td>\n",
       "      <td>5.184966</td>\n",
       "      <td>5.188974</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.132100</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.024047</td>\n",
       "      <td>4.939919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>6.653846</td>\n",
       "      <td>3.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.922000</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>3.947113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.070764</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>5.120567</td>\n",
       "      <td>4.750466</td>\n",
       "      <td>4.401866</td>\n",
       "      <td>5.602273</td>\n",
       "      <td>4.308824</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.508067</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.047550</td>\n",
       "      <td>3.766104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extr_accel_0  extr_accel_1  extr_accel_2  extr_accel_3  extr_accel_4  \\\n",
       "0     21.777778     18.363636     24.428571     18.222222      1.887850   \n",
       "1      8.750000      7.500000      0.004669      0.007467      0.007361   \n",
       "2     11.555556      0.004831      0.004381      0.004831      0.004917   \n",
       "3      6.800000      7.875000      2.880000      2.769231      2.666667   \n",
       "4      0.297872      0.070764      0.002079      0.465909      0.573529   \n",
       "\n",
       "   extr_mean_0  extr_mean_1  extr_mean_2  extr_mean_3  extr_mean_4    ...     \\\n",
       "0    13.555556    11.454545    15.714286   -22.111111     5.429907    ...      \n",
       "1     8.625000     7.200000     4.917457     4.881322     4.889489    ...      \n",
       "2     2.777778     5.189255     5.190115     5.184966     5.188974    ...      \n",
       "3     9.800000    10.750000     2.920000     6.653846     3.148148    ...      \n",
       "4     5.120567     4.750466     4.401866     5.602273     4.308824    ...      \n",
       "\n",
       "     max      mean   min  rel_freq_0  rel_freq_1  rel_freq_2  rel_freq_3  \\\n",
       "0  104.0  5.011700 -98.0    0.001516    0.001949    0.001328    0.001094   \n",
       "1   40.0  4.846700 -35.0    0.001546    0.001259    0.001963    0.001066   \n",
       "2   52.0  5.132100 -56.0    0.001701    0.001442    0.001192    0.002355   \n",
       "3   40.0  4.922000 -32.0    0.001388    0.001139    0.001747    0.000981   \n",
       "4   26.0  4.508067 -16.0    0.001359    0.001104    0.000924    0.001852   \n",
       "\n",
       "   rel_freq_4      skew       std  \n",
       "0    0.002663 -0.129510  7.367779  \n",
       "1    0.002614  0.070504  4.630081  \n",
       "2    0.001067  0.024047  4.939919  \n",
       "3    0.002424  0.018361  3.947113  \n",
       "4    0.002671  0.047550  3.766104  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X.loc[X[col] != -np.inf, col].mean()\n",
    "        X.loc[X[col] == -np.inf, col] = mean_value\n",
    "        X[col] = X[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_columns = X.columns\n",
    "\n",
    "X[train_columns] = scaler.fit_transform(X[train_columns])\n",
    "X_test[train_columns] = scaler.transform(X_test[train_columns])\n",
    "test_X = X_test\n",
    "\n",
    "print(type(X))\n",
    "print(type(X_test))\n",
    "print(type(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extr_accel_0</th>\n",
       "      <th>extr_accel_1</th>\n",
       "      <th>extr_accel_2</th>\n",
       "      <th>extr_accel_3</th>\n",
       "      <th>extr_accel_4</th>\n",
       "      <th>extr_mean_0</th>\n",
       "      <th>extr_mean_1</th>\n",
       "      <th>extr_mean_2</th>\n",
       "      <th>extr_mean_3</th>\n",
       "      <th>extr_mean_4</th>\n",
       "      <th>...</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>rel_freq_0</th>\n",
       "      <th>rel_freq_1</th>\n",
       "      <th>rel_freq_2</th>\n",
       "      <th>rel_freq_3</th>\n",
       "      <th>rel_freq_4</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988970</td>\n",
       "      <td>0.718051</td>\n",
       "      <td>1.273306</td>\n",
       "      <td>0.937130</td>\n",
       "      <td>-0.321056</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.453977</td>\n",
       "      <td>0.648854</td>\n",
       "      <td>-1.578266</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143610</td>\n",
       "      <td>1.731440</td>\n",
       "      <td>-0.187763</td>\n",
       "      <td>0.585335</td>\n",
       "      <td>1.692747</td>\n",
       "      <td>-0.145393</td>\n",
       "      <td>-0.589257</td>\n",
       "      <td>0.932536</td>\n",
       "      <td>-0.574439</td>\n",
       "      <td>0.157014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005752</td>\n",
       "      <td>-0.065023</td>\n",
       "      <td>-0.571063</td>\n",
       "      <td>-0.521407</td>\n",
       "      <td>-0.483163</td>\n",
       "      <td>0.259964</td>\n",
       "      <td>0.139379</td>\n",
       "      <td>-0.022595</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329195</td>\n",
       "      <td>1.151043</td>\n",
       "      <td>0.290751</td>\n",
       "      <td>0.663968</td>\n",
       "      <td>-0.158424</td>\n",
       "      <td>0.973843</td>\n",
       "      <td>-0.630294</td>\n",
       "      <td>0.884871</td>\n",
       "      <td>-0.011521</td>\n",
       "      <td>-0.149899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217490</td>\n",
       "      <td>-0.605291</td>\n",
       "      <td>-0.571085</td>\n",
       "      <td>-0.521618</td>\n",
       "      <td>-0.483374</td>\n",
       "      <td>-0.218862</td>\n",
       "      <td>-0.009304</td>\n",
       "      <td>-0.005639</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240544</td>\n",
       "      <td>2.154953</td>\n",
       "      <td>0.131246</td>\n",
       "      <td>1.064304</td>\n",
       "      <td>0.332338</td>\n",
       "      <td>-0.386772</td>\n",
       "      <td>1.313055</td>\n",
       "      <td>-0.628703</td>\n",
       "      <td>-0.142269</td>\n",
       "      <td>-0.115164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.141416</td>\n",
       "      <td>-0.037993</td>\n",
       "      <td>-0.353933</td>\n",
       "      <td>-0.300260</td>\n",
       "      <td>-0.253919</td>\n",
       "      <td>0.356185</td>\n",
       "      <td>0.401880</td>\n",
       "      <td>-0.146816</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329195</td>\n",
       "      <td>1.415915</td>\n",
       "      <td>0.313537</td>\n",
       "      <td>0.255240</td>\n",
       "      <td>-0.482048</td>\n",
       "      <td>0.591956</td>\n",
       "      <td>-0.759129</td>\n",
       "      <td>0.698914</td>\n",
       "      <td>-0.158271</td>\n",
       "      <td>-0.226464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.632138</td>\n",
       "      <td>-0.600538</td>\n",
       "      <td>-0.571258</td>\n",
       "      <td>-0.484697</td>\n",
       "      <td>-0.434357</td>\n",
       "      <td>-0.027012</td>\n",
       "      <td>-0.041749</td>\n",
       "      <td>-0.054660</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>-0.042602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432621</td>\n",
       "      <td>-0.040117</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.180286</td>\n",
       "      <td>-0.576587</td>\n",
       "      <td>-0.859040</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>0.940487</td>\n",
       "      <td>-0.076123</td>\n",
       "      <td>-0.246756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extr_accel_0  extr_accel_1  extr_accel_2  extr_accel_3  extr_accel_4  \\\n",
       "0      0.988970      0.718051      1.273306      0.937130     -0.321056   \n",
       "1      0.005752     -0.065023     -0.571063     -0.521407     -0.483163   \n",
       "2      0.217490     -0.605291     -0.571085     -0.521618     -0.483374   \n",
       "3     -0.141416     -0.037993     -0.353933     -0.300260     -0.253919   \n",
       "4     -0.632138     -0.600538     -0.571258     -0.484697     -0.434357   \n",
       "\n",
       "   extr_mean_0  extr_mean_1  extr_mean_2  extr_mean_3  extr_mean_4    ...     \\\n",
       "0     0.663726     0.453977     0.648854    -1.578266     0.032600    ...      \n",
       "1     0.259964     0.139379    -0.022595    -0.012324    -0.003651    ...      \n",
       "2    -0.218862    -0.009304    -0.005639     0.005291     0.016438    ...      \n",
       "3     0.356185     0.401880    -0.146816     0.090507    -0.120460    ...      \n",
       "4    -0.027012    -0.041749    -0.054660     0.029501    -0.042602    ...      \n",
       "\n",
       "        max      mean       min  rel_freq_0  rel_freq_1  rel_freq_2  \\\n",
       "0  0.143610  1.731440 -0.187763    0.585335    1.692747   -0.145393   \n",
       "1 -0.329195  1.151043  0.290751    0.663968   -0.158424    0.973843   \n",
       "2 -0.240544  2.154953  0.131246    1.064304    0.332338   -0.386772   \n",
       "3 -0.329195  1.415915  0.313537    0.255240   -0.482048    0.591956   \n",
       "4 -0.432621 -0.040117  0.435065    0.180286   -0.576587   -0.859040   \n",
       "\n",
       "   rel_freq_3  rel_freq_4      skew       std  \n",
       "0   -0.589257    0.932536 -0.574439  0.157014  \n",
       "1   -0.630294    0.884871 -0.011521 -0.149899  \n",
       "2    1.313055   -0.628703 -0.142269 -0.115164  \n",
       "3   -0.759129    0.698914 -0.158271 -0.226464  \n",
       "4    0.553917    0.940487 -0.076123 -0.246756  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_columns(X, selection_list=['extr_accel_%i' % i for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, holdout_idx, _, _ = train_test_split(\n",
    "    X.index,\n",
    "    y.index,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X.iloc[train_idx]\n",
    "holdout_X = X.iloc[holdout_idx]\n",
    "train_y = y.iloc[train_idx]\n",
    "holdout_y = y.iloc[holdout_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([20338, 7490, 705, 15580, 3569], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(train_idx[:5])\n",
    "#print(train_y_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    model = None\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            #model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model = lgb.LGBMRegressor(**params, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=600)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance, model\n",
    "        return oof, prediction, scores, model\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_filtered_columns = []\n",
    "xgb_filtered_columns = ['extr_accel_%i' % i for i in range(5)] + ['extr_mean_%i' % i for i in range(5)] + ['extr_std_%i' % i for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_X)\n",
    "type(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Jun  2 00:45:53 2019\n",
      "[0]\ttrain-mae:5.06212\tvalid_data-mae:5.03231\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:1.78726\tvalid_data-mae:2.28126\n",
      "Stopping. Best iteration:\n",
      "[99]\ttrain-mae:2.09435\tvalid_data-mae:2.26315\n",
      "\n",
      "Fold 1 started at Sun Jun  2 00:45:57 2019\n",
      "[0]\ttrain-mae:5.03327\tvalid_data-mae:5.15119\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:1.79032\tvalid_data-mae:2.24723\n",
      "Stopping. Best iteration:\n",
      "[125]\ttrain-mae:2.07403\tvalid_data-mae:2.23633\n",
      "\n",
      "Fold 2 started at Sun Jun  2 00:46:00 2019\n",
      "[0]\ttrain-mae:5.0688\tvalid_data-mae:5.00478\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:1.78744\tvalid_data-mae:2.29226\n",
      "Stopping. Best iteration:\n",
      "[86]\ttrain-mae:2.11517\tvalid_data-mae:2.25714\n",
      "\n",
      "Fold 3 started at Sun Jun  2 00:46:03 2019\n",
      "[0]\ttrain-mae:5.05972\tvalid_data-mae:5.04199\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:1.79579\tvalid_data-mae:2.25956\n",
      "Stopping. Best iteration:\n",
      "[94]\ttrain-mae:2.11306\tvalid_data-mae:2.2278\n",
      "\n",
      "Fold 4 started at Sun Jun  2 00:46:06 2019\n",
      "[0]\ttrain-mae:5.05716\tvalid_data-mae:5.05447\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 600 rounds.\n",
      "[500]\ttrain-mae:1.7747\tvalid_data-mae:2.30966\n",
      "Stopping. Best iteration:\n",
      "[92]\ttrain-mae:2.09559\tvalid_data-mae:2.28341\n",
      "\n",
      "CV mean score: 2.2536, std: 0.0198.\n",
      "CPU times: user 2min 27s, sys: 52 ms, total: 2min 27s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_params = {\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.85,\n",
    "    #'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': True,\n",
    "    'nthread': 10,\n",
    "    'n_estimators': 4000\n",
    "}\n",
    "oof_xgb, prediction_xgb, scores, xgb_model = train_model(\n",
    "    train_X[train_X.columns.drop(xgb_filtered_columns)],\n",
    "    test_X[test_X.columns.drop(xgb_filtered_columns)],\n",
    "    train_y,\n",
    "    params=xgb_params,\n",
    "    model_type='xgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_X))\n",
    "print(type(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_filtered_columns = []\n",
    "#lgb_filtered_columns_np = select_columns(X, lgb_filtered_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Jun  1 21:56:53 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1986]\ttraining's l1: 1.9665\tvalid_1's l1: 2.29132\n",
      "Fold 1 started at Sat Jun  1 21:56:56 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttraining's l1: 2.0383\tvalid_1's l1: 2.25179\n",
      "Fold 2 started at Sat Jun  1 21:56:59 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2149]\ttraining's l1: 1.94794\tvalid_1's l1: 2.28445\n",
      "Fold 3 started at Sat Jun  1 21:57:02 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1738]\ttraining's l1: 2.004\tvalid_1's l1: 2.24556\n",
      "Fold 4 started at Sat Jun  1 21:57:05 2019\n",
      "Training until validation scores don't improve for 600 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1438]\ttraining's l1: 2.01638\tvalid_1's l1: 2.30449\n",
      "CV mean score: 2.2755, std: 0.0229.\n",
      "CPU times: user 2min 29s, sys: 920 ms, total: 2min 30s\n",
      "Wall time: 16.3 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'num_leaves': 128,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': 6,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.2,\n",
    "          'n_estimators': 4000\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance, lgb_model = train_model(\n",
    "    train_X[train_X.columns.drop(lgb_filtered_columns)],\n",
    "    test_X[test_X.columns.drop(lgb_filtered_columns)],    \n",
    "    train_y,\n",
    "    params=params,\n",
    "    model_type='lgb',\n",
    "    plot_feature_importance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 50\n",
    "call_ES = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=patience,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    #restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "def create_model(input_dim=10):\n",
    "\n",
    "    # The LSTM architecture\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with Dropout regularisation\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=(None, input_dim)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Second LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Fourth LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    # The output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "nn_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "#def train_nn(train_X, test_X, train_columns):\n",
    "def train_nn(train_X, test_X, train_y):\n",
    "    nn_predictions = np.zeros(len(test_X))\n",
    "    num_of_features = train_X.shape[-1]\n",
    "    model = None\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n",
    "        strLog = \"fold {}\".format(fold_)\n",
    "        print(strLog)\n",
    "        fold_idxs.append(val_idx)\n",
    "    \n",
    "        #X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "        X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "        #X_tr = (X_tr.values).reshape(len(X_tr), 1, num_of_features)\n",
    "        X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "        print(\"X_tr:\\n\", X_tr[2, :])\n",
    "        #X_val = (X_val.values).reshape(len(X_val), 1, num_of_features)\n",
    "        X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "        print(\"X_val:\\n\", X_val[2, :])\n",
    "        y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "        #print(\"y_tr:\\n\", y_tr[2])\n",
    "        #print(\"y_val:\\n\", y_val[2])\n",
    "        model = create_model(num_of_features)\n",
    "        model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=2, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "        nn_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "    \n",
    "        #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "        #test_X = (test_X.values).reshape(len(test_X), 1, num_of_features)\n",
    "        test_X = test_X.reshape(len(test_X), 1, num_of_features)\n",
    "        nn_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "        history = model.history.history\n",
    "        tr_loss = history[\"loss\"]\n",
    "        val_loss = history[\"val_loss\"]\n",
    "        print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "        train_score.append(tr_loss[-patience])\n",
    "        #     break\n",
    "    \n",
    "        cv_score = mean_absolute_error(train_y, nn_oof)\n",
    "        print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n",
    "    return model, nn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16777, 26)\n",
      "(16777,)\n",
      "fold 0\n",
      "X_tr:\n",
      " [[-0.65450979 -0.60320399 -0.57103877 -0.52166276 -0.48369213 -0.11474455\n",
      "  -0.07063697 -0.06643076 -0.05340519 -0.05456589 -0.61203492 -0.57886643\n",
      "  -0.57649052 -0.5417292  -0.54227301 -0.93730963 -0.51388457 -1.28568255\n",
      "   0.5034237  -1.00090528 -0.40416118 -1.20491134  0.38395192 -1.06404442\n",
      "  -0.20177305 -0.36780096]]\n",
      "X_val:\n",
      " [[ 0.76989471  0.86303393 -0.52760528 -0.47233437 -0.38267298  2.60405439\n",
      "   0.8363004  -0.04072378  0.01440743  0.07600203  0.23752994  0.30639637\n",
      "  -0.25521012 -0.19185554 -0.10065116 -0.05805667  0.09928462  0.04125559\n",
      "   0.04769649 -0.32278737  0.48685566 -0.79711388  0.67250741 -0.88884689\n",
      "   0.68317434  0.01905903]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_13 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_14 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13421 samples, validate on 3356 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.8022 - val_loss: 2.3571\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3699 - val_loss: 2.3292\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3448 - val_loss: 2.3244\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3449 - val_loss: 2.3523\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3256 - val_loss: 2.3103\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3185 - val_loss: 2.3010\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3079 - val_loss: 2.2951\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3044 - val_loss: 2.2867\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3028 - val_loss: 2.2864\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2968 - val_loss: 2.2807\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2859 - val_loss: 2.2750\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2925 - val_loss: 2.2738\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2788 - val_loss: 2.2730\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2772 - val_loss: 2.2836\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2621 - val_loss: 2.2687\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2659 - val_loss: 2.2803\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2637 - val_loss: 2.2696\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2608 - val_loss: 2.2828\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2616 - val_loss: 2.2734\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2596 - val_loss: 2.2636\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2490 - val_loss: 2.3033\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2506 - val_loss: 2.2684\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2513 - val_loss: 2.2788\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2435 - val_loss: 2.2601\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2362 - val_loss: 2.2735\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2429 - val_loss: 2.2646\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2392 - val_loss: 2.2570\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2366 - val_loss: 2.2598\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2317 - val_loss: 2.2597\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2303 - val_loss: 2.2592\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2283 - val_loss: 2.2583\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2237 - val_loss: 2.2594\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2250 - val_loss: 2.2652\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2169 - val_loss: 2.2567\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2178 - val_loss: 2.2614\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2194 - val_loss: 2.2553\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2185 - val_loss: 2.2714\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2254 - val_loss: 2.2555\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2180 - val_loss: 2.2590\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2118 - val_loss: 2.2754\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.2055 - val_loss: 2.2636\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2117 - val_loss: 2.2672\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2049 - val_loss: 2.2606\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.2012 - val_loss: 2.2786\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.2073 - val_loss: 2.2557\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.2002 - val_loss: 2.2720\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1990 - val_loss: 2.2533\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1970 - val_loss: 2.2580\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.2023 - val_loss: 2.2665\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1985 - val_loss: 2.2537\n",
      "loss: 2.802 | val_loss: 2.357 | diff: -0.445\n",
      "After 5 test_CV = 4.993 | train_CV = 2.802 | 2.191 fold 1\n",
      "X_tr:\n",
      " [[-0.65401099 -0.1731465  -0.5351683  -0.52138001 -0.46583821 -0.13484154\n",
      "   0.07837505 -0.11012445 -0.07534272 -0.10092324 -0.58439036 -0.32740363\n",
      "  -0.48664367 -0.51050187 -0.41854681 -0.78724195 -0.41045844 -1.88331475\n",
      "   0.35911008 -0.61973735  0.07863489 -0.99208272  0.71816807 -1.02477075\n",
      "  -0.63292608 -0.25385349]]\n",
      "X_val:\n",
      " [[-0.65439728 -0.60331397 -0.55425297 -0.24174403 -0.48352424 -0.06058982\n",
      "  -0.04435655 -0.06511376 -0.21574055 -0.01588112 -0.60628175 -0.55935108\n",
      "  -0.52472385 -0.41194978 -0.53587753 -0.89912259 -0.46955909  0.83974006\n",
      "   0.44266007 -0.2858017  -0.84039459  0.41136845 -0.94334448  0.57886311\n",
      "  -0.12047485 -0.33022355]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13421 samples, validate on 3356 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7623 - val_loss: 2.3212\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3798 - val_loss: 2.2978\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3639 - val_loss: 2.2862\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3511 - val_loss: 2.2869\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3386 - val_loss: 2.3063\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3164 - val_loss: 2.2734\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3201 - val_loss: 2.2754\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3074 - val_loss: 2.2635\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3050 - val_loss: 2.2915\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2946 - val_loss: 2.2652\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2858 - val_loss: 2.2619\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2809 - val_loss: 2.2451\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2714 - val_loss: 2.2517\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2719 - val_loss: 2.2674\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2722 - val_loss: 2.2530\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2681 - val_loss: 2.2539\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2583 - val_loss: 2.2519\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2515 - val_loss: 2.2485\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2606 - val_loss: 2.2572\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2554 - val_loss: 2.2418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      " - 5s - loss: 2.2481 - val_loss: 2.2423\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2522 - val_loss: 2.2379\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2512 - val_loss: 2.2359\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2453 - val_loss: 2.2382\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2355 - val_loss: 2.2321\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2305 - val_loss: 2.2528\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2400 - val_loss: 2.2383\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2311 - val_loss: 2.2420\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2261 - val_loss: 2.2418\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2323 - val_loss: 2.2356\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2354 - val_loss: 2.2496\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2279 - val_loss: 2.2584\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2252 - val_loss: 2.2435\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2174 - val_loss: 2.2540\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2224 - val_loss: 2.2501\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2226 - val_loss: 2.2426\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2208 - val_loss: 2.2349\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2210 - val_loss: 2.2465\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2150 - val_loss: 2.2499\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2191 - val_loss: 2.2414\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.2067 - val_loss: 2.2455\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2085 - val_loss: 2.2591\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2083 - val_loss: 2.2524\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.2048 - val_loss: 2.2523\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.2016 - val_loss: 2.2532\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1939 - val_loss: 2.2427\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.2030 - val_loss: 2.2475\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1901 - val_loss: 2.2514\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1975 - val_loss: 2.2537\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1950 - val_loss: 2.2489\n",
      "loss: 2.762 | val_loss: 2.321 | diff: -0.441\n",
      "After 5 test_CV = 4.288 | train_CV = 2.782 | 1.506 fold 2\n",
      "X_tr:\n",
      " [[-0.65401099 -0.1731465  -0.5351683  -0.52138001 -0.46583821 -0.13484154\n",
      "   0.07837505 -0.11012445 -0.07534272 -0.10092324 -0.58439036 -0.32740363\n",
      "  -0.48664367 -0.51050187 -0.41854681 -0.78724195 -0.41045844 -1.88331475\n",
      "   0.35911008 -0.61973735  0.07863489 -0.99208272  0.71816807 -1.02477075\n",
      "  -0.63292608 -0.25385349]]\n",
      "X_val:\n",
      " [[-0.36905277 -0.41953628 -0.30603451 -0.10161374 -0.01750957 -0.04352521\n",
      "   0.0170352  -0.05832945  0.17827178 -0.36212842 -0.05471319 -0.02237952\n",
      "  -0.05672986  0.06701403 -0.04625577 -0.2116464   0.00324607 -1.39003661\n",
      "   0.12365103 -0.35905486 -1.09793298  0.20739969 -1.18943484  0.37431751\n",
      "   0.42059806 -0.0440065 ]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_21 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_22 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_23 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_24 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7471 - val_loss: 2.3425\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3688 - val_loss: 2.3568\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3528 - val_loss: 2.3361\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3402 - val_loss: 2.3370\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3280 - val_loss: 2.2987\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3113 - val_loss: 2.2904\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3116 - val_loss: 2.3161\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3142 - val_loss: 2.2906\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3010 - val_loss: 2.3011\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2987 - val_loss: 2.2684\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2878 - val_loss: 2.2664\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2813 - val_loss: 2.2841\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2855 - val_loss: 2.2624\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2705 - val_loss: 2.2613\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2604 - val_loss: 2.3189\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2627 - val_loss: 2.2517\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2634 - val_loss: 2.2447\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2558 - val_loss: 2.2788\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2483 - val_loss: 2.2455\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2494 - val_loss: 2.2453\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2442 - val_loss: 2.2495\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2466 - val_loss: 2.2571\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2368 - val_loss: 2.2485\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2325 - val_loss: 2.2604\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2341 - val_loss: 2.2820\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2349 - val_loss: 2.2413\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2336 - val_loss: 2.2491\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2304 - val_loss: 2.2522\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2286 - val_loss: 2.2426\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2327 - val_loss: 2.2406\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2263 - val_loss: 2.2611\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2264 - val_loss: 2.2433\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2267 - val_loss: 2.2534\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2334 - val_loss: 2.2473\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2153 - val_loss: 2.2804\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2226 - val_loss: 2.2701\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2088 - val_loss: 2.2444\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2142 - val_loss: 2.2772\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2121 - val_loss: 2.2539\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2039 - val_loss: 2.2633\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.2102 - val_loss: 2.2441\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2083 - val_loss: 2.2455\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2048 - val_loss: 2.3063\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.2084 - val_loss: 2.2479\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.2091 - val_loss: 2.2525\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.2058 - val_loss: 2.2770\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1986 - val_loss: 2.2584\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.2013 - val_loss: 2.2533\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1970 - val_loss: 2.2677\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1999 - val_loss: 2.2728\n",
      "loss: 2.747 | val_loss: 2.342 | diff: -0.405\n",
      "After 5 test_CV = 3.620 | train_CV = 2.771 | 0.849 fold 3\n",
      "X_tr:\n",
      " [[-0.56490761 -0.35875795 -0.47373104 -0.28778684  0.67517395 -0.1095046\n",
      "   0.04694888 -0.02031477 -0.22009161  2.13541322 -0.20706506 -0.0299884\n",
      "  -0.14035886  0.01193714  0.14503074  0.15339435 -0.02630425  0.27024827\n",
      "   0.10086467  0.08840613 -0.67542952  0.62731916 -0.86302127  0.95956751\n",
      "   0.19430332 -0.03888309]]\n",
      "X_val:\n",
      " [[-0.65364956 -0.60462866 -0.57056001  0.50895397 -0.47972593 -0.07291482\n",
      "  -0.05584818 -0.04490354  0.45867318 -0.02142018 -0.54611669 -0.52235384\n",
      "  -0.51339319 -0.02447023 -0.45235555 -0.397356   -0.10756764  0.34399963\n",
      "   0.10086467  0.42061938 -0.19425232 -0.7214919   0.8263337   1.07464718\n",
      "   0.06618876 -0.02965214]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_25 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_26 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_27 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_28 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7579 - val_loss: 2.3202\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3854 - val_loss: 2.2829\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3630 - val_loss: 2.3049\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3490 - val_loss: 2.2881\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3376 - val_loss: 2.2689\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3361 - val_loss: 2.2665\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3253 - val_loss: 2.2532\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.3150 - val_loss: 2.2539\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.3063 - val_loss: 2.2586\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.3066 - val_loss: 2.2388\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.3007 - val_loss: 2.2452\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2898 - val_loss: 2.2300\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2870 - val_loss: 2.2398\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2903 - val_loss: 2.2301\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2800 - val_loss: 2.2576\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2798 - val_loss: 2.2230\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2757 - val_loss: 2.2307\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2749 - val_loss: 2.2402\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2777 - val_loss: 2.2431\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2614 - val_loss: 2.2236\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2610 - val_loss: 2.2083\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2600 - val_loss: 2.2131\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2573 - val_loss: 2.2311\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2488 - val_loss: 2.2630\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2453 - val_loss: 2.2344\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2510 - val_loss: 2.2187\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2497 - val_loss: 2.2084\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2460 - val_loss: 2.2111\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2484 - val_loss: 2.2133\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2466 - val_loss: 2.2080\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2455 - val_loss: 2.2072\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2384 - val_loss: 2.2160\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2369 - val_loss: 2.2136\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2363 - val_loss: 2.2017\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2423 - val_loss: 2.2233\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2352 - val_loss: 2.2181\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2329 - val_loss: 2.2118\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2276 - val_loss: 2.2222\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2314 - val_loss: 2.2125\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2233 - val_loss: 2.2264\n",
      "Epoch 41/50\n",
      " - 5s - loss: 2.2245 - val_loss: 2.2229\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2217 - val_loss: 2.2294\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2207 - val_loss: 2.2166\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.2166 - val_loss: 2.2095\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.2147 - val_loss: 2.2092\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.2188 - val_loss: 2.2129\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.2173 - val_loss: 2.2102\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.2129 - val_loss: 2.2030\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.2142 - val_loss: 2.2112\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.2038 - val_loss: 2.2075\n",
      "loss: 2.758 | val_loss: 2.320 | diff: -0.438\n",
      "After 5 test_CV = 2.930 | train_CV = 2.767 | 0.162 fold 4\n",
      "X_tr:\n",
      " [[-0.56490761 -0.35875795 -0.47373104 -0.28778684  0.67517395 -0.1095046\n",
      "   0.04694888 -0.02031477 -0.22009161  2.13541322 -0.20706506 -0.0299884\n",
      "  -0.14035886  0.01193714  0.14503074  0.15339435 -0.02630425  0.27024827\n",
      "   0.10086467  0.08840613 -0.67542952  0.62731916 -0.86302127  0.95956751\n",
      "   0.19430332 -0.03888309]]\n",
      "X_val:\n",
      " [[-0.50794845  0.26835647 -0.35507546 -0.36927001  0.16665455  0.10835266\n",
      "  -0.94759705 -0.06452439  0.25132655  0.09523474 -0.24012736 -0.06397994\n",
      "  -0.13290501 -0.1624869  -0.1837995  -0.19606057 -0.16666828  1.7768161\n",
      "   0.20720101 -0.05944829  0.7255237  -0.60741048  1.05795635 -0.72750237\n",
      "  -1.0311953  -0.22446838]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_29 (CuDNNLSTM)    (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_30 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_31 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_32 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 76,851\n",
      "Trainable params: 76,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13422 samples, validate on 3355 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.7300 - val_loss: 2.3809\n",
      "Epoch 2/50\n",
      " - 5s - loss: 2.3620 - val_loss: 2.3374\n",
      "Epoch 3/50\n",
      " - 5s - loss: 2.3514 - val_loss: 2.3460\n",
      "Epoch 4/50\n",
      " - 5s - loss: 2.3373 - val_loss: 2.3286\n",
      "Epoch 5/50\n",
      " - 5s - loss: 2.3292 - val_loss: 2.3271\n",
      "Epoch 6/50\n",
      " - 5s - loss: 2.3125 - val_loss: 2.3180\n",
      "Epoch 7/50\n",
      " - 5s - loss: 2.3012 - val_loss: 2.3159\n",
      "Epoch 8/50\n",
      " - 5s - loss: 2.2885 - val_loss: 2.3228\n",
      "Epoch 9/50\n",
      " - 5s - loss: 2.2836 - val_loss: 2.3052\n",
      "Epoch 10/50\n",
      " - 5s - loss: 2.2857 - val_loss: 2.3163\n",
      "Epoch 11/50\n",
      " - 5s - loss: 2.2790 - val_loss: 2.3083\n",
      "Epoch 12/50\n",
      " - 5s - loss: 2.2722 - val_loss: 2.3280\n",
      "Epoch 13/50\n",
      " - 5s - loss: 2.2764 - val_loss: 2.2989\n",
      "Epoch 14/50\n",
      " - 5s - loss: 2.2638 - val_loss: 2.2994\n",
      "Epoch 15/50\n",
      " - 5s - loss: 2.2582 - val_loss: 2.2924\n",
      "Epoch 16/50\n",
      " - 5s - loss: 2.2643 - val_loss: 2.3048\n",
      "Epoch 17/50\n",
      " - 5s - loss: 2.2597 - val_loss: 2.2910\n",
      "Epoch 18/50\n",
      " - 5s - loss: 2.2558 - val_loss: 2.2955\n",
      "Epoch 19/50\n",
      " - 5s - loss: 2.2510 - val_loss: 2.3099\n",
      "Epoch 20/50\n",
      " - 5s - loss: 2.2523 - val_loss: 2.3248\n",
      "Epoch 21/50\n",
      " - 5s - loss: 2.2512 - val_loss: 2.2932\n",
      "Epoch 22/50\n",
      " - 5s - loss: 2.2393 - val_loss: 2.2922\n",
      "Epoch 23/50\n",
      " - 5s - loss: 2.2413 - val_loss: 2.2952\n",
      "Epoch 24/50\n",
      " - 5s - loss: 2.2389 - val_loss: 2.2887\n",
      "Epoch 25/50\n",
      " - 5s - loss: 2.2368 - val_loss: 2.2878\n",
      "Epoch 26/50\n",
      " - 5s - loss: 2.2397 - val_loss: 2.2882\n",
      "Epoch 27/50\n",
      " - 5s - loss: 2.2277 - val_loss: 2.2894\n",
      "Epoch 28/50\n",
      " - 5s - loss: 2.2162 - val_loss: 2.2928\n",
      "Epoch 29/50\n",
      " - 5s - loss: 2.2290 - val_loss: 2.2864\n",
      "Epoch 30/50\n",
      " - 5s - loss: 2.2286 - val_loss: 2.3033\n",
      "Epoch 31/50\n",
      " - 5s - loss: 2.2214 - val_loss: 2.2992\n",
      "Epoch 32/50\n",
      " - 5s - loss: 2.2232 - val_loss: 2.2920\n",
      "Epoch 33/50\n",
      " - 5s - loss: 2.2157 - val_loss: 2.3039\n",
      "Epoch 34/50\n",
      " - 5s - loss: 2.2090 - val_loss: 2.2859\n",
      "Epoch 35/50\n",
      " - 5s - loss: 2.2112 - val_loss: 2.2907\n",
      "Epoch 36/50\n",
      " - 5s - loss: 2.2130 - val_loss: 2.2829\n",
      "Epoch 37/50\n",
      " - 5s - loss: 2.2070 - val_loss: 2.3005\n",
      "Epoch 38/50\n",
      " - 5s - loss: 2.2062 - val_loss: 2.3050\n",
      "Epoch 39/50\n",
      " - 5s - loss: 2.2064 - val_loss: 2.2889\n",
      "Epoch 40/50\n",
      " - 5s - loss: 2.2059 - val_loss: 2.2847\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.2039 - val_loss: 2.2843\n",
      "Epoch 42/50\n",
      " - 5s - loss: 2.2027 - val_loss: 2.2848\n",
      "Epoch 43/50\n",
      " - 5s - loss: 2.2106 - val_loss: 2.2763\n",
      "Epoch 44/50\n",
      " - 5s - loss: 2.2019 - val_loss: 2.2787\n",
      "Epoch 45/50\n",
      " - 5s - loss: 2.1947 - val_loss: 2.2906\n",
      "Epoch 46/50\n",
      " - 5s - loss: 2.1926 - val_loss: 2.2944\n",
      "Epoch 47/50\n",
      " - 5s - loss: 2.1912 - val_loss: 2.2887\n",
      "Epoch 48/50\n",
      " - 5s - loss: 2.1884 - val_loss: 2.2790\n",
      "Epoch 49/50\n",
      " - 5s - loss: 2.1941 - val_loss: 2.2897\n",
      "Epoch 50/50\n",
      " - 5s - loss: 2.1966 - val_loss: 2.2857\n",
      "loss: 2.730 | val_loss: 2.381 | diff: -0.349\n",
      "After 5 test_CV = 2.254 | train_CV = 2.760 | -0.506 CPU times: user 25min 43s, sys: 2min 6s, total: 27min 50s\n",
      "Wall time: 21min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "NN_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "NN_predictions = np.zeros(len(test_X))\n",
    "\n",
    "num_of_features = train_X.shape[-1]\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X.values, train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "    \n",
    "    ## X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "    X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "    X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "    y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "    model = create_model(num_of_features)\n",
    "    model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=2, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "    NN_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "    \n",
    "    #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "    test_X = (test_X.values).reshape(len(test_X), 1, num_of_features)\n",
    "    NN_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "    history = model.history.history\n",
    "    tr_loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "    train_score.append(tr_loss[-patience])\n",
    "#     break\n",
    "    \n",
    "cv_score = mean_absolute_error(train_y, NN_oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n",
    "'''\n",
    "\n",
    "nn_filtered_columns = []\n",
    "nn_filtered_columns_np = select_columns(X, nn_filtered_columns)\n",
    "\n",
    "#nn_model = train_nn(train_X, test_X, train_X.columns.drop(nn_filtered_columns))\n",
    "nn_model, nn_predictions = train_nn(\n",
    "    (train_X.values)[:, nn_filtered_columns_np],\n",
    "    (test_X.values)[:, nn_filtered_columns_np],\n",
    "    train_y.values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195,)\n",
      "(4195,)\n",
      "(4195,)\n",
      "xgb holdout prediction MAE: 2.2993551163200125\n",
      "lgb holdout prediction MAE: 2.3058892886979128\n",
      "nn holdout prediction MAE: 2.3003619337932717\n"
     ]
    }
   ],
   "source": [
    "xgb_holdout_pred = xgb_model.predict(xgb.DMatrix(holdout_X[holdout_X.columns.drop(xgb_filtered_columns)].values, feature_names=holdout_X.columns.drop(xgb_filtered_columns)), ntree_limit=xgb_model.best_ntree_limit)\n",
    "print(xgb_holdout_pred.shape)\n",
    "lgb_holdout_pred = lgb_model.predict(holdout_X.values)\n",
    "print(lgb_holdout_pred.shape)\n",
    "nn_holdout_pred = nn_model.predict(\n",
    "    (holdout_X.values).reshape(len(holdout_X), 1, holdout_X.shape[-1])).reshape((holdout_X.shape[0]))\n",
    "print(nn_holdout_pred.shape)\n",
    "print(\"xgb holdout prediction MAE:\", mean_absolute_error(holdout_y, xgb_holdout_pred))\n",
    "print(\"lgb holdout prediction MAE:\", mean_absolute_error(holdout_y, lgb_holdout_pred))\n",
    "print(\"nn holdout prediction MAE:\", mean_absolute_error(holdout_y, nn_holdout_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4195,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195,)\n",
      "total holdout prediction MAE: 2.2829409437665733\n"
     ]
    }
   ],
   "source": [
    "holdout_prediction = (xgb_holdout_pred + lgb_holdout_pred + nn_holdout_pred) / 3\n",
    "print(holdout_prediction.shape)\n",
    "print(\"total holdout prediction MAE:\", mean_absolute_error(holdout_y, holdout_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_00030f</td>\n",
       "      <td>2.392543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_0012b5</td>\n",
       "      <td>6.205802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_00184e</td>\n",
       "      <td>7.113163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_003339</td>\n",
       "      <td>8.912213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_0042cc</td>\n",
       "      <td>6.434860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id  time_to_failure\n",
       "0  seg_00030f         2.392543\n",
       "1  seg_0012b5         6.205802\n",
       "2  seg_00184e         7.113163\n",
       "3  seg_003339         8.912213\n",
       "4  seg_0042cc         6.434860"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = (prediction_xgb + prediction_lgb + nn_predictions) / 3\n",
    "submission.to_csv(f'xgb_lgb_nn_{i}_{today}_submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
