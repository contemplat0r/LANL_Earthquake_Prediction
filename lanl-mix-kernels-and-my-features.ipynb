{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow import keras\n",
    "#from gplearn.genetic import SymbolicRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "#import numpy as np \n",
    "#import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, TimeDistributed, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "#seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "#set_random_seed(5944)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with simple derived features\n",
    "\n",
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1]).values\n",
    "    change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return np.mean(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(df, sort_field_name='acoustic_data', num_of_extremals=12):\n",
    "    sorted_df = df.sort_values(sort_field_name)\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_df.index[i]\n",
    "        idx_max = sorted_df.index[-i - 1]\n",
    "        min_v = df.iloc[idx_min][sort_field_name]\n",
    "        max_v = df.iloc[idx_max][sort_field_name]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        min_v = series.iloc[idx_min]\n",
    "        max_v = series.iloc[idx_max]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dtype = dtype\n",
    "        self.filename = None\n",
    "        self.n_jobs = n_jobs\n",
    "        self.test_files = []\n",
    "        if self.dtype == 'train':\n",
    "            self.filename = '../input/train/train.csv'\n",
    "            self.total_data = int(629145481 / self.chunk_size)\n",
    "            #print(\"Feature Generator __init__, self.total_data:\", self.total_data)\n",
    "        else:\n",
    "            submission = pd.read_csv('../input/sample_submission.csv')\n",
    "            for seg_id in submission.seg_id.values:\n",
    "                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n",
    "            #print(\"Feature Generator __init__, int(len(submission)):\", int(len(submission)))\n",
    "            self.total_data = int(len(submission))\n",
    "\n",
    "    def read_chunks(self):\n",
    "        if self.dtype == 'train':\n",
    "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
    "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
    "            for counter, df in enumerate(iter_df):\n",
    "                x = df.acoustic_data.values\n",
    "                y = df.time_to_failure.values[-1]\n",
    "                seg_id = 'train_' + str(counter)\n",
    "                del df\n",
    "                yield seg_id, x, y\n",
    "        else:\n",
    "            for seg_id, f in self.test_files:\n",
    "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
    "                x = df.acoustic_data.values[-self.chunk_size:]\n",
    "                del df\n",
    "                yield seg_id, x, -999\n",
    "    \n",
    "    def get_features(self, x, y, seg_id):\n",
    "        \"\"\"\n",
    "        Gets three groups of features: from original data and from reald and imaginary parts of FFT.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = pd.Series(x)\n",
    "        \n",
    "        '''\n",
    "        zc = np.fft.fft(x)\n",
    "        realFFT = pd.Series(np.real(zc))\n",
    "        imagFFT = pd.Series(np.imag(zc))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        main_dict = self.features(x, y, seg_id)\n",
    "        \n",
    "        '''\n",
    "        r_dict = self.features(realFFT, y, seg_id)\n",
    "        i_dict = self.features(imagFFT, y, seg_id)\n",
    "        \n",
    "        for k, v in r_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'fftr_{k}'] = v\n",
    "                \n",
    "        for k, v in i_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'ffti_{k}'] = v\n",
    "        '''\n",
    "        return main_dict\n",
    "        \n",
    "    \n",
    "    def features(self, x, y, seg_id):\n",
    "        feature_dict = dict()\n",
    "        feature_dict['target'] = y\n",
    "        feature_dict['seg_id'] = seg_id\n",
    "\n",
    "        # create features here\n",
    "\n",
    "        # lists with parameters to iterate over them\n",
    "        #percentiles = [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]\n",
    "        percentiles = [10, 20]\n",
    "        hann_windows = [50, 150, 1500, 15000]\n",
    "        spans = [300, 3000, 30000, 50000]\n",
    "        windows = [10, 50, 100, 500, 1000, 10000]\n",
    "        borders = list(range(-4000, 4001, 1000))\n",
    "        #peaks = [10, 20, 50, 100]\n",
    "        peaks = [10]\n",
    "        coefs = [1, 5, 10, 50, 100]\n",
    "        lags = [10, 100, 1000, 10000]\n",
    "        #autocorr_lags = [5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "        autocorr_lags = [5]\n",
    "        # basic stats\n",
    "        feature_dict['mean'] = x.mean()\n",
    "        feature_dict['std'] = x.std()\n",
    "        feature_dict['max'] = x.max()\n",
    "        feature_dict['min'] = x.min()\n",
    "        \n",
    "        for i, e_acc in enumerate(get_extremal_accelerations(x, num_of_extremals=6)):\n",
    "            feature_dict[f'e_acc_{i}'] = e_acc\n",
    "\n",
    "        '''\n",
    "        # basic stats on absolute values\n",
    "        feature_dict['mean_change_abs'] = np.mean(np.diff(x))\n",
    "        feature_dict['abs_max'] = np.abs(x).max()\n",
    "        feature_dict['abs_mean'] = np.abs(x).mean()\n",
    "        feature_dict['abs_std'] = np.abs(x).std()\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # geometric and harminic means\n",
    "        '''\n",
    "        feature_dict['hmean'] = stats.hmean(np.abs(x[np.nonzero(x)[0]]))\n",
    "        feature_dict['gmean'] = stats.gmean(np.abs(x[np.nonzero(x)[0]])) \n",
    "\n",
    "        # k-statistic and moments\n",
    "        for i in range(1, 5):\n",
    "            feature_dict[f'kstat_{i}'] = stats.kstat(x, i)\n",
    "            feature_dict[f'moment_{i}'] = stats.moment(x, i)\n",
    "\n",
    "        for i in [1, 2]:\n",
    "            feature_dict[f'kstatvar_{i}'] = stats.kstatvar(x, i)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # aggregations on various slices of data\n",
    "        for agg_type, slice_length, direction in product(['std', 'min', 'max', 'mean'], [1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[:slice_length].agg(agg_type)\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[-slice_length:].agg(agg_type)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''\n",
    "        feature_dict['max_to_min'] = x.max() / np.abs(x.min())\n",
    "        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "        feature_dict['count_big'] = len(x[np.abs(x) > 500])\n",
    "        feature_dict['sum'] = x.sum()\n",
    "\n",
    "        feature_dict['mean_change_rate'] = calc_change_rate(x)\n",
    "        # calc_change_rate on slices of data\n",
    "        for slice_length, direction in product([1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[:slice_length])\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[-slice_length:])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # percentiles on original and absolute values\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'percentile_{p}'] = np.percentile(x, p)\n",
    "            feature_dict[f'abs_percentile_{p}'] = np.percentile(np.abs(x), p)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        feature_dict['trend'] = add_trend_feature(x)\n",
    "        feature_dict['abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "        '''\n",
    "\n",
    "        feature_dict['mad'] = x.mad()\n",
    "        feature_dict['kurt'] = x.kurtosis()\n",
    "        feature_dict['skew'] = x.skew()\n",
    "        feature_dict['med'] = x.median()\n",
    "\n",
    "        '''\n",
    "        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "\n",
    "        for hw in hann_windows:\n",
    "            feature_dict[f'Hann_window_mean_{hw}'] = (convolve(x, hann(hw), mode='same') / sum(hann(hw))).mean()\n",
    "\n",
    "        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
    "        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
    "        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
    "        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # exponential rolling statistics\n",
    "        ewma = pd.Series.ewm\n",
    "        for s in spans:\n",
    "            feature_dict[f'exp_Moving_average_{s}_mean'] = (ewma(x, span=s).mean(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_average_{s}_std'] = (ewma(x, span=s).mean(skipna=True)).std(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_mean'] = (ewma(x, span=s).std(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_std'] = (ewma(x, span=s).std(skipna=True)).std(skipna=True)\n",
    "\n",
    "        feature_dict['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "        feature_dict['iqr1'] = np.subtract(*np.percentile(x, [95, 5]))\n",
    "        feature_dict['ave10'] = stats.trim_mean(x, 0.1)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for slice_length, threshold in product([50000, 100000, 150000],\n",
    "                                                     [5, 10, 20, 50, 100]):\n",
    "            feature_dict[f'count_big_{slice_length}_threshold_{threshold}'] = (np.abs(x[-slice_length:]) > threshold).sum()\n",
    "            feature_dict[f'count_big_{slice_length}_less_threshold_{threshold}'] = (np.abs(x[-slice_length:]) < threshold).sum()\n",
    "\n",
    "        # tfresh features take too long to calculate, so I comment them for now\n",
    "\n",
    "#         feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
    "#         feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
    "#         feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
    "#         feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
    "#         feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
    "#         feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
    "#         feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
    "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
    "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for i, j in zip(borders, borders[1:]):\n",
    "            feature_dict[f'range_{i}_{j}'] = feature_calculators.range_count(x, i, j)\n",
    "        '''\n",
    "\n",
    "#         feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
    "#         feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
    "#         feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
    "#         feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
    "#         feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
    "\n",
    "#         for lag in lags:\n",
    "#             feature_dict[f'time_rev_asym_stat_{lag}'] = feature_calculators.time_reversal_asymmetry_statistic(x, lag)\n",
    "        ## for autocorr_lag in autocorr_lags:\n",
    "        ##    feature_dict[f'autocorrelation_{autocorr_lag}'] = feature_calculators.autocorrelation(x, autocorr_lag)\n",
    "        ##    #feature_dict[f'c3_{autocorr_lag}'] = feature_calculators.c3(x, autocorr_lag)\n",
    "\n",
    "#         for coeff, attr in product([1, 2, 3, 4, 5], ['real', 'imag', 'angle']):\n",
    "#             feature_dict[f'fft_{coeff}_{attr}'] = list(feature_calculators.fft_coefficient(x, [{'coeff': coeff, 'attr': attr}]))[0][1]\n",
    "\n",
    "#         feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
    "#         feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
    "#         feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
    "#         feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
    "        \n",
    "    \n",
    "        '''\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'binned_entropy_{p}'] = feature_calculators.binned_entropy(x, p)\n",
    "\n",
    "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
    "        '''\n",
    "        \n",
    "        ## for peak in peaks:\n",
    "        ##    feature_dict[f'num_peaks_{peak}'] = feature_calculators.number_peaks(x, peak)\n",
    "        \n",
    "        '''\n",
    "        for c in coefs:\n",
    "            feature_dict[f'spkt_welch_density_{c}'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': c}]))[0][1]\n",
    "            feature_dict[f'time_rev_asym_stat_{c}'] = feature_calculators.time_reversal_asymmetry_statistic(x, c)  \n",
    "        '''\n",
    "        \n",
    "        # statistics on rolling windows of various sizes\n",
    "        for w in windows:\n",
    "            pass\n",
    "            ## x_roll_std = x.rolling(w).std().dropna().values\n",
    "            ## x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "            \n",
    "            \n",
    "            #feature_dict[f'ave_roll_std_{w}'] = x_roll_std.mean()\n",
    "            #feature_dict[f'std_roll_std_{w}'] = x_roll_std.std()\n",
    "            #feature_dict[f'max_roll_std_{w}'] = x_roll_std.max()\n",
    "            \n",
    "            ## feature_dict[f'min_roll_std_{w}'] = x_roll_std.min()\n",
    "            \n",
    "\n",
    "            ## for p in percentiles:\n",
    "            ##    feature_dict[f'percentile_roll_std_{p}_window_{w}'] = np.percentile(x_roll_std, p)\n",
    "            \n",
    "            '''\n",
    "            feature_dict[f'av_change_abs_roll_std_{w}'] = np.mean(np.diff(x_roll_std))\n",
    "            feature_dict[f'av_change_rate_roll_std_{w}'] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_std_{w}'] = np.abs(x_roll_std).max()\n",
    "\n",
    "            feature_dict[f'ave_roll_mean_{w}'] = x_roll_mean.mean()\n",
    "            feature_dict[f'std_roll_mean_{w}'] = x_roll_mean.std()\n",
    "            feature_dict[f'max_roll_mean_{w}'] = x_roll_mean.max()\n",
    "            feature_dict[f'min_roll_mean_{w}'] = x_roll_mean.min()\n",
    "            \n",
    "            for p in percentiles:\n",
    "                feature_dict[f'percentile_roll_mean_{p}_window_{w}'] = np.percentile(x_roll_mean, p)\n",
    "\n",
    "            feature_dict[f'av_change_abs_roll_mean_{w}'] = np.mean(np.diff(x_roll_mean))\n",
    "            feature_dict[f'av_change_rate_roll_mean_{w}'] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_mean_{w}'] = np.abs(x_roll_mean).max()    \n",
    "            '''\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def generate(self):\n",
    "        feature_list = []\n",
    "        res = Parallel(n_jobs=self.n_jobs,\n",
    "                       backend='threading')(delayed(self.get_features)(x, y, s)\n",
    "                                            for s, x, y in tqdm_notebook(self.read_chunks(), total=self.total_data))\n",
    "        #print(\"FeatureGenerator, generate, type(res)\", type(res))\n",
    "        #print(\"FeatureGenerator, generate, len(res)\", len(res))\n",
    "        for r in res:\n",
    "            feature_list.append(r)\n",
    "        #print(\"FeatureGenerator, generate, len(feature_list)\", len(feature_list))\n",
    "        return pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "train_X_0 = pd.read_csv(\"../input/lanl-masters-features-creating-0/train_X_features_865.csv\")\n",
    "train_X_1 = pd.read_csv(\"../input/lanl-masters-features-creating-1/train_X_features_865.csv\")\n",
    "y_0 = pd.read_csv(\"../input/lanl-masters-features-creating-0/train_y.csv\", index_col=False,  header=None)\n",
    "y_1 = pd.read_csv(\"../input/lanl-masters-features-creating-1/train_y.csv\", index_col=False,  header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_X = pd.concat([train_X_0, train_X_1], axis=0)\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = pd.concat([y_0, y_1], axis=0)\n",
    "y = y.reset_index(drop=True)\n",
    "y[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_y = pd.Series(y[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "test_X = pd.read_csv(\"../input/lanl-masters-features-creating-0/test_X_features_10.csv\")\n",
    "'''\n",
    "del X[\"seg_id\"], test_X[\"seg_id\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84eac8a74794fc791d390191b20f703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41943), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No axis named acoustic_data for object type <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-208186cfe7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_fg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_fg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-c50a7896c13e>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         res = Parallel(n_jobs=self.n_jobs,\n\u001b[1;32m    292\u001b[0m                        \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                                             for s, x, y in tqdm_notebook(self.read_chunks(), total=self.total_data))\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m#print(\"FeatureGenerator, generate, type(res)\", type(res))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m#print(\"FeatureGenerator, generate, len(res)\", len(res))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;31m# We capture the KeyboardInterrupt and reraise it as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-c50a7896c13e>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, x, y, seg_id)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         '''\n",
      "\u001b[0;32m<ipython-input-69-c50a7896c13e>\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, x, y, seg_id)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_acc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_extremal_accelerations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_extremals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'e_acc_{i}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-8b12bb5a12e2>\u001b[0m in \u001b[0;36mget_extremal_accelerations\u001b[0;34m(df, sort_field_name, num_of_extremals)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_extremal_accelerations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_field_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_extremals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msorted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_field_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextremal_accelerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_extremals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0midx_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   2444\u001b[0m         \"\"\"\n\u001b[1;32m   2445\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inplace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0;31m# GH 5856/5853\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         raise ValueError('No axis named {0} for object type {1}'\n\u001b[0;32m--> 375\u001b[0;31m                          .format(axis, type(self)))\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No axis named acoustic_data for object type <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "training_fg = FeatureGenerator(dtype='train', n_jobs=20, chunk_size=15000)\n",
    "\n",
    "\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=20, chunk_size=150000)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2624, 8) (41944, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kurt</th>\n",
       "      <th>mad</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>med</th>\n",
       "      <th>min</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.406174</td>\n",
       "      <td>5.293493</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.083667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>-0.131321</td>\n",
       "      <td>9.648327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.195286</td>\n",
       "      <td>2.849929</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.939733</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-0.039745</td>\n",
       "      <td>3.933370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.026044</td>\n",
       "      <td>3.739596</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.931600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>5.220505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.339573</td>\n",
       "      <td>2.930669</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.761800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.176724</td>\n",
       "      <td>3.950776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.269695</td>\n",
       "      <td>3.419978</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.143667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-0.055047</td>\n",
       "      <td>5.301783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        kurt       mad    max      mean  med   min      skew       std\n",
       "0  21.406174  5.293493  104.0  5.083667  5.0 -98.0 -0.131321  9.648327\n",
       "1   4.195286  2.849929   30.0  4.939733  5.0 -20.0 -0.039745  3.933370\n",
       "2   4.026044  3.739596   40.0  4.931600  5.0 -35.0  0.000936  5.220505\n",
       "3   3.339573  2.930669   34.0  4.761800  5.0 -19.0  0.176724  3.950776\n",
       "4  14.269695  3.419978   52.0  5.143667  5.0 -56.0 -0.055047  5.301783"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X.loc[X[col] != -np.inf, col].mean()\n",
    "        X.loc[X[col] == -np.inf, col] = mean_value\n",
    "        X[col] = X[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#train_columns = train_X.columns\n",
    "\n",
    "#train_X[train_columns] = scaler.fit_transform(train_X[train_columns])\n",
    "#test_X[train_columns] = scaler.transform(test_X[train_columns])\n",
    "\n",
    "train_X = scaler.fit_transform(X)\n",
    "test_X = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = train_X.columns\n",
    "\n",
    "# df = pd.concat([train_X, test_X[cols]], axis=0)\n",
    "# df = df.reset_index(drop=True)\n",
    "# df[cols] = np.round(df.values, 3)\n",
    "\n",
    "# for col in cols:\n",
    "#     df[col+\"_count\"] = df[col].map(df[col].value_counts())\n",
    "    \n",
    "# count_cols = [i for i in df.columns if \"_count\" in i]\n",
    "\n",
    "# train_X = pd.concat([train_X, df.loc[:40000-1, count_cols]], axis=1)\n",
    "# test_X = pd.concat([test_X, df.loc[40000:, count_cols].reset_index(drop=True)], axis=1)\n",
    "# train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_columns = train_X.columns\n",
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "'''\n",
    " if PREDICTION:\n",
    " '''\n",
    "predictions = np.zeros(len(test_X))\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#run model\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "\n",
    "    X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    model = CatBoostRegressor(n_estimators=25000, verbose=-1, objective=\"MAE\", loss_function=\"MAE\", boosting_type=\"Ordered\", task_type=\"GPU\")\n",
    "    model.fit(X_tr, \n",
    "              y_tr, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              '''\n",
    "              eval_metric='mae',\n",
    "              '''\n",
    "              verbose=2500, \n",
    "              early_stopping_rounds=500)\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    #feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    #predictions\n",
    "    #if PREDICTION:\n",
    "\n",
    "    predictions += model.predict(test_X[train_columns]) / folds.n_splits\n",
    "    train_score.append(model.best_score_['learn'][\"MAE\"])\n",
    "\n",
    "cv_score = mean_absolute_error(train_y, oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = predictions\n",
    "submission.to_csv(f'CatBoost_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2624,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed May 29 08:48:12 2019\n",
      "[0]\ttrain-mae:5.06683\tvalid_data-mae:5.04653\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[88]\ttrain-mae:2.03006\tvalid_data-mae:2.39734\n",
      "\n",
      "Fold 1 started at Wed May 29 08:48:19 2019\n",
      "[0]\ttrain-mae:5.07537\tvalid_data-mae:5.01251\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[86]\ttrain-mae:2.02799\tvalid_data-mae:2.4139\n",
      "\n",
      "Fold 2 started at Wed May 29 08:48:26 2019\n",
      "[0]\ttrain-mae:5.0551\tvalid_data-mae:5.09846\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[100]\ttrain-mae:2.00676\tvalid_data-mae:2.41033\n",
      "\n",
      "Fold 3 started at Wed May 29 08:48:33 2019\n",
      "[0]\ttrain-mae:5.05116\tvalid_data-mae:5.11292\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[96]\ttrain-mae:1.99547\tvalid_data-mae:2.4633\n",
      "\n",
      "Fold 4 started at Wed May 29 08:48:41 2019\n",
      "[0]\ttrain-mae:5.06473\tvalid_data-mae:5.05607\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[94]\ttrain-mae:2.00943\tvalid_data-mae:2.40686\n",
      "\n",
      "CV mean score: 2.4183, std: 0.0231.\n",
      "CPU times: user 2min 21s, sys: 8 ms, total: 2min 21s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_params = {'eta': 0.03, 'max_depth': 10, 'subsample': 0.85, #'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 4}\n",
    "oof_xgb, prediction_xgb, scores = train_model(X, X_test, y, params=xgb_params, model_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed May 29 08:49:05 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8325]\ttraining's l1: 2.4378\tvalid_1's l1: 2.53658\n",
      "Fold 1 started at Wed May 29 08:49:59 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[10000]\ttraining's l1: 2.42998\tvalid_1's l1: 2.54096\n",
      "Early stopping, best iteration is:\n",
      "[10917]\ttraining's l1: 2.42832\tvalid_1's l1: 2.53885\n",
      "Fold 2 started at Wed May 29 08:51:16 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9374]\ttraining's l1: 2.43139\tvalid_1's l1: 2.52999\n",
      "Fold 3 started at Wed May 29 08:52:20 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9360]\ttraining's l1: 2.41964\tvalid_1's l1: 2.59327\n",
      "Fold 4 started at Wed May 29 08:53:25 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7252]\ttraining's l1: 2.43803\tvalid_1's l1: 2.54065\n",
      "CV mean score: 2.5480, std: 0.0230.\n",
      "CPU times: user 19min 28s, sys: 19.8 s, total: 19min 48s\n",
      "Wall time: 5min 8s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'num_leaves': 128,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.2\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(\n",
    "    X,\n",
    "    X_test,\n",
    "    y,\n",
    "    params=params,\n",
    "    model_type='lgb',\n",
    "    plot_feature_importance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim=10):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(256, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(96, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #'rmsprop'\n",
    "    model.compile(optimizer=optimizer,loss='mae')\n",
    "    return model\n",
    "\n",
    "patience = 50\n",
    "call_ES = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=patience,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    #restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='rmsprop', loss='mae')\\n\\nhistory = model.fit_generator(train_gen,\\n                              steps_per_epoch=1000,\\n                              epochs=40,\\n                              verbose=2,\\n                              callbacks=cb,\\n                              validation_data=valid_gen,\\n                              validation_steps=200\\n                             )\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "def create_model(input_dim=10):\n",
    "\n",
    "    # The LSTM architecture\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with Dropout regularisation\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=(None, input_dim)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Second LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Fourth LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    # The output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop', loss='mae')\n",
    "    return model\n",
    "\n",
    "    # Compile and fit model\n",
    "'''\n",
    "model.compile(optimizer='rmsprop', loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=40,\n",
    "                              verbose=2,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200\n",
    "                             )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(41944, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape[-1])\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37751595,  0.50450393,  0.41568109,  1.89713401,  0.95679979,\n",
       "        -0.47351687, -0.6322583 ,  0.43445122],\n",
       "       [-0.62443566, -0.17494832, -0.32425303,  1.41315073,  0.95679979,\n",
       "         0.32830551, -0.31991565, -0.19392467],\n",
       "       [-0.63428828,  0.07243066, -0.22426193,  1.38580197,  0.95679979,\n",
       "         0.17410889, -0.18116013, -0.05240048],\n",
       "       [-0.67425198, -0.15249782, -0.28425659,  0.81484068,  0.95679979,\n",
       "         0.33858528,  0.41840853, -0.19201077],\n",
       "       [-0.03794237, -0.01644181, -0.10427262,  2.09888712,  0.95679979,\n",
       "        -0.04176636, -0.37210531, -0.04346375]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, None, 50)          12000     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_11 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_12 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 73,251\n",
      "Trainable params: 73,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33555 samples, validate on 8389 samples\n",
      "Epoch 1/50\n",
      "33555/33555 [==============================] - 14s 404us/step - loss: 2.7799 - val_loss: 2.4983\n",
      "Epoch 2/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.5487 - val_loss: 2.4756\n",
      "Epoch 3/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.5242 - val_loss: 2.4460\n",
      "Epoch 4/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.5074 - val_loss: 2.4296\n",
      "Epoch 5/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.4983 - val_loss: 2.4531\n",
      "Epoch 6/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4957 - val_loss: 2.4256\n",
      "Epoch 7/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4886 - val_loss: 2.4181\n",
      "Epoch 8/50\n",
      "33555/33555 [==============================] - 12s 355us/step - loss: 2.4798 - val_loss: 2.4373\n",
      "Epoch 9/50\n",
      "33555/33555 [==============================] - 12s 354us/step - loss: 2.4842 - val_loss: 2.4121\n",
      "Epoch 10/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4800 - val_loss: 2.4095\n",
      "Epoch 11/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4733 - val_loss: 2.4064\n",
      "Epoch 12/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4670 - val_loss: 2.4041\n",
      "Epoch 13/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4663 - val_loss: 2.4166\n",
      "Epoch 14/50\n",
      "33555/33555 [==============================] - 13s 375us/step - loss: 2.4624 - val_loss: 2.4569\n",
      "Epoch 15/50\n",
      "33555/33555 [==============================] - 13s 377us/step - loss: 2.4532 - val_loss: 2.3953\n",
      "Epoch 16/50\n",
      "33555/33555 [==============================] - 13s 374us/step - loss: 2.4524 - val_loss: 2.4039\n",
      "Epoch 17/50\n",
      "33555/33555 [==============================] - 13s 376us/step - loss: 2.4505 - val_loss: 2.3896\n",
      "Epoch 18/50\n",
      "33555/33555 [==============================] - 13s 373us/step - loss: 2.4543 - val_loss: 2.4013\n",
      "Epoch 19/50\n",
      "33555/33555 [==============================] - 12s 372us/step - loss: 2.4452 - val_loss: 2.4004\n",
      "Epoch 20/50\n",
      "33555/33555 [==============================] - 13s 379us/step - loss: 2.4475 - val_loss: 2.3804\n",
      "Epoch 21/50\n",
      "33555/33555 [==============================] - 13s 376us/step - loss: 2.4366 - val_loss: 2.3799\n",
      "Epoch 22/50\n",
      "33555/33555 [==============================] - 12s 372us/step - loss: 2.4405 - val_loss: 2.3911\n",
      "Epoch 23/50\n",
      "33555/33555 [==============================] - 13s 377us/step - loss: 2.4342 - val_loss: 2.3900\n",
      "Epoch 24/50\n",
      "33555/33555 [==============================] - 13s 384us/step - loss: 2.4316 - val_loss: 2.3839\n",
      "Epoch 25/50\n",
      "33555/33555 [==============================] - 13s 376us/step - loss: 2.4364 - val_loss: 2.3944\n",
      "Epoch 26/50\n",
      "33555/33555 [==============================] - 13s 374us/step - loss: 2.4322 - val_loss: 2.3707\n",
      "Epoch 27/50\n",
      "33555/33555 [==============================] - 13s 373us/step - loss: 2.4286 - val_loss: 2.3754\n",
      "Epoch 28/50\n",
      "33555/33555 [==============================] - 13s 378us/step - loss: 2.4269 - val_loss: 2.3700\n",
      "Epoch 29/50\n",
      "33555/33555 [==============================] - 12s 372us/step - loss: 2.4276 - val_loss: 2.3886\n",
      "Epoch 30/50\n",
      "33555/33555 [==============================] - 13s 380us/step - loss: 2.4255 - val_loss: 2.3882\n",
      "Epoch 31/50\n",
      "33555/33555 [==============================] - 12s 369us/step - loss: 2.4269 - val_loss: 2.3670\n",
      "Epoch 32/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4235 - val_loss: 2.3607\n",
      "Epoch 33/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4200 - val_loss: 2.3812\n",
      "Epoch 34/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4232 - val_loss: 2.3600\n",
      "Epoch 35/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4221 - val_loss: 2.3739\n",
      "Epoch 36/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4180 - val_loss: 2.3676\n",
      "Epoch 37/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4215 - val_loss: 2.3828\n",
      "Epoch 38/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4177 - val_loss: 2.3683\n",
      "Epoch 39/50\n",
      "33555/33555 [==============================] - 12s 366us/step - loss: 2.4163 - val_loss: 2.3632\n",
      "Epoch 40/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4152 - val_loss: 2.3681\n",
      "Epoch 41/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4146 - val_loss: 2.3599\n",
      "Epoch 42/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4150 - val_loss: 2.3796\n",
      "Epoch 43/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4160 - val_loss: 2.3589\n",
      "Epoch 44/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4080 - val_loss: 2.3548\n",
      "Epoch 45/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4099 - val_loss: 2.3802\n",
      "Epoch 46/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4144 - val_loss: 2.3715\n",
      "Epoch 47/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4084 - val_loss: 2.3703\n",
      "Epoch 48/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4059 - val_loss: 2.3619\n",
      "Epoch 49/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4064 - val_loss: 2.3534\n",
      "Epoch 50/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4133 - val_loss: 2.3521\n",
      "loss: 2.780 | val_loss: 2.498 | diff: -0.282\n",
      "fold 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_13 (CuDNNLSTM)    (None, None, 50)          12000     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_14 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 73,251\n",
      "Trainable params: 73,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33555 samples, validate on 8389 samples\n",
      "Epoch 1/50\n",
      "33555/33555 [==============================] - 14s 403us/step - loss: 2.7718 - val_loss: 2.4937\n",
      "Epoch 2/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.5555 - val_loss: 2.4652\n",
      "Epoch 3/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.5194 - val_loss: 2.4654\n",
      "Epoch 4/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.5050 - val_loss: 2.4516\n",
      "Epoch 5/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4936 - val_loss: 2.4354\n",
      "Epoch 6/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.4874 - val_loss: 2.4329\n",
      "Epoch 7/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4769 - val_loss: 2.4367\n",
      "Epoch 8/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4748 - val_loss: 2.4294\n",
      "Epoch 9/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4720 - val_loss: 2.4209\n",
      "Epoch 10/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4687 - val_loss: 2.4227\n",
      "Epoch 11/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4667 - val_loss: 2.4246\n",
      "Epoch 12/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.4676 - val_loss: 2.4407\n",
      "Epoch 13/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4622 - val_loss: 2.4111\n",
      "Epoch 14/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4565 - val_loss: 2.4181\n",
      "Epoch 15/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4567 - val_loss: 2.4009\n",
      "Epoch 16/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4516 - val_loss: 2.4245\n",
      "Epoch 17/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4468 - val_loss: 2.4029\n",
      "Epoch 18/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4395 - val_loss: 2.3938\n",
      "Epoch 19/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4459 - val_loss: 2.4018\n",
      "Epoch 20/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4348 - val_loss: 2.3980\n",
      "Epoch 21/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4337 - val_loss: 2.3954\n",
      "Epoch 22/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4320 - val_loss: 2.3942\n",
      "Epoch 23/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4314 - val_loss: 2.3990\n",
      "Epoch 24/50\n",
      "33555/33555 [==============================] - 12s 366us/step - loss: 2.4377 - val_loss: 2.4067\n",
      "Epoch 25/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4292 - val_loss: 2.4005\n",
      "Epoch 26/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4223 - val_loss: 2.4140\n",
      "Epoch 27/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.4264 - val_loss: 2.3971\n",
      "Epoch 28/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4252 - val_loss: 2.3974\n",
      "Epoch 29/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4227 - val_loss: 2.3866\n",
      "Epoch 30/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4214 - val_loss: 2.3830\n",
      "Epoch 31/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4172 - val_loss: 2.4111\n",
      "Epoch 32/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4167 - val_loss: 2.3879\n",
      "Epoch 33/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4163 - val_loss: 2.4011\n",
      "Epoch 34/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4156 - val_loss: 2.3829\n",
      "Epoch 35/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.4165 - val_loss: 2.3997\n",
      "Epoch 36/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4155 - val_loss: 2.3751\n",
      "Epoch 37/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4129 - val_loss: 2.3741\n",
      "Epoch 38/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4162 - val_loss: 2.3729\n",
      "Epoch 39/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4064 - val_loss: 2.3902\n",
      "Epoch 40/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.4090 - val_loss: 2.3847\n",
      "Epoch 41/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4120 - val_loss: 2.3871\n",
      "Epoch 42/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4062 - val_loss: 2.3738\n",
      "Epoch 43/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.4081 - val_loss: 2.3715\n",
      "Epoch 44/50\n",
      "33555/33555 [==============================] - 12s 369us/step - loss: 2.4079 - val_loss: 2.3706\n",
      "Epoch 45/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4042 - val_loss: 2.3680\n",
      "Epoch 46/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4074 - val_loss: 2.3851\n",
      "Epoch 47/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4033 - val_loss: 2.3748\n",
      "Epoch 48/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4064 - val_loss: 2.3887\n",
      "Epoch 49/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.4069 - val_loss: 2.3663\n",
      "Epoch 50/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4002 - val_loss: 2.3881\n",
      "loss: 2.772 | val_loss: 2.494 | diff: -0.278\n",
      "fold 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, None, 50)          12000     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 73,251\n",
      "Trainable params: 73,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33555 samples, validate on 8389 samples\n",
      "Epoch 1/50\n",
      "33555/33555 [==============================] - 14s 420us/step - loss: 2.7744 - val_loss: 2.5011\n",
      "Epoch 2/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.5438 - val_loss: 2.4773\n",
      "Epoch 3/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.5177 - val_loss: 2.4582\n",
      "Epoch 4/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.5088 - val_loss: 2.4628\n",
      "Epoch 5/50\n",
      "33555/33555 [==============================] - 12s 369us/step - loss: 2.4954 - val_loss: 2.4514\n",
      "Epoch 6/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4881 - val_loss: 2.4429\n",
      "Epoch 7/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4794 - val_loss: 2.4344\n",
      "Epoch 8/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4748 - val_loss: 2.4444\n",
      "Epoch 9/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4795 - val_loss: 2.4406\n",
      "Epoch 10/50\n",
      "33555/33555 [==============================] - 12s 369us/step - loss: 2.4714 - val_loss: 2.4406\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33555/33555 [==============================] - 14s 405us/step - loss: 2.4688 - val_loss: 2.4365\n",
      "Epoch 12/50\n",
      "33555/33555 [==============================] - 13s 395us/step - loss: 2.4662 - val_loss: 2.4355\n",
      "Epoch 13/50\n",
      "33555/33555 [==============================] - 13s 374us/step - loss: 2.4631 - val_loss: 2.4198\n",
      "Epoch 14/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4588 - val_loss: 2.4239\n",
      "Epoch 15/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4526 - val_loss: 2.4130\n",
      "Epoch 16/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4539 - val_loss: 2.4152\n",
      "Epoch 17/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4517 - val_loss: 2.4150\n",
      "Epoch 18/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4460 - val_loss: 2.4019\n",
      "Epoch 19/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4463 - val_loss: 2.4016\n",
      "Epoch 20/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4442 - val_loss: 2.4152\n",
      "Epoch 21/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.4456 - val_loss: 2.3993\n",
      "Epoch 22/50\n",
      "33555/33555 [==============================] - 12s 367us/step - loss: 2.4371 - val_loss: 2.4031\n",
      "Epoch 23/50\n",
      "33555/33555 [==============================] - 12s 366us/step - loss: 2.4363 - val_loss: 2.4003\n",
      "Epoch 24/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4294 - val_loss: 2.4077\n",
      "Epoch 25/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4305 - val_loss: 2.4175\n",
      "Epoch 26/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4327 - val_loss: 2.3920\n",
      "Epoch 27/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4328 - val_loss: 2.3837\n",
      "Epoch 28/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4258 - val_loss: 2.3871\n",
      "Epoch 29/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4233 - val_loss: 2.3902\n",
      "Epoch 30/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4220 - val_loss: 2.3974\n",
      "Epoch 31/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4199 - val_loss: 2.3861\n",
      "Epoch 32/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4196 - val_loss: 2.4144\n",
      "Epoch 33/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4209 - val_loss: 2.3806\n",
      "Epoch 34/50\n",
      "33555/33555 [==============================] - 13s 375us/step - loss: 2.4185 - val_loss: 2.3794\n",
      "Epoch 35/50\n",
      "33555/33555 [==============================] - 13s 383us/step - loss: 2.4191 - val_loss: 2.4282\n",
      "Epoch 36/50\n",
      "33555/33555 [==============================] - 13s 373us/step - loss: 2.4097 - val_loss: 2.3864\n",
      "Epoch 37/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4113 - val_loss: 2.3728\n",
      "Epoch 38/50\n",
      "33555/33555 [==============================] - 13s 382us/step - loss: 2.4136 - val_loss: 2.3956\n",
      "Epoch 39/50\n",
      "33555/33555 [==============================] - 13s 377us/step - loss: 2.4143 - val_loss: 2.3813\n",
      "Epoch 40/50\n",
      "33555/33555 [==============================] - 13s 379us/step - loss: 2.4113 - val_loss: 2.3805\n",
      "Epoch 41/50\n",
      "33555/33555 [==============================] - 13s 380us/step - loss: 2.4068 - val_loss: 2.3745\n",
      "Epoch 42/50\n",
      "33555/33555 [==============================] - 12s 372us/step - loss: 2.4085 - val_loss: 2.3780\n",
      "Epoch 43/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4122 - val_loss: 2.3929\n",
      "Epoch 44/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4048 - val_loss: 2.3686\n",
      "Epoch 45/50\n",
      "33555/33555 [==============================] - 12s 353us/step - loss: 2.4073 - val_loss: 2.3738\n",
      "Epoch 46/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4062 - val_loss: 2.3818\n",
      "Epoch 47/50\n",
      "33555/33555 [==============================] - 12s 366us/step - loss: 2.4072 - val_loss: 2.3678\n",
      "Epoch 48/50\n",
      "33555/33555 [==============================] - 12s 364us/step - loss: 2.4046 - val_loss: 2.3716\n",
      "Epoch 49/50\n",
      "33555/33555 [==============================] - 12s 354us/step - loss: 2.4089 - val_loss: 2.3641\n",
      "Epoch 50/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4054 - val_loss: 2.3844\n",
      "loss: 2.774 | val_loss: 2.501 | diff: -0.273\n",
      "fold 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_21 (CuDNNLSTM)    (None, None, 50)          12000     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_22 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_23 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_24 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 73,251\n",
      "Trainable params: 73,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33555 samples, validate on 8389 samples\n",
      "Epoch 1/50\n",
      "33555/33555 [==============================] - 14s 412us/step - loss: 2.7743 - val_loss: 2.5707\n",
      "Epoch 2/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.5307 - val_loss: 2.5414\n",
      "Epoch 3/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.5045 - val_loss: 2.5084\n",
      "Epoch 4/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4935 - val_loss: 2.5029\n",
      "Epoch 5/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4794 - val_loss: 2.4955\n",
      "Epoch 6/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4749 - val_loss: 2.4966\n",
      "Epoch 7/50\n",
      "33555/33555 [==============================] - 12s 354us/step - loss: 2.4671 - val_loss: 2.4956\n",
      "Epoch 8/50\n",
      "33555/33555 [==============================] - 12s 354us/step - loss: 2.4675 - val_loss: 2.4961\n",
      "Epoch 9/50\n",
      "33555/33555 [==============================] - 12s 355us/step - loss: 2.4582 - val_loss: 2.4880\n",
      "Epoch 10/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4539 - val_loss: 2.4900\n",
      "Epoch 11/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4547 - val_loss: 2.4850\n",
      "Epoch 12/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4513 - val_loss: 2.4726\n",
      "Epoch 13/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4473 - val_loss: 2.4742\n",
      "Epoch 14/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4470 - val_loss: 2.4740\n",
      "Epoch 15/50\n",
      "33555/33555 [==============================] - 12s 358us/step - loss: 2.4390 - val_loss: 2.4688\n",
      "Epoch 16/50\n",
      "33555/33555 [==============================] - 13s 379us/step - loss: 2.4396 - val_loss: 2.4631\n",
      "Epoch 17/50\n",
      "33555/33555 [==============================] - 13s 380us/step - loss: 2.4330 - val_loss: 2.4625\n",
      "Epoch 18/50\n",
      "33555/33555 [==============================] - 12s 362us/step - loss: 2.4287 - val_loss: 2.4642\n",
      "Epoch 19/50\n",
      "33555/33555 [==============================] - 12s 353us/step - loss: 2.4291 - val_loss: 2.4591\n",
      "Epoch 20/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4291 - val_loss: 2.4633\n",
      "Epoch 21/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4258 - val_loss: 2.4597\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4244 - val_loss: 2.4499\n",
      "Epoch 23/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4227 - val_loss: 2.4503\n",
      "Epoch 24/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4184 - val_loss: 2.4963\n",
      "Epoch 25/50\n",
      "33555/33555 [==============================] - 12s 359us/step - loss: 2.4166 - val_loss: 2.4427\n",
      "Epoch 26/50\n",
      "33555/33555 [==============================] - 12s 356us/step - loss: 2.4090 - val_loss: 2.4489\n",
      "Epoch 27/50\n",
      "33555/33555 [==============================] - 12s 366us/step - loss: 2.4171 - val_loss: 2.4380\n",
      "Epoch 28/50\n",
      "33555/33555 [==============================] - 13s 384us/step - loss: 2.4177 - val_loss: 2.4380\n",
      "Epoch 29/50\n",
      "33555/33555 [==============================] - 12s 363us/step - loss: 2.4093 - val_loss: 2.4390\n",
      "Epoch 30/50\n",
      "33555/33555 [==============================] - 12s 357us/step - loss: 2.4160 - val_loss: 2.4356\n",
      "Epoch 31/50\n",
      "33555/33555 [==============================] - 12s 360us/step - loss: 2.4136 - val_loss: 2.4279\n",
      "Epoch 32/50\n",
      "33555/33555 [==============================] - 12s 361us/step - loss: 2.4084 - val_loss: 2.4373\n",
      "Epoch 33/50\n",
      "33555/33555 [==============================] - 13s 381us/step - loss: 2.4042 - val_loss: 2.4449\n",
      "Epoch 34/50\n",
      "33555/33555 [==============================] - 13s 378us/step - loss: 2.4060 - val_loss: 2.4297\n",
      "Epoch 35/50\n",
      "33555/33555 [==============================] - 13s 375us/step - loss: 2.4040 - val_loss: 2.4362\n",
      "Epoch 36/50\n",
      "33555/33555 [==============================] - 13s 380us/step - loss: 2.4016 - val_loss: 2.4225\n",
      "Epoch 37/50\n",
      "33555/33555 [==============================] - 13s 381us/step - loss: 2.4035 - val_loss: 2.4374\n",
      "Epoch 38/50\n",
      "33555/33555 [==============================] - 13s 382us/step - loss: 2.4039 - val_loss: 2.4425\n",
      "Epoch 39/50\n",
      "33555/33555 [==============================] - 13s 378us/step - loss: 2.4006 - val_loss: 2.4180\n",
      "Epoch 40/50\n",
      "33555/33555 [==============================] - 13s 378us/step - loss: 2.4025 - val_loss: 2.4213\n",
      "Epoch 41/50\n",
      "33555/33555 [==============================] - 13s 382us/step - loss: 2.3964 - val_loss: 2.4250\n",
      "Epoch 42/50\n",
      "33555/33555 [==============================] - 13s 385us/step - loss: 2.3965 - val_loss: 2.4545\n",
      "Epoch 43/50\n",
      "33555/33555 [==============================] - 13s 378us/step - loss: 2.3942 - val_loss: 2.4142\n",
      "Epoch 44/50\n",
      "33555/33555 [==============================] - 13s 375us/step - loss: 2.3931 - val_loss: 2.4231\n",
      "Epoch 45/50\n",
      "33555/33555 [==============================] - 12s 369us/step - loss: 2.3981 - val_loss: 2.4170\n",
      "Epoch 46/50\n",
      "33555/33555 [==============================] - 12s 372us/step - loss: 2.3912 - val_loss: 2.4143\n",
      "Epoch 47/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.3968 - val_loss: 2.4416\n",
      "Epoch 48/50\n",
      "33555/33555 [==============================] - 12s 368us/step - loss: 2.3903 - val_loss: 2.4168\n",
      "Epoch 49/50\n",
      "33555/33555 [==============================] - 12s 365us/step - loss: 2.3893 - val_loss: 2.4160\n",
      "Epoch 50/50\n",
      "33555/33555 [==============================] - 12s 370us/step - loss: 2.3932 - val_loss: 2.4149\n",
      "loss: 2.774 | val_loss: 2.571 | diff: -0.204\n",
      "fold 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_25 (CuDNNLSTM)    (None, None, 50)          12000     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_26 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_27 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_28 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 73,251\n",
      "Trainable params: 73,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33556 samples, validate on 8388 samples\n",
      "Epoch 1/50\n",
      "33556/33556 [==============================] - 14s 427us/step - loss: 2.7672 - val_loss: 2.5639\n",
      "Epoch 2/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.5418 - val_loss: 2.4690\n",
      "Epoch 3/50\n",
      "33556/33556 [==============================] - 13s 373us/step - loss: 2.5136 - val_loss: 2.4549\n",
      "Epoch 4/50\n",
      "33556/33556 [==============================] - 13s 384us/step - loss: 2.4976 - val_loss: 2.4508\n",
      "Epoch 5/50\n",
      "33556/33556 [==============================] - 13s 378us/step - loss: 2.4917 - val_loss: 2.4497\n",
      "Epoch 6/50\n",
      "33556/33556 [==============================] - 13s 374us/step - loss: 2.4893 - val_loss: 2.4444\n",
      "Epoch 7/50\n",
      "33556/33556 [==============================] - 13s 376us/step - loss: 2.4780 - val_loss: 2.4366\n",
      "Epoch 8/50\n",
      "33556/33556 [==============================] - 13s 373us/step - loss: 2.4737 - val_loss: 2.4313\n",
      "Epoch 9/50\n",
      "33556/33556 [==============================] - 12s 371us/step - loss: 2.4706 - val_loss: 2.4255\n",
      "Epoch 10/50\n",
      "33556/33556 [==============================] - 12s 371us/step - loss: 2.4646 - val_loss: 2.4267\n",
      "Epoch 11/50\n",
      "33556/33556 [==============================] - 13s 378us/step - loss: 2.4635 - val_loss: 2.4204\n",
      "Epoch 12/50\n",
      "33556/33556 [==============================] - 12s 371us/step - loss: 2.4574 - val_loss: 2.4202\n",
      "Epoch 13/50\n",
      "33556/33556 [==============================] - 12s 364us/step - loss: 2.4563 - val_loss: 2.4185\n",
      "Epoch 14/50\n",
      "33556/33556 [==============================] - 12s 363us/step - loss: 2.4542 - val_loss: 2.4103\n",
      "Epoch 15/50\n",
      "33556/33556 [==============================] - 12s 362us/step - loss: 2.4507 - val_loss: 2.4118\n",
      "Epoch 16/50\n",
      "33556/33556 [==============================] - 12s 358us/step - loss: 2.4439 - val_loss: 2.4063\n",
      "Epoch 17/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4422 - val_loss: 2.4255\n",
      "Epoch 18/50\n",
      "33556/33556 [==============================] - 12s 365us/step - loss: 2.4358 - val_loss: 2.4093\n",
      "Epoch 19/50\n",
      "33556/33556 [==============================] - 12s 364us/step - loss: 2.4342 - val_loss: 2.4088\n",
      "Epoch 20/50\n",
      "33556/33556 [==============================] - 12s 367us/step - loss: 2.4311 - val_loss: 2.3934\n",
      "Epoch 21/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4320 - val_loss: 2.4024\n",
      "Epoch 22/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4306 - val_loss: 2.4132\n",
      "Epoch 23/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4286 - val_loss: 2.3954\n",
      "Epoch 24/50\n",
      "33556/33556 [==============================] - 12s 365us/step - loss: 2.4261 - val_loss: 2.3875\n",
      "Epoch 25/50\n",
      "33556/33556 [==============================] - 12s 363us/step - loss: 2.4249 - val_loss: 2.4007\n",
      "Epoch 26/50\n",
      "33556/33556 [==============================] - 12s 363us/step - loss: 2.4205 - val_loss: 2.4046\n",
      "Epoch 27/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4186 - val_loss: 2.3926\n",
      "Epoch 28/50\n",
      "33556/33556 [==============================] - 12s 365us/step - loss: 2.4206 - val_loss: 2.3998\n",
      "Epoch 29/50\n",
      "33556/33556 [==============================] - 12s 372us/step - loss: 2.4191 - val_loss: 2.3771\n",
      "Epoch 30/50\n",
      "33556/33556 [==============================] - 13s 378us/step - loss: 2.4229 - val_loss: 2.3776\n",
      "Epoch 31/50\n",
      "33556/33556 [==============================] - 12s 360us/step - loss: 2.4153 - val_loss: 2.3897\n",
      "Epoch 32/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4140 - val_loss: 2.3983\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33556/33556 [==============================] - 12s 362us/step - loss: 2.4171 - val_loss: 2.3751\n",
      "Epoch 34/50\n",
      "33556/33556 [==============================] - 12s 365us/step - loss: 2.4102 - val_loss: 2.3752\n",
      "Epoch 35/50\n",
      "33556/33556 [==============================] - 12s 362us/step - loss: 2.4130 - val_loss: 2.3785\n",
      "Epoch 36/50\n",
      "33556/33556 [==============================] - 12s 362us/step - loss: 2.4108 - val_loss: 2.3934\n",
      "Epoch 37/50\n",
      "33556/33556 [==============================] - 13s 374us/step - loss: 2.4161 - val_loss: 2.3732\n",
      "Epoch 38/50\n",
      "33556/33556 [==============================] - 12s 362us/step - loss: 2.4117 - val_loss: 2.3863\n",
      "Epoch 39/50\n",
      "33556/33556 [==============================] - 12s 367us/step - loss: 2.4077 - val_loss: 2.3833\n",
      "Epoch 40/50\n",
      "33556/33556 [==============================] - 12s 363us/step - loss: 2.4065 - val_loss: 2.3673\n",
      "Epoch 41/50\n",
      "33556/33556 [==============================] - 12s 365us/step - loss: 2.4072 - val_loss: 2.3968\n",
      "Epoch 42/50\n",
      "33556/33556 [==============================] - 12s 369us/step - loss: 2.4076 - val_loss: 2.3845\n",
      "Epoch 43/50\n",
      "33556/33556 [==============================] - 12s 370us/step - loss: 2.4063 - val_loss: 2.4087\n",
      "Epoch 44/50\n",
      "33556/33556 [==============================] - 12s 368us/step - loss: 2.4074 - val_loss: 2.3692\n",
      "Epoch 45/50\n",
      "33556/33556 [==============================] - 12s 369us/step - loss: 2.4030 - val_loss: 2.3899\n",
      "Epoch 46/50\n",
      "33556/33556 [==============================] - 12s 366us/step - loss: 2.4017 - val_loss: 2.4218\n",
      "Epoch 47/50\n",
      "33556/33556 [==============================] - 13s 373us/step - loss: 2.4026 - val_loss: 2.3725\n",
      "Epoch 48/50\n",
      "33556/33556 [==============================] - 12s 367us/step - loss: 2.4075 - val_loss: 2.3766\n",
      "Epoch 49/50\n",
      "33556/33556 [==============================] - 12s 364us/step - loss: 2.4036 - val_loss: 2.3656\n",
      "Epoch 50/50\n",
      "33556/33556 [==============================] - 12s 369us/step - loss: 2.4056 - val_loss: 2.3624\n",
      "loss: 2.767 | val_loss: 2.564 | diff: -0.203\n",
      "After 5 test_CV = 2.380 | train_CV = 2.774 | -0.393 CPU times: user 1h 1min 41s, sys: 5min 16s, total: 1h 6min 58s\n",
      "Wall time: 51min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "NN_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "NN_predictions = np.zeros(len(test_X))\n",
    "\n",
    "num_of_features = train_X.shape[-1]\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "    \n",
    "    ## X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "    X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "    X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "    y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "    model = create_model(num_of_features)\n",
    "    model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=1, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "    NN_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "    \n",
    "    #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "    test_X = test_X.reshape(len(test_X), 1, num_of_features)\n",
    "    NN_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "    history = model.history.history\n",
    "    tr_loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "    train_score.append(tr_loss[-patience])\n",
    "#     break\n",
    "    \n",
    "cv_score = mean_absolute_error(train_y, NN_oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5248"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NN_predictions) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(NN_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = NN_predictions\n",
    "submission.to_csv(f'NN_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_parameters = {'tournament_size': 17, 'population_size': 4000, 'p_crossover': 0.8, 'generations': 18}\n",
    "# function_set = ('add', 'sub', 'mul', 'div', \"sqrt\", \"log\", \"max\", \"min\", \"sin\", \"cos\", \"tan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # n_fold = 5\n",
    "# folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# GPL_oof = np.zeros(len(train_X))\n",
    "# GPL_predictions = np.zeros(len(test_X))\n",
    "# train_score = []\n",
    "\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n",
    "#     strLog = \"fold {}\".format(fold_)\n",
    "#     print(strLog)\n",
    "    \n",
    "#     X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "#     y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "#     model = SymbolicRegressor(**best_parameters, stopping_criteria=0.0,const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', \n",
    "#                           function_set=function_set, metric='mean absolute error', parsimony_coefficient=0.001,\n",
    "#                           p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, \n",
    "#                           p_point_replace=0.05, max_samples=1.0, feature_names=None, \n",
    "#                           warm_start=False, low_memory=False, n_jobs=-1, verbose=1, random_state=42)\n",
    "    \n",
    "#     model.fit(X_tr, y_tr) #\n",
    "    \n",
    "#     GPL_oof[val_idx] = model.predict(X_val)\n",
    "#     GPL_predictions += model.predict(test_X[train_columns]) / folds.n_splits\n",
    "    \n",
    "#     train_score.append(model.run_details_[\"best_fitness\"][-1])\n",
    "# #     break\n",
    "    \n",
    "# cv_score = mean_absolute_error(train_y, GPL_oof)\n",
    "# print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = str(datetime.date.today())\n",
    "# submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "# submission[\"time_to_failure\"] = GPL_predictions\n",
    "# submission.to_csv(f'GPL_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scirpus_prediction = pd.read_csv(\"../input/andrews-new-script-plus-a-genetic-program-model/gpI.csv\")\n",
    "Scirpus_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = (predictions+NN_predictions+Scirpus_prediction.time_to_failure.values)/3\n",
    "submission.to_csv(f'FINAL_{today}_submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
