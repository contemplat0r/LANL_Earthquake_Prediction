{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow import keras\n",
    "#from gplearn.genetic import SymbolicRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "#import numpy as np \n",
    "#import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, TimeDistributed, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "#seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "#set_random_seed(5944)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with simple derived features\n",
    "\n",
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1]).values\n",
    "    change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return np.mean(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(df, sort_field_name='acoustic_data', num_of_extremals=12):\n",
    "    sorted_df = df.sort_values(sort_field_name)\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_df.index[i]\n",
    "        idx_max = sorted_df.index[-i - 1]\n",
    "        min_v = df.iloc[idx_min][sort_field_name]\n",
    "        max_v = df.iloc[idx_max][sort_field_name]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremal_accelerations(series, num_of_extremals=12):\n",
    "    sorted_series = series.sort_values()\n",
    "    extremal_accelerations = []\n",
    "    for i in range(num_of_extremals):\n",
    "        idx_min = sorted_series.index[i]\n",
    "        idx_max = sorted_series.index[-i - 1]\n",
    "        min_v = series.iloc[idx_min]\n",
    "        max_v = series.iloc[idx_max]\n",
    "        extremal_accelerations.append((\n",
    "            (max_v - min_v) / (idx_max - idx_min)\n",
    "        ))\n",
    "    return extremal_accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dtype = dtype\n",
    "        self.filename = None\n",
    "        self.n_jobs = n_jobs\n",
    "        self.test_files = []\n",
    "        if self.dtype == 'train':\n",
    "            self.filename = '../input/train/train.csv'\n",
    "            self.total_data = int(629145481 / self.chunk_size)\n",
    "            #print(\"Feature Generator __init__, self.total_data:\", self.total_data)\n",
    "        else:\n",
    "            submission = pd.read_csv('../input/sample_submission.csv')\n",
    "            for seg_id in submission.seg_id.values:\n",
    "                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n",
    "            #print(\"Feature Generator __init__, int(len(submission)):\", int(len(submission)))\n",
    "            self.total_data = int(len(submission))\n",
    "\n",
    "    def read_chunks(self):\n",
    "        if self.dtype == 'train':\n",
    "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
    "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
    "            for counter, df in enumerate(iter_df):\n",
    "                x = df.acoustic_data.values\n",
    "                y = df.time_to_failure.values[-1]\n",
    "                seg_id = 'train_' + str(counter)\n",
    "                del df\n",
    "                yield seg_id, x, y\n",
    "        else:\n",
    "            for seg_id, f in self.test_files:\n",
    "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
    "                x = df.acoustic_data.values[-self.chunk_size:]\n",
    "                del df\n",
    "                yield seg_id, x, -999\n",
    "    \n",
    "    def get_features(self, x, y, seg_id):\n",
    "        \"\"\"\n",
    "        Gets three groups of features: from original data and from reald and imaginary parts of FFT.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = pd.Series(x)\n",
    "        \n",
    "        '''\n",
    "        zc = np.fft.fft(x)\n",
    "        realFFT = pd.Series(np.real(zc))\n",
    "        imagFFT = pd.Series(np.imag(zc))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        main_dict = self.features(x, y, seg_id)\n",
    "        \n",
    "        '''\n",
    "        r_dict = self.features(realFFT, y, seg_id)\n",
    "        i_dict = self.features(imagFFT, y, seg_id)\n",
    "        \n",
    "        for k, v in r_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'fftr_{k}'] = v\n",
    "                \n",
    "        for k, v in i_dict.items():\n",
    "            if k not in ['target', 'seg_id']:\n",
    "                main_dict[f'ffti_{k}'] = v\n",
    "        '''\n",
    "        return main_dict\n",
    "        \n",
    "    \n",
    "    def features(self, x, y, seg_id):\n",
    "        feature_dict = dict()\n",
    "        feature_dict['target'] = y\n",
    "        feature_dict['seg_id'] = seg_id\n",
    "\n",
    "        # create features here\n",
    "\n",
    "        # lists with parameters to iterate over them\n",
    "        #percentiles = [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]\n",
    "        percentiles = [10, 20]\n",
    "        hann_windows = [50, 150, 1500, 15000]\n",
    "        spans = [300, 3000, 30000, 50000]\n",
    "        windows = [10, 50, 100, 500, 1000, 10000]\n",
    "        borders = list(range(-4000, 4001, 1000))\n",
    "        #peaks = [10, 20, 50, 100]\n",
    "        peaks = [10]\n",
    "        coefs = [1, 5, 10, 50, 100]\n",
    "        lags = [10, 100, 1000, 10000]\n",
    "        #autocorr_lags = [5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "        autocorr_lags = [5]\n",
    "        # basic stats\n",
    "        feature_dict['mean'] = x.mean()\n",
    "        feature_dict['std'] = x.std()\n",
    "        feature_dict['max'] = x.max()\n",
    "        feature_dict['min'] = x.min()\n",
    "        \n",
    "        for i, e_acc in enumerate(get_extremal_accelerations(x, num_of_extremals=6)):\n",
    "            feature_dict[f'e_acc_{i}'] = np.abs(e_acc)\n",
    "\n",
    "        '''\n",
    "        # basic stats on absolute values\n",
    "        feature_dict['mean_change_abs'] = np.mean(np.diff(x))\n",
    "        feature_dict['abs_max'] = np.abs(x).max()\n",
    "        feature_dict['abs_mean'] = np.abs(x).mean()\n",
    "        feature_dict['abs_std'] = np.abs(x).std()\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # geometric and harminic means\n",
    "        '''\n",
    "        feature_dict['hmean'] = stats.hmean(np.abs(x[np.nonzero(x)[0]]))\n",
    "        feature_dict['gmean'] = stats.gmean(np.abs(x[np.nonzero(x)[0]])) \n",
    "\n",
    "        # k-statistic and moments\n",
    "        for i in range(1, 5):\n",
    "            feature_dict[f'kstat_{i}'] = stats.kstat(x, i)\n",
    "            feature_dict[f'moment_{i}'] = stats.moment(x, i)\n",
    "\n",
    "        for i in [1, 2]:\n",
    "            feature_dict[f'kstatvar_{i}'] = stats.kstatvar(x, i)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # aggregations on various slices of data\n",
    "        for agg_type, slice_length, direction in product(['std', 'min', 'max', 'mean'], [1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[:slice_length].agg(agg_type)\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'{agg_type}_{direction}_{slice_length}'] = x[-slice_length:].agg(agg_type)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''\n",
    "        feature_dict['max_to_min'] = x.max() / np.abs(x.min())\n",
    "        feature_dict['max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "        feature_dict['count_big'] = len(x[np.abs(x) > 500])\n",
    "        feature_dict['sum'] = x.sum()\n",
    "\n",
    "        feature_dict['mean_change_rate'] = calc_change_rate(x)\n",
    "        # calc_change_rate on slices of data\n",
    "        for slice_length, direction in product([1000, 10000, 50000], ['first', 'last']):\n",
    "            if direction == 'first':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[:slice_length])\n",
    "            elif direction == 'last':\n",
    "                feature_dict[f'mean_change_rate_{direction}_{slice_length}'] = calc_change_rate(x[-slice_length:])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # percentiles on original and absolute values\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'percentile_{p}'] = np.percentile(x, p)\n",
    "            feature_dict[f'abs_percentile_{p}'] = np.percentile(np.abs(x), p)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        feature_dict['trend'] = add_trend_feature(x)\n",
    "        feature_dict['abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "        '''\n",
    "\n",
    "        feature_dict['mad'] = x.mad()\n",
    "        feature_dict['kurt'] = x.kurtosis()\n",
    "        feature_dict['skew'] = x.skew()\n",
    "        feature_dict['med'] = x.median()\n",
    "\n",
    "        '''\n",
    "        feature_dict['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "\n",
    "        for hw in hann_windows:\n",
    "            feature_dict[f'Hann_window_mean_{hw}'] = (convolve(x, hann(hw), mode='same') / sum(hann(hw))).mean()\n",
    "\n",
    "        feature_dict['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "        feature_dict['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "        feature_dict['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "        feature_dict['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "        feature_dict['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
    "        feature_dict['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
    "        feature_dict['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
    "        feature_dict['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # exponential rolling statistics\n",
    "        ewma = pd.Series.ewm\n",
    "        for s in spans:\n",
    "            feature_dict[f'exp_Moving_average_{s}_mean'] = (ewma(x, span=s).mean(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_average_{s}_std'] = (ewma(x, span=s).mean(skipna=True)).std(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_mean'] = (ewma(x, span=s).std(skipna=True)).mean(skipna=True)\n",
    "            feature_dict[f'exp_Moving_std_{s}_std'] = (ewma(x, span=s).std(skipna=True)).std(skipna=True)\n",
    "\n",
    "        feature_dict['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "        feature_dict['iqr1'] = np.subtract(*np.percentile(x, [95, 5]))\n",
    "        feature_dict['ave10'] = stats.trim_mean(x, 0.1)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for slice_length, threshold in product([50000, 100000, 150000],\n",
    "                                                     [5, 10, 20, 50, 100]):\n",
    "            feature_dict[f'count_big_{slice_length}_threshold_{threshold}'] = (np.abs(x[-slice_length:]) > threshold).sum()\n",
    "            feature_dict[f'count_big_{slice_length}_less_threshold_{threshold}'] = (np.abs(x[-slice_length:]) < threshold).sum()\n",
    "\n",
    "        # tfresh features take too long to calculate, so I comment them for now\n",
    "\n",
    "#         feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
    "#         feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
    "#         feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
    "#         feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
    "#         feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
    "#         feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
    "#         feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
    "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
    "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for i, j in zip(borders, borders[1:]):\n",
    "            feature_dict[f'range_{i}_{j}'] = feature_calculators.range_count(x, i, j)\n",
    "        '''\n",
    "\n",
    "#         feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
    "#         feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
    "#         feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
    "#         feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
    "#         feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
    "\n",
    "#         for lag in lags:\n",
    "#             feature_dict[f'time_rev_asym_stat_{lag}'] = feature_calculators.time_reversal_asymmetry_statistic(x, lag)\n",
    "        ## for autocorr_lag in autocorr_lags:\n",
    "        ##    feature_dict[f'autocorrelation_{autocorr_lag}'] = feature_calculators.autocorrelation(x, autocorr_lag)\n",
    "        ##    #feature_dict[f'c3_{autocorr_lag}'] = feature_calculators.c3(x, autocorr_lag)\n",
    "\n",
    "#         for coeff, attr in product([1, 2, 3, 4, 5], ['real', 'imag', 'angle']):\n",
    "#             feature_dict[f'fft_{coeff}_{attr}'] = list(feature_calculators.fft_coefficient(x, [{'coeff': coeff, 'attr': attr}]))[0][1]\n",
    "\n",
    "#         feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
    "#         feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
    "#         feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
    "#         feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
    "        \n",
    "    \n",
    "        '''\n",
    "        for p in percentiles:\n",
    "            feature_dict[f'binned_entropy_{p}'] = feature_calculators.binned_entropy(x, p)\n",
    "\n",
    "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
    "        '''\n",
    "        \n",
    "        ## for peak in peaks:\n",
    "        ##    feature_dict[f'num_peaks_{peak}'] = feature_calculators.number_peaks(x, peak)\n",
    "        \n",
    "        '''\n",
    "        for c in coefs:\n",
    "            feature_dict[f'spkt_welch_density_{c}'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': c}]))[0][1]\n",
    "            feature_dict[f'time_rev_asym_stat_{c}'] = feature_calculators.time_reversal_asymmetry_statistic(x, c)  \n",
    "        '''\n",
    "        \n",
    "        # statistics on rolling windows of various sizes\n",
    "        for w in windows:\n",
    "            pass\n",
    "            ## x_roll_std = x.rolling(w).std().dropna().values\n",
    "            ## x_roll_mean = x.rolling(w).mean().dropna().values\n",
    "            \n",
    "            \n",
    "            #feature_dict[f'ave_roll_std_{w}'] = x_roll_std.mean()\n",
    "            #feature_dict[f'std_roll_std_{w}'] = x_roll_std.std()\n",
    "            #feature_dict[f'max_roll_std_{w}'] = x_roll_std.max()\n",
    "            \n",
    "            ## feature_dict[f'min_roll_std_{w}'] = x_roll_std.min()\n",
    "            \n",
    "\n",
    "            ## for p in percentiles:\n",
    "            ##    feature_dict[f'percentile_roll_std_{p}_window_{w}'] = np.percentile(x_roll_std, p)\n",
    "            \n",
    "            '''\n",
    "            feature_dict[f'av_change_abs_roll_std_{w}'] = np.mean(np.diff(x_roll_std))\n",
    "            feature_dict[f'av_change_rate_roll_std_{w}'] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_std_{w}'] = np.abs(x_roll_std).max()\n",
    "\n",
    "            feature_dict[f'ave_roll_mean_{w}'] = x_roll_mean.mean()\n",
    "            feature_dict[f'std_roll_mean_{w}'] = x_roll_mean.std()\n",
    "            feature_dict[f'max_roll_mean_{w}'] = x_roll_mean.max()\n",
    "            feature_dict[f'min_roll_mean_{w}'] = x_roll_mean.min()\n",
    "            \n",
    "            for p in percentiles:\n",
    "                feature_dict[f'percentile_roll_mean_{p}_window_{w}'] = np.percentile(x_roll_mean, p)\n",
    "\n",
    "            feature_dict[f'av_change_abs_roll_mean_{w}'] = np.mean(np.diff(x_roll_mean))\n",
    "            feature_dict[f'av_change_rate_roll_mean_{w}'] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "            feature_dict[f'abs_max_roll_mean_{w}'] = np.abs(x_roll_mean).max()    \n",
    "            '''\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def generate(self):\n",
    "        feature_list = []\n",
    "        res = Parallel(n_jobs=self.n_jobs,\n",
    "                       backend='threading')(delayed(self.get_features)(x, y, s)\n",
    "                                            for s, x, y in tqdm_notebook(self.read_chunks(), total=self.total_data))\n",
    "        #print(\"FeatureGenerator, generate, type(res)\", type(res))\n",
    "        #print(\"FeatureGenerator, generate, len(res)\", len(res))\n",
    "        for r in res:\n",
    "            feature_list.append(r)\n",
    "        #print(\"FeatureGenerator, generate, len(feature_list)\", len(feature_list))\n",
    "        return pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "train_X_0 = pd.read_csv(\"../input/lanl-masters-features-creating-0/train_X_features_865.csv\")\n",
    "train_X_1 = pd.read_csv(\"../input/lanl-masters-features-creating-1/train_X_features_865.csv\")\n",
    "y_0 = pd.read_csv(\"../input/lanl-masters-features-creating-0/train_y.csv\", index_col=False,  header=None)\n",
    "y_1 = pd.read_csv(\"../input/lanl-masters-features-creating-1/train_y.csv\", index_col=False,  header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_X = pd.concat([train_X_0, train_X_1], axis=0)\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = pd.concat([y_0, y_1], axis=0)\n",
    "y = y.reset_index(drop=True)\n",
    "y[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_y = pd.Series(y[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "test_X = pd.read_csv(\"../input/lanl-masters-features-creating-0/test_X_features_10.csv\")\n",
    "'''\n",
    "del X[\"seg_id\"], test_X[\"seg_id\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59444966c6d1475a996215c39e9e7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33a5088f3b74c1f94773d307d2c9dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_fg = FeatureGenerator(dtype='train', n_jobs=20, chunk_size=30000)\n",
    "\n",
    "\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=20, chunk_size=30000)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target\n",
    "train_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2624, 14) (20972, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_acc_0</th>\n",
       "      <th>e_acc_1</th>\n",
       "      <th>e_acc_2</th>\n",
       "      <th>e_acc_3</th>\n",
       "      <th>e_acc_4</th>\n",
       "      <th>e_acc_5</th>\n",
       "      <th>kurt</th>\n",
       "      <th>mad</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>med</th>\n",
       "      <th>min</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.444444</td>\n",
       "      <td>17.909091</td>\n",
       "      <td>27.142857</td>\n",
       "      <td>20.444444</td>\n",
       "      <td>1.626168</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>33.165839</td>\n",
       "      <td>4.066226</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.011700</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>-0.129510</td>\n",
       "      <td>7.367779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.375000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>4.354795</td>\n",
       "      <td>3.336135</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.846700</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.070504</td>\n",
       "      <td>4.630081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>15.249594</td>\n",
       "      <td>3.203834</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.132100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>0.024047</td>\n",
       "      <td>4.939919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.200000</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.461538</td>\n",
       "      <td>2.259259</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>6.538675</td>\n",
       "      <td>2.852130</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.922000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>3.947113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.021752</td>\n",
       "      <td>2.022456</td>\n",
       "      <td>2.839988</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.508067</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>0.047550</td>\n",
       "      <td>3.766104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     e_acc_0    e_acc_1    e_acc_2    e_acc_3   e_acc_4   e_acc_5       kurt  \\\n",
       "0  22.444444  17.909091  27.142857  20.444444  1.626168  1.105263  33.165839   \n",
       "1   9.375000   7.000000   0.005121   0.005974  0.005791  0.010464   4.354795   \n",
       "2  12.000000   0.004695   0.004518   0.004289  0.003961  0.003915  15.249594   \n",
       "3   7.200000   8.375000   2.600000   2.461538  2.259259  0.022822   6.538675   \n",
       "4   0.297872   0.078212   0.002079   0.465909  0.573529  0.021752   2.022456   \n",
       "\n",
       "        mad    max      mean  med   min      skew       std  \n",
       "0  4.066226  104.0  5.011700  5.0 -98.0 -0.129510  7.367779  \n",
       "1  3.336135   40.0  4.846700  5.0 -35.0  0.070504  4.630081  \n",
       "2  3.203834   52.0  5.132100  5.0 -56.0  0.024047  4.939919  \n",
       "3  2.852130   40.0  4.922000  5.0 -32.0  0.018361  3.947113  \n",
       "4  2.839988   26.0  4.508067  5.0 -16.0  0.047550  3.766104  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = {}\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        print(col)\n",
    "        mean_value = X.loc[X[col] != -np.inf, col].mean()\n",
    "        X.loc[X[col] == -np.inf, col] = mean_value\n",
    "        X[col] = X[col].fillna(mean_value)\n",
    "        means_dict[col] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull().any():\n",
    "        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n",
    "        X_test[col] = X_test[col].fillna(means_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#train_columns = train_X.columns\n",
    "\n",
    "#train_X[train_columns] = scaler.fit_transform(train_X[train_columns])\n",
    "#test_X[train_columns] = scaler.transform(test_X[train_columns])\n",
    "\n",
    "train_X = scaler.fit_transform(X)\n",
    "test_X = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = train_X.columns\n",
    "\n",
    "# df = pd.concat([train_X, test_X[cols]], axis=0)\n",
    "# df = df.reset_index(drop=True)\n",
    "# df[cols] = np.round(df.values, 3)\n",
    "\n",
    "# for col in cols:\n",
    "#     df[col+\"_count\"] = df[col].map(df[col].value_counts())\n",
    "    \n",
    "# count_cols = [i for i in df.columns if \"_count\" in i]\n",
    "\n",
    "# train_X = pd.concat([train_X, df.loc[:40000-1, count_cols]], axis=1)\n",
    "# test_X = pd.concat([test_X, df.loc[40000:, count_cols].reset_index(drop=True)], axis=1)\n",
    "# train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_columns = train_X.columns\n",
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "'''\n",
    " if PREDICTION:\n",
    " '''\n",
    "predictions = np.zeros(len(test_X))\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#run model\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "\n",
    "    X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    model = CatBoostRegressor(n_estimators=25000, verbose=-1, objective=\"MAE\", loss_function=\"MAE\", boosting_type=\"Ordered\", task_type=\"GPU\")\n",
    "    model.fit(X_tr, \n",
    "              y_tr, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              '''\n",
    "              eval_metric='mae',\n",
    "              '''\n",
    "              verbose=2500, \n",
    "              early_stopping_rounds=500)\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    #feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    #predictions\n",
    "    #if PREDICTION:\n",
    "\n",
    "    predictions += model.predict(test_X[train_columns]) / folds.n_splits\n",
    "    train_score.append(model.best_score_['learn'][\"MAE\"])\n",
    "\n",
    "cv_score = mean_absolute_error(train_y, oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = predictions\n",
    "submission.to_csv(f'CatBoost_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed May 29 17:15:46 2019\n",
      "[0]\ttrain-mae:5.07515\tvalid_data-mae:5.01269\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[86]\ttrain-mae:1.66463\tvalid_data-mae:2.28381\n",
      "\n",
      "Fold 1 started at Wed May 29 17:15:51 2019\n",
      "[0]\ttrain-mae:5.03663\tvalid_data-mae:5.17603\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[108]\ttrain-mae:1.56181\tvalid_data-mae:2.3668\n",
      "\n",
      "Fold 2 started at Wed May 29 17:15:57 2019\n",
      "[0]\ttrain-mae:5.08\tvalid_data-mae:5.00036\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[95]\ttrain-mae:1.61595\tvalid_data-mae:2.30769\n",
      "\n",
      "Fold 3 started at Wed May 29 17:16:03 2019\n",
      "[0]\ttrain-mae:5.05991\tvalid_data-mae:5.07993\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[84]\ttrain-mae:1.6239\tvalid_data-mae:2.35839\n",
      "\n",
      "Fold 4 started at Wed May 29 17:16:08 2019\n",
      "[0]\ttrain-mae:5.06298\tvalid_data-mae:5.06838\n",
      "Multiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-mae hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[101]\ttrain-mae:1.59503\tvalid_data-mae:2.33419\n",
      "\n",
      "CV mean score: 2.3302, std: 0.0310.\n",
      "CPU times: user 1min 52s, sys: 188 ms, total: 1min 52s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_params = {'eta': 0.03, 'max_depth': 10, 'subsample': 0.85, #'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 4}\n",
    "oof_xgb, prediction_xgb, scores = train_model(X, X_test, y, params=xgb_params, model_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed May 29 17:16:14 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1152]\ttraining's l1: 2.01083\tvalid_1's l1: 2.39433\n",
      "Fold 1 started at Wed May 29 17:16:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1244]\ttraining's l1: 1.97129\tvalid_1's l1: 2.48217\n",
      "Fold 2 started at Wed May 29 17:16:28 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1307]\ttraining's l1: 1.96809\tvalid_1's l1: 2.42841\n",
      "Fold 3 started at Wed May 29 17:16:35 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1085]\ttraining's l1: 2.01514\tvalid_1's l1: 2.46347\n",
      "Fold 4 started at Wed May 29 17:16:41 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1308]\ttraining's l1: 1.96703\tvalid_1's l1: 2.42143\n",
      "CV mean score: 2.4380, std: 0.0312.\n",
      "CPU times: user 2min 13s, sys: 3.2 s, total: 2min 16s\n",
      "Wall time: 35 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8sAAALJCAYAAACDTrEQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3X24bnVdJ/73R47KEQ4gAm7BBBSddNQ0JXW0B4OJ0DQfUFQQsSnyKfPnWPxMf2lP03imZjJMHTAVSg1/qGk+BA3paFR4NAXCME1TUwqJh8OTyMNn/rgXuV2z9zkHzrnP2nuf1+u69uW91/re3/Vet+LFe3/XWnd1dwAAAIDvuNPUAQAAAGClUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBoBdSFXdtao+V1ULU2fZWarq31XVZ6rqmqp66VbGnlhVf7GF/R+rqp/eyhx3rapLquqAO5oZgOkpywCsWFX1j1V15DL7NlTVfx/GXFdVX62qs6rqBxaN6WHftVV1eVW9q6r22crxbhjG3/Zz4Haew49U1T9tzxw72ElJPt7d/zx1kJ3oF5N8rLs3dPfvzvtg3X1jkrcmOXnexwJgfpRlAFadqrprkj9P8pAkP5FkryQPTPJHSZ4wGv593b1nkvsmuXuS125l+id1956Lfr6xQ8PfTlW1bgdP+bNJ/mAHz7ki1MxS/25zcJKLd3KcdyZ53vC/VQBWIWUZgNXouUnuneQp3f233X1Ld1/X3Wd192uXekN3b07ygSQPuiMHrKpHV9VfVtVVVXVBVf3Ion3Pr6q/Gy7z/VJV/eywfY8kH0ly4OKV6qp6e1X9+qL3f9fq87DCfXJVXZjkuqpaN7zvPVX1zar68uLLiavqB6rqU1W1uar+par++zLncJ8k90ty/qJtTxwuUd5cVV+rqtcu2venVfWS0RwXVNXThtc/VlWfr6qrq+qNVfW/l7tEebg0+Xeq6hvDz+/cViSHz+4nFo1dN1wJ8P3b8Nl/rKp+o6rOS3J9Zn8UWXzcP0/y+CRvGD7/B1TV3lV1xvBZfqWqXr1MyU5V/cfhkuqrq+oNSWrRvsOGc756yHvmbfu6+5+SXJnk0UvNC8DKpywDsBodmeTs7r5uW99QVXdP8pQkf317D1ZVByX5UJJfT7JvklckeU9V7T8MuSzfWeF+fpL/UVXfP+Q7Osk37sBK9bOTPDHJPkluTfInSS5IclCSI5K8rKqOGsa+Psnru3uvzMrwu5eZ8yFJvtTdNy/adl2SE4bjPDHJC6vqKcO+dw45bvscHpTZKu2Hqmq/JGcleWWSeyT5fJL/sIXzeVVmxfFhSb4vyQ8kefWw712Lj5PkqCSXd/ffbMNnn8z+eHJSkg1JvrL4oN39o0k+keQlw+f/90lOSbJ3ZsX6h4fzf/448HCO7xly7pfkH5I8dtGQX0tyTmZXLNx7mHexvxvOFYBVSFkGYDXaL8m/3XNbVQ8bVh03V9XnR2P/pqquSnJ5kvsk+Z9bmfuPh7muqqo/HrYdn+TD3f3h7r61u/8syacyXPLd3R/q7n/omf+dWYH6we08x9/t7q919w1JDk+yf3f/and/u7u/lOS0JM8axt6U5LCq2q+7r+3u5f4gsE+SaxZv6O6PdfdFw3ldmFlx/eFh9/uSPKyqDh5+Py7Je4d7cp+Q5OLufu9Qvn83i/47WcJxSX61uy/r7m8m+ZXMSm4yK+VPrqq7Db8/Z9iWbOWzH7y9uy/u7pu7+6YtZEhV7Zbk2CSv7O5ruvsfk/z2oiyLPSHJ54YrFm5K8jujc7wpsz8eHNjd3+ru8YPBrsnsMwdgFVKWAViN/jXJvW77pbs/2937JHlakvE9ot8/7Ns9yZuSfKKqdt/C3E/p7n2Gn9tWWA9O8oxFJfqqJI+7LUNVHV1Vf11VVwz7npBZod8eX1v0+uDMLuVefPxfSnLPYf9/SvKAJJdU1abFlzSPXJnZ6uu/qapHVdVHh0uSr07ygtuyd/c1ma3q3lbKn5XkHcPrAxdn7O5OsqUHmR2Y7171/cqwLd39xcxWYZ80FOYn5ztleYuf/WDxZ7U1+yW5yxJZDlom8/gcFx/rFzO7LPuTVXVxVf3U6P0bklx1O7IBsIIoywCsRucm+bHhnuBtMqwMviXJoUkefDuP97Ukf7CoRO/T3Xt0938d7rt9T5LfSnLPoZh/ON+5t7WXmO+6JHdb9PtSX+O0+H1fS/Ll0fE3dPdtK9tf6O5nJzkgyeuSnLXMZ3NhkvvWdz807J2Z3cv9Pd29d5I3L8qeDJdIV9VjkqxP8tFh+6WZXXqcZPZwrcW/L+EbmRXf29xn2PZdx0nyk5mt5n5x0bkv+dkveu9Sn/FyLs93VoQXZ/n6EmMvTfI9t/0ynOO//d7d/9zdP9PdB2b24LQ3VtVhi97/wMwunQdgFVKWAVjp7lxVuy/6WZfkjMyKzPuq6sFVtduwWvzI5SYZLr99fpIbknzpdmb4w8xWPY+67Vg1eyjXvTNbpbxrkm8mubmqjk7yY4ve+y9J7lFVey/a9tkkT6iqfWv2fccv28rxP5lkc80e+rV+yPDgqjp8OLfjq2r/7r4131nJvGU8yfDQqS9kdr/wbTYkuaK7v1Wzr916zuhtH86sWP5qkjOHYySzFeeHVNVThv9OXpylS/9t3pXk1VW1/3Av8C9n9rne5o8y+9xemO+sKidb/uxvt+6+JbN7un+jZl8/dnCSl4+y3OZDSf59VT1tOMeXLj7HqnrGohxXZlbabxn2HZTZPda3+x55AFYGZRmAle7DmRXc235e293fyuwJx5/LrNBszuwBU4cneebo/RdU1bWZlZnnJXlqd19xewJ099cyW/H8pcxK8deS/EKSOw2XKr80swJ2ZWZl8wOL3ntJZkXxS8NlxAdm9tVNFyT5x8zub/63pygvc/xbkjwps4djfTmz1dG3ZPaQqiT58SQXD+f5+iTPGj6jpfzPfPf9uS9K8qtVdU1mBfa7Hg423J/83sweqvbORdsvT/KMJBszuyz+QZndS3zjMsf99WH/hUkuSvI3w7bb5rs0yV9l9pCwxU+VXvazX+Y42+LnMlvd/1KSvxjO663jQYvO8b9mdo73T3LeoiGHJzl/+Nw/kOTnu/vLw77nJDl9+PwAWIVqdvsNALArGC4b/0ySI4aCuqPmvVNm9ywf190f3dr4tWz4jC9I8kPdfdnUeQC4Y5RlAOAOGb666vzMVvx/IbNLse87PMEbAFY1l2EDAHfUYzL77uHLM7tM/CmKMgBrhZVlAAAAGLGyDAAAACPrtj5k17Lffvv1IYccMnUMAAAA5uDTn/705d29/9bGKcsjhxxySD71qU9NHQMAAIA5qKqvbMs4l2EDAADAiLIMAAAAI8oyAAAAjLhneeTmb16Rb77pD6eOAQAArFH7v/D4qSOwDawsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIys6bJcVYdW1flV9YWqOrOq7jJ1JgAAAFa+NV2Wk7wuyf/o7vsnuTLJf5o4DwAAAKvAunlOXlXHJ3lpkrskOT/Ji7r7liXGvSnJ4UnWJzmru18zbD88yeuT7JHkxiRHJLk+sxJ8VJJOclp3n7LEnJXkR5M8Z9h0epLXJnnTjjtDAABge/3Gx8/ON6+/duoYO81uf3XO1BHmYmFhIRs3bpw6xg4zt7JcVQ9McmySx3b3TVX1xiTHJTljieGv6u4rqmq3JOdW1UOTXJLkzCTHdvemqtoryQ1JTkpyaJKHd/fNVbXvMhHukeSq7r55+P2fkhy0TNaThnlz733vcUdOFwAAuIO+ef21+edrN08dY+fZlc51FZvnyvIRSR6RZNNskTfrk1y2zNhnDoV1XZJ7JXlQZqvGl3b3piTp7s1JUlVHJnnzbSW4u69YZs5aYlsvNbC7T01yapI87OD7LjkGAACYj/3vtufUEXaq3fbeMHWEuVhYWJg6wg41z7JcSU7v7lducVDVoUlekeTw7r6yqt6eZPfh/UsV1+W2j12eZJ+qWjcU63sn+cbtyA8AAOwEr/qho6aOsFPt/8Ljp47ANpjnA77OTXJMVR2QJFW1b1UdvMS4vZJcl+TqqrpnkqOH7ZckOXC4bzlVtaGq1iU5J8kLhtdZ7jLs7u4kH01yzLDpeUnev0PODAAAgDVtbmW5uz+X5NVJzqmqC5P8WWaXWI/HXZDkM0kuTvLWJOcN27+d2T3Pp1TVBcP7d0/yliRfTXLhsP054zkXOTnJy6vqi5ndw/z7O+bsAAAAWMvm+jTs7j4zs4d0bW3cicts35Tk0Uvsevnws7V5v5TkB7Y2DgAAABZb69+zDAAAALfbXFeWx6rq/CR3HW1+bndftJ3zvi+zr5Na7OTuPnt75gUAAGDXtFPLcnc/ak7zPnUe8wIAALBrchk2AAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMrJs6wEqzbv99s/8Lj586BgAAABOysgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPrpg6w0tx02Vfz9d978dQxAAAAtuigF//e1BHWNCvLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACNruixX1Uuq6otV1VW139R5AAAAWB3WTR1gzs5L8sEkH5s4BwAAsMZs/Iuv5pvX3zTZ8dedf8Ikx11YWMjGjRsnOfbONNeyXFXHJ3lpkrskOT/Ji7r7liXGvSnJ4UnWJzmru18zbD88yeuT7JHkxiRHJLk+yeuSHJWkk5zW3acsdfzu/swwz9ZynpTkpCQ56O573t7TBAAAdkHfvP6m/Mu105XlXPv16Y69C5hbWa6qByY5Nslju/umqnpjkuOSnLHE8Fd19xVVtVuSc6vqoUkuSXJmkmO7e1NV7ZXkhsxK7aFJHt7dN1fVvtubtbtPTXJqknzffQ7o7Z0PAABY+/a/250nPf66vQ+Y5LgLCwuTHHdnm+fK8hFJHpFk07Cyuz7JZcuMfeawursuyb2SPCizVeNLu3tTknT35iSpqiOTvLm7bx62XzHHcwAAAFjSLz7uPpMe/6AX/96kx1/r5lmWK8np3f3KLQ6qOjTJK5Ic3t1XVtXbk+w+vH+pVd7ltgMAAMAOMc+nYZ+b5JiqOiBJqmrfqjp4iXF7JbkuydVVdc8kRw/bL0ly4HDfcqpqQ1WtS3JOkhcMr7MjLsMGAACAxeZWlrv7c0leneScqrowyZ9ldon1eNwFST6T5OIkb83sCdbp7m9nds/zKVV1wfD+3ZO8JclXk1w4bH/Ochmq6qVV9U9J7j2Mf8uOO0MAAADWqup2RfNi33efA/rDJz9j6hgAAABb5J7lO6aqPt3dj9zauHlehg0AAACr0ly/Z3msqs5PctfR5ud290XbOe/7Mvs6qcVO7u6zt2deAAAAdk07tSx396PmNO9T5zEvAAAAuyaXYQMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwMi6qQOsNHc+4D456MW/N3UMAAAAJmRlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAkXVTB1hprr38i/nEaT8xdQwAAGA7/eDPfHDqCKxiVpYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARnapslxVH6uqR06dAwAAgJVtlyrLAAAAsC3WTR1ga6rqkCR/muQvkjw6yQVJ3pbkV5IckOS4YejvJFmf5IYkz+/uz1fV+mHsg5L83bAfAABWjTM+emOuur6njrEqnfaJE6aOsCItLCxk48aNU8dY8VZ8WR4cluQZSU5KsinJc5I8LsmTk/xSkhOS/FB331xVRyb5L0menuSFSa7v7odW1UOT/M1Sk1fVScPcuee++jQAACvHVdd3rrhGWb5Drvn61AlYxVZLWf5yd1+UJFV1cZJzu7ur6qIkhyTZO8npVXX/JJ3kzsP7fijJ7yZJd19YVRcuNXl3n5rk1CT53kP28f9EAACsGPvcraaOsGqt3+vAqSOsSAsLC1NHWBVWS1m+cdHrWxf9fmtm5/BrST7a3U8dLtv+2KLxyi8AAKvWCY+/69QRVq0f/Jkzpo7AKrZWHvC1d5LbrrE4cdH2j2e4p7mqHpzkoTs3FgAAAKvRWinLG5P8ZlWdl2S3RdvflGTP4fLrX0zyySnCAQAAsLqs+Muwu/sfkzx40e8nLrPvAYve9v8N+29I8qx5ZwQAAGBtWSsrywAAALDDKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMrJs6wEqz536H5Qd/5oNTxwAAAGBCVpYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhZN3WAleaKf/1C3vX2o6aOAQAA7CDPPvHsqSOwCllZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEbWVFmuqpdV1d2W2XdiVb1hZ2cCAABg9VlTZTnJy5IsWZYBAABgW62bOsAdVVV7JHl3knsn2S3J/5/kwCQfrarLu/vxVfX8JK9McmmSv09y41R5AQAAWD1WbVlO8uNJvtHdT0ySqto7yfOTPL67L6+qeyX5lSSPSHJ1ko8m+cxUYQEAYCX6yP+6Jddc21PHmKuP/PkJU0fYbgsLC9m4cePUMXYpq7ksX5Tkt6rqdUk+2N2fqKrF+x+V5GPd/c0kqaozkzxgqYmq6qQkJyXJfvfYfa6hAQBgJbnm2s7ma6ZOMV+br/n61BFYhVZtWe7uv6+qRyR5QpLfrKpzlhq2jXOdmuTUJLnvoXuv7T+rAQDAIhv2rGzjvzavWhv2OmjqCNttYWFh6gi7nFVblqvqwCRXdPcfVtW1SU5Mck2SDUkuT3J+ktdX1T2SbE7yjCQXTBQXAABWpKOP3G3qCHP37BPPmDoCq9CqLctJHpLkv1XVrUluSvLCJI9J8pGqunR4wNdrk/xVZg/4+pvMHgQGAAAAW7Rqy3J3n53k7NHmTyU5ZdGYtyV5287MBQAAwOq31r5nGQAAALabsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMDIuqkDrDT73uP+efaJZ08dAwAAgAlZWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgJF1UwdYaf75ii/kdX901NQxAACACZ38rLOnjsDErCwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjKzpslxVb6+qL1fVZ4efh02dCQAAgJVv3dQBdoJf6O6zpg4BAABrzac/fEtuuKanjjEXF3/4hKkj3G4LCwvZuHHj1DHWjLmW5ao6PslLk9wlyflJXtTdtywx7k1JDk+yPslZ3f2aYfvhSV6fZI8kNyY5Isn1SV6X5KgkneS07j5lO3OelOSkJNlnv923ZyoAANhl3HBN5/rNU6eYj+s3f33qCExsbmW5qh6Y5Ngkj+3um6rqjUmOS3LGEsNf1d1XVNVuSc6tqocmuSTJmUmO7e5NVbVXkhsyK7WHJnl4d99cVftuJcpvVNUvJzk3yf/b3TeOB3T3qUlOTZJ733fvtfmnMQAA2MHWb6jM1q/WnrtvOGjqCLfbwsLC1BHWlHmuLB+R5BFJNlVVMls1vmyZsc8cVnfXJblXkgdl9k/dpd29KUm6e3OSVNWRSd7c3TcP26/YQoZXJvnnzFa2T01ycpJf3b7TAgAAkuQRT9ht6ghzc/KzllrjY1cyz7JcSU7v7lducVDVoUlekeTw7r6yqt6eZPfh/Uv9mWqb/3zV3ZcOL2+sqrcNxwEAAIAtmufTsM9NckxVHZAkVbVvVR28xLi9klyX5OqqumeSo4ftlyQ5cLhvOVW1oarWJTknyQuG19nSZdhVda/hPyvJU5L87Q45MwAAANa0ua0sd/fnqurVSc6pqjsluSnJi5N8ZTTugqr6TJKLk3wpyXnD9m9X1bFJTqmq9Zndr3xkkrckeUCSC6vqpiSnJXnDMjHeUVX7Z7Ya/dkkL9jBpwkAAMAaNNenYXf3mZk9pGtr405cZvumJI9eYtfLh5+tzfujWxsDAAAAY/O8DBsAAABWpbmuLI9V1flJ7jra/Nzuvmg7531fZl8ntdjJ3X329swLAADArmmnluXuftSc5n3qPOYFAABg1+QybAAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFk3dYCVZmHf++fkZ509dQwAAAAmZGUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAICRdVMHWGm+cNU/5uj3P2/qGAAAwA7wkZ88feoIrFJWlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABG1nRZrqrfr6oLqurCqjqrqvacOhMAAAAr35ouy0n+n+7+vu5+aJKvJnnJ1IEAAABY+dbNc/KqOj7JS5PcJcn5SV7U3bcsMe5NSQ5Psj7JWd39mmH74Ulen2SPJDcmOSLJ9Ulel+SoJJ3ktO4+Zanjd/fmYZ4a5u4deX4AALDa3fS+b6av+b/+FX3NOOE9J0wdYa4WFhaycePGqWOsSXMry1X1wCTHJnlsd99UVW9MclySM5YY/qruvqKqdktyblU9NMklSc5Mcmx3b6qqvZLckOSkJIcmeXh331xV+24lx9uSPCHJ55L852XGnDTMm9333+MOnC0AAKxOfc0tyVU3Tx1jbr5+1denjsAqNc+V5SOSPCLJptnCbtYnuWyZsc8cCuu6JPdK8qDMVoEv7e5NyXetEh+Z5M3dffOw/Yotheju5w8l/JTMyvvblhhzapJTk2Tvw/az+gwAwC6jNuy2pi+/PGiPe04dYa4WFhamjrBmzbMsV5LTu/uVWxxUdWiSVyQ5vLuvrKq3J9l9eP9S/9wut31Z3X1LVZ2Z5BeyRFkGAIBd1Z2fuv/UEebqjJ88feoIrFLzfMDXuUmOqaoDkqSq9q2qg5cYt1eS65JcXVX3THL0sP2SJAcO9y2nqjZU1bok5yR5wfA6y12GXTOH3fY6yZOGOQEAAGCL5ray3N2fq6pXJzmnqu6U5KYkL07yldG4C6rqM0kuTvKlJOcN279dVccmOaWq1md2v/KRSd6S5AFJLqyqm5KcluQNS0SoJKcP9zpXkguSvHDHnykAAABrTXWv5TsUbr+9D9uv/8NvP3HqGAAAwA7wEZdhM1JVn+7uR25t3Fr/nmUAAAC43eb6PctjVXV+kruONj+3uy/aznnfl9nXSS12cnefvT3zAgAAsGvaqWW5ux81p3mfOo95AQAA2DW5DBsAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEbWTR1gpbn/PofkIz95+tQxAAAAmJCVZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFk3dYCV5gtX/Uue+N7fmToGAADscB962sumjgCrhpVlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgJE1XZar6h1V9fmq+tuqemtV3XnqTAAAAKx866YOMGfvSHL88PqdSX46yZumiwMAwGr07Q/8VXrzDVPH2G4n/PHfTB1hh1hYWMjGjRunjsEaN9eyXFXHJ3lpkrskOT/Ji7r7liXGvSnJ4UnWJzmru18zbD88yeuT7JHkxiRHJLk+yeuSHJWkk5zW3acsdfzu/vCiY3wyyb2XyXlSkpOSZPf97n5HThUAgDWsN9+Qvvq6qWNst6+vgXOAnWVuZbmqHpjk2CSP7e6bquqNSY5LcsYSw1/V3VdU1W5Jzq2qhya5JMmZSY7t7k1VtVeSGzIrtYcmeXh331xV+25DljsneW6Sn19qf3efmuTUJNn7sO/p23uuAACsbbXX+qkj7BAH7rnP1BF2iIWFhakjsAuY58ryEUkekWRTVSWzVePLlhn7zGF1d12SeyV5UGarxpd296Yk6e7NSVJVRyZ5c3ffPGy/YhuyvDHJx7v7E3f8dAAA2FXd5cmPmTrCDnHG0142dQRYNeZZlivJ6d39yi0Oqjo0ySuSHN7dV1bV25PsPrx/qVXe5bYvN/9rkuyf5Ge39T0AAADs2ub5NOxzkxxTVQckSVXtW1UHLzFuryTXJbm6qu6Z5Ohh+yVJDhzuW05VbaiqdUnOSfKC4XW2dBl2Vf10Zvc2P7u7b90wp/8nAAAgAElEQVRB5wUAAMAaN7ey3N2fS/LqJOdU1YVJ/iyzS6zH4y5I8pkkFyd5a5Lzhu3fzuye51Oq6oLh/bsneUuSrya5cNj+nC3EeHOSeyb5q6r6bFX98g46PQAAANawuT4Nu7vPzOwhXVsbd+Iy2zclefQSu14+/Gxt3rX+1VgAAADMwTwvwwYAAIBVaaeuvFbV+UnuOtr83O6+aDvnfV9mXye12Mndffb2zAsAAMCuaatleXjo1n9JcmB3H11VD0rymO7+/dt7sO5+1B3IuC3zPnUe8wIAALBr2pbLsN+e5OwkBw6//30SX9AGAADAmrUtZXm/7n53kluTpLtvTnLLXFMBAADAhLalLF9XVfdI0klSVY9OcvVcUwEAAMCEtuUBXy9P8oEk96uq85Lsn+SYuaYCAACACW2xLFfVnZLsnuSHk/y7JJXk8919007IBgAAAJPYYlnu7lur6re7+zFJLt5JmQAAAGBS23LP8jlV9fSqqrmnAQAAgBVgW+9Z3iPJzVX1rcwuxe7u3muuyQAAAGAiWy3L3b1hZwQBAACAlWKrZbmqfmip7d398R0fZ3r33+ee+dDTXjZ1DAAAACa0LZdh/8Ki17sn+YEkn07yo3NJBAAAABPblsuwn7T496r6niQb55YIAAAAJrYtT8Me+6ckD97RQQAAAGCl2JZ7lk9J0sOvd0rysCQXzDMUAAAATGlb7ln+1KLXNyd5V3efN6c8AAAAMLltKcv7dPfrF2+oqp8fbwMAAIC1YlvuWX7eEttO3ME5AAAAYMVYdmW5qp6d5DlJDq2qDyzatSHJv847GAAAAExlS5dh/2WSS5Psl+S3F22/JsmF8ww1pS9eeUV+4qx3TB0DAAAm8cFjjps6AqwIy5bl7v5Kkq8keczOiwMAAADT2+o9y1X16KraVFXXVtW3q+qWqtq8M8IBAADAFLblAV9vSPLsJF9Isj7JTyc5ZZ6hAAAAYErb8tVR6e4vVtVu3X1LkrdV1V/OORcAAABMZlvK8vVVdZckn62qjZk99GuP+cYCAACA6WzLZdjPHca9JMl1Sb4nydPnGQoAAACmtNWV5e7+SlWtT3Kv7v6VnZAJAAAAJrUtT8N+UpLPJvnT4feHVdUH5h0MAAAAprItl2G/NskPJLkqSbr7s0kOmV8kAAAAmNa2lOWbu/vquScBAACAFWJbnob9t1X1nCS7VdX9k7w0ia+OAgAAYM1admW5qv5gePkPSf59khuTvCvJ5iQvm380AAAAmMaWVpYfUVUHJzk2yeOT/PaifXdL8q15BgMAAICpbKksvzmzJ2DfN8mnFm2vJD1sBwAAgDVn2cuwu/t3u/uBSd7a3fdd9HNodyvKAAAArFlbfRp2d79wZwQBAACAlWJbvjoKAAAAdinb8tVRc1dVh2R2f/RfJHl0kguSvC3JryQ5IMlxSS5OckqSh2SW+7Xd/f7hvX+QZI9hupd0919W1Y8keW2Sy5M8OMmnkxzf3b0TTgkAAJZ145/8afqaa6eOsaQTPnD21BG+y8LCQjZu3Dh1DHZBK6IsDw5L8owkJyXZlOQ5SR6X5MlJfinJ55L8eXf/VFXtk+STVfW/klyW5D9297eG74F+V5JHDnM+PLOvvfpGkvOSPDazQv5dquqk4bhZv9895naCAACQJH3NtemrN08dY0lfX6G5YGdbSWX5y919UZJU1cVJzu3urqqLkhyS5N5JnlxVrxjG757kPpkV4TdU1cOS3JLkAYvm/GR3/9Mw52eHef6vstzdpyY5NUn2ud99rTwDADBXtWHPqSMs68A9N0wd4bssLCxMHYFd1Eoqyzcuen3rot9vzSznLUme3t2fX/ymqnptkn9J8n2Z3YO9+PufF895S1bW+QIAsIu665N+fOoIyzrjmOOmjgArwmp6wNfZSX6uqipJqurhw/a9k1za3bcmeW6S3SbKBwAAwBqxmsryryW5c5ILq+pvh9+T5I1JnldVf53ZJdjXTZQPAACANaI8HPq77XO/+/bjXvdrWx8IAABr0Addhs0aV1Wf7u5Hbm3calpZBgAAgJ1CWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBk3dQBVprD7r5vPnjMcVPHAAAAYEJWlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgRFkGAACAEWUZAAAARpRlAAAAGFk3dYCV5otXbs5PnnX21DEAAOC7vP+Yo6aOALsUK8sAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI7tEWa6qU6rq2qlzAAAAsDqs+bJcVY9Mss/UOQAAAFg91s1z8qo6PslLk9wlyflJXtTdtywx7k1JDk+yPslZ3f2aYfvhSV6fZI8kNyY5Isn1SV6X5KgkneS07j5lmePvluS/JXlOkqfu0JMDAGDVuOFP3p1br7l66hjb5YQPvGPqCNtlYWEhGzdunDoGbLO5leWqemCSY5M8trtvqqo3JjkuyRlLDH9Vd18xlNtzq+qhSS5JcmaSY7t7U1XtleSGJCclOTTJw7v75qradwsxXpLkA919aVVtKetJw7xZv98Bt/tcAQBY2W695ur01VdOHWO7fH2V54fVZp4ry0ckeUSSTUNRXZ/ksmXGPnMorOuS3CvJgzJbNb60uzclSXdvTpKqOjLJm7v75mH7FUtNWFUHJnlGkh/ZWtDuPjXJqUmyz/0e0Nt2egAArBZ32rB3bp06xHY6cM+7TR1huywsLEwdAW6XeZblSnJ6d79yi4OqDk3yiiSHd/eVVfX2JLsP71+quC63fezhSQ5L8sWhrN+tqr7Y3Ydt+ykAALAWrH/SM6eOsN3OOOaoqSPALmWeD/g6N8kxVXVAklTVvlV18BLj9kpyXZKrq+qeSY4etl+S5MDhvuVU1YaqWpfknCQvGF5nucuwu/tD3b3Q3Yd09yFJrleUAQAA2BZzW1nu7s9V1auTnFNVd0pyU5IXJ/nKaNwFVfWZJBcn+VKS84bt366qY5OcUlXrM7tf+cgkb0nygCQXVtVNSU5L8oZ5nQcAAAC7nrk+Dbu7z8zsIV1bG3fiMts3JXn0ErtePvzcnix73p7xAAAA7LrW/PcsAwAAwO0115Xlsao6P8ldR5uf290Xbee878vs66QWO7m7z96eeQEAANg17dSy3N2PmtO8T53HvAAAAOyaXIYNAAAAI8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAj66YOsNIcdve98v5jjpo6BgAAABOysgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwMi6qQOsNF+68lt55nsumToGAAC7gHc//XunjgAsw8oyAAAAjCjLAAAAMKIsAwAAwIiyDAAAACPKMgAAAIwoywAAADCiLAMAAMCIsgwAAAAjyjIAAACMKMsAAAAwoiwDAADAiLIMAAAAI8oyAAAAjCjLAAAAMKIsAwAAwMiqKMtV9Y9Vtd/UOQAAANg1rIqyDAAAADvTuqkDjFXVHkneneTeSXZL8muL9q1P8r4k7+nu06rq+CQvTXKXJOcneVGSpyd5dHe/vKp+PsnPd/d9q+p+SU7v7sft3DMCAGDeNn/glNxyzRVTx7jdTnj/naeOcIctLCxk48aNU8eAuVlxZTnJjyf5Rnc/MUmqau8kr0uyZ5I/SnJGd59RVQ9McmySx3b3TVX1xiTHJTknyS8Mc/1gkn+tqoOSPC7JJ5Y6YFWdlOSkJLnbfgfO7cQAAJiPW665IrdefdnUMW63r189dQJgOSuxLF+U5Leq6nVJPtjdn6iqJHl/ko3d/Y5h3BFJHpFk07B/fZLLuvufq2rPqtqQ5HuSvDPJD2VWnN+71AG7+9QkpybJvvd7cM/tzAAAmIvdNuw7dYQ75F57ru6VZVjLVlxZ7u6/r6pHJHlCkt+sqnOGXeclObqq3tndnaQyu6z6lUtM81dJnp/k85mtJv9Uksck+c9zPwEAAHa6vZ78c1NHuEPOePr3Th0BWMaKe8BXVR2Y5Pru/sMkv5Xk+4ddv5zkX5O8cfj93CTHVNUBw/v2raqDh30fT/KK4T8/k+TxSW7sbhe6AAAAsFUrriwneUiST1bVZ5O8KsmvL9r3siS7V9XG7v5cklcnOaeqLkzyZ0nuNYz7RGaXYH+8u29J8rUkf7GzTgAAAIDVbSVehn12krNHmw9Z9Pr5i8aemeTMJeb4h8wu077t9x/bsSkBAABYy1biyjIAAABMSlkGAACAEWUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAABgZN3UAVaa+95997z76d87dQwAAAAmZGUZAAAARpRlAAAAGFGWAQAAYERZBgAAgBFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAICRdVMHWGn+9aqbc/p7vzl1DAAA5uB5T9t/6gjAKmFlGQAAAEaUZQAAABhRlgEAAGBEWQYAAIARZRkAAABGlGUAAAAYUZYBAOD/tHf/wZqW5X3Av1d2iSAoEH4dhcgawZqUIKZIaHQomapJ0wxqDI3RGladwbaKmjSmEaetxml0NklbNdFkaxWTSTVOjMo4rYgG6wQnYf3Jgko0QiqRn0GBBcTAXv3jfZk53Lvn7CK75znn7Ofzz77P89zP+1zvzsX9ni/3c54FGAjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgIGwDAAAAANhGQAAAAbCMgAAAAzWTFiuqk1VddXDOP/VVfXIfVkTAAAA69OaCcsPR1VtSPLqJMIyAAAAe7Qmw3JV/VBVfb6qXlNVv7to/0eq6uz56x1V9RtV9VdJXpfksUkuq6rLpqkaAACAtWLj1AU8VFX1j5K8L8mLk5yW5MQlhh6a5Kru/k/z816S5Ce7+9YVKRQAgAe59OL/kh133DJpDZ/40IZJr7+wsJAtW7ZMWgOwd9ZaWD4myYeTPK+7r66q05YZe3+SD+zNm1bV+UnOT5Kjjj7hYRcJAMCudtxxS+68/cZJa7jz9kkvD6whay0s357kG0meluTqJPflwbeSH7zo9Xe6+/69edPu3ppka5I8/qTTet+UCgDAYoc9+pipS8ijD5t+ZRlYG9ZaWP5ukuckuaSqdiS5Lsm/q6rvS3J8kjOWOffOJI9K4jZsAIAJPPOc101dQs77uekDO7A2rLkHfHX3XUl+NskvJzkqybVJtif57SSfW+bUrUn+jwd8AQAAsCdrZmW5u69Lcsr89beTPHV+6MNLjD9s2H5bkrftxxIBAABYJ9bcyjIAAADsb8IyAAAADIRlAAAAGAjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgIGwDAAAAANhGQAAAAbCMgAAAAyEZQAAABgIywAAADAQlgEAAGCwceoCVpujjtiY837umKnLAAAAYEJWlgEAAGAgLAMAAMBAWAYAAICBsAwAAAADYRkAAAAGwjIAAAAMhGUAAAAYCMsAAAAwEJYBAABgsHHqAlabu/7+vvzlRTdPXQYAAN+DMzcfO3UJwDphZRkAAAAGwjIAAAAMhGUAAAAYCMsAAAAwEJYBAABgICwDAADAQFgGAACAgbAMAAAAA2EZAAAABsIyAAAADIRlAAAAGAjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgIGwDAAAAIONUxewJ1W1KclHk/xFkjOTfDHJu5O8IcmxSV44H/rfkxyS5J4kL+7ua6rqV5Kc0t0vqaofTfLeJGd0990r+iEAAA5g7/74b+ZbO25ZkWsd/OcbVuQ6D1hYWMiWLVtW9JrAylj1YXnupCTnJjk/ybYkL0jy9CTnJLkwyS8lOau776uqZyT5zSTPyyxAf7KqnpvkdUletrugXFXnz987C0edsP8/DQDAAeRbO27J399548pc7M6VuQyw/q2VsHxtd29Pkqq6OsknururanuSTUkOT/Keqjo5SSc5KEm6e2dVbU5yZZI/6O7Ld/fm3b01ydYk+eHHn9b7+bMAABxQjjzsmBW71sGPXvmVZWB9With+d5Fr3cu2t6Z2Wd4Y5LLuvu589u2P7lo/MlJdiR57H6vEgCAXbz4GReu2LXO3Hzsil0LWN/WywO+Dk/yd/PXmx/YWVWHJ3lLkrOSHFVVP7/ypQEAALDWrJewvCXJm6rq8iSL7735b0ne3t1/neSlSd5cVf53IwAAAMta9bdhd/d1SU5ZtL15iWNPXHTaf5wff8misd/I7EFhAAAAsKz1srIMAAAA+4ywDAAAAANhGQAAAAbCMgAAAAyEZQAAABgIywAAADAQlgEAAGAgLAMAAMBAWAYAAICBsAwAAAADYRkAAAAGwjIAAAAMhGUAAAAYbJy6gNXm0KM25szNx05dBgAAABOysgwAAAADYRkAAAAGwjIAAAAMhGUAAAAYCMsAAAAwEJYBAABgICwDAADAQFgGAACAwcapC1ht/uHG7+bG3/rbqcsAAJjUwmtOnLoEgElZWQYAAICBsAwAAAADYRkAAAAGwjIAAAAMhGUAAAAYCMsAAAAwEJYBAABgICwDAADAQFgGAACAgbAMAAAAA2EZAAAABsIyAAAADIRlAAAAGAjLAAAAMBCWAQAAYLDuwnJVnVNVvz51HQAAAKxdG6cuYF/r7ouTXDx1HQAAAKxdayosV9WmJB9N8hdJzkzyxSTvTvKGJMcmeWGSH0lyene/oqouSnJHktOTLCT5te7+0xUvHABgCW/a9ju59Z5bpy5jFxu2r/4fExcWFrJly5apywDWqdU/C+7qpCTnJjk/ybYkL0jy9CTnJLkwyYeG8Y+ZH39SZivOu4Tlqjp//n45/ojj91fdAAC7uPWeW3Pj3TdNXcau7p66AIBprcWwfG13b0+Sqro6ySe6u6tqe5JNuxn/oe7emeRLVXXc7t6wu7cm2ZokTz7h1N4/ZQMA7OroQ46euoTd2nDk6v8xcWFhYeoSgHVs9c+Cu7p30eudi7Z3ZvefZ/H42l9FAQB8L1771H8/dQm7tfCaE6cuAWBS6+5p2AAAAPBwCcsAAAAwWFO3YXf3dUlOWbS9eYljF43H59uH7d8KAQAAWA+sLAMAAMBAWAYAAICBsAwAAAADYRkAAAAGwjIAAAAMhGUAAAAYCMsAAAAwEJYBAABgICwDAADAQFgGAACAgbAMAAAAA2EZAAAABsIyAAAADDZOXcBqc9DC92fhNSdOXQYAAAATsrIMAAAAA2EZAAAABsIyAAAADIRlAAAAGAjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgMHGqQtYbf7h5h256a2fmrqMVeu4V541dQkAAAD7nZVlAAAAGAjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgIGwDAAAAANhGQAAAAbCMgAAAAyEZQAAABgIywAAADAQlgEAAGAgLAMAAMBAWAYAAICBsAwAAACDAyosV9Unq+r0qesAAABgdTugwjIAAADsjVUflqtqU1V9pareWVVXVdUfV9UzquryqvpqVZ1RVYdW1buqaltVfb6qnj0/95Cqel9VXVlVf5LkkIk/DgAAAGvAxqkL2EsnJTk3yflJtiV5QZKnJzknyYVJvpTkz7v7JVV1RJIrqurjSV6W5O7uPrWqTk3yuUmqXyPe9Ok/yi13f3vZMRs+884ljy0sLGTLli37uiwAAIAVt1bC8rXdvT1JqurqJJ/o7q6q7Uk2JTkhyTlV9avz8QcneVySs5K8NUm6+8qqunJ3b15V52cWxHPCkcftz8+xqt1y97dz4123LT/orpWpBQAAYEprJSzfu+j1zkXbOzP7DPcneV53X7P4pKpKkt7Tm3f31iRbk+TJj3vSHsevV8c88og9jtlwxNJ3si8sLOzLcgAAACazVsLynlyS5IKqumC+4vyU7v58kk8leWGSy6rqlCSnTlrlKvfan3jRHscc98qzVqASAACAaa36B3ztpTcmOSjJlVV11Xw7Sd6R5LD57de/luSKieoDAABgDVn1K8vdfV2SUxZtb17i2Mt2c+49SZ6/XwsEAABg3VkvK8sAAACwzwjLAAAAMBCWAQAAYCAsAwAAwEBYBgAAgIGwDAAAAANhGQAAAAbCMgAAAAyEZQAAABgIywAAADAQlgEAAGAgLAMAAMBAWAYAAIDBxqkLWG0OOvawHPfKs6YuAwAAgAlZWQYAAICBsAwAAAADYRkAAAAGwjIAAAAMqrunrmFVqao7k1wzdR0cEI5OcuvURXBA0GusFL3GStJvrBS9tv6c2N3H7GmQp2Hv6pruPn3qIlj/quozeo2VoNdYKXqNlaTfWCl67cDlNmwAAAAYCMsAAAAwEJZ3tXXqAjhg6DVWil5jpeg1VpJ+Y6XotQOUB3wBAADAwMoyAAAADIRlAAAAGAjLc1X101V1TVV9rap+fep6WPuq6rqq2l5VX6iqz8z3/UBVXVpVX53/eeR8f1XVW+f9d2VV/di01bPaVdW7qurmqrpq0b6H3F9Vdd58/Fer6rwpPgur2xK99vqq+rv5/PaFqvqZRcdeO++1a6rqpxbt9z3LsqrqB6vqsqr6clVdXVWvmu83t7FPLdNr5jYexO8sJ6mqDUn+Oskzk1yfZFuSX+zuL01aGGtaVV2X5PTuvnXRvi1JbuvuN88n1CO7+z/MJ+MLkvxMkh9P8pbu/vEp6mZtqKqzkuxI8ofdfcp830Pqr6r6gSSfSXJ6kk7y2ST/pLu/NcFHYpVaotden2RHd//2MPZHkrw3yRlJHpvk40meOD/se5ZlVdVjkjymuz9XVY/KbE56TpLNMbexDy3Ta/8q5jYWsbI8c0aSr3X317v7u0nel+TZE9fE+vTsJO+Zv35PZhPzA/v/sGf+MskR84kcdqu7P5XktmH3Q+2vn0pyaXffNv8h8tIkP73/q2ctWaLXlvLsJO/r7nu7+9okX8vsO9b3LHvU3Td09+fmr+9M8uUkx8fcxj62TK8txdx2gBKWZ45P8o1F29dn+f9gYG90ko9V1Wer6vz5vuO6+4ZkNlEnOXa+Xw+yLzzU/tJ3PByvmN/6+q4HbouNXmMfqapNSZ6S5K9ibmM/GnotMbexiLA8U7vZ5/50Hq6ndfePJfkXSV4+v5VxKXqQ/Wmp/tJ3fK/ekeQJSU5LckOS35nv12s8bFV1WJIPJHl1d9+x3NDd7NNv7LXd9Jq5jQcRlmeuT/KDi7ZPSPLNiWphnejub87/vDnJBzO7VeemB26vnv9583y4HmRfeKj9pe/4nnT3Td19f3fvTPI/MpvfEr3Gw1RVB2UWXv64u/9svtvcxj63u14ztzESlme2JTm5qh5fVd+f5PlJLp64Jtawqjp0/sCIVNWhSZ6V5KrM+uqBp3Kel+TD89cXJ/ml+ZM9z0xy+wO3nMFD8FD765Ikz6qqI+e3mj1rvg+WNTxT4bmZzW/JrNeeX1WPqKrHJzk5yRXxPcteqKpK8j+TfLm7/+uiQ+Y29qmles3cxmjj1AWsBt19X1W9IrOJdEOSd3X31ROXxdp2XJIPzubibEzyv7r7o1W1Lcn7q+qlSf5fknPn4/93Zk/z/FqSu5O8eOVLZi2pqvcmOTvJ0VV1fZL/nOTNeQj91d23VdUbM/uyT5Lf6O69fZATB4gleu3sqjots9sNr0vysiTp7qur6v1JvpTkviQv7+775+/je5Y9eVqSFyXZXlVfmO+7MOY29r2leu0XzW0s5p+OAgAAgIHbsAEAAGAgLAMAAMBAWAYAAICBsAwAAAADYRkAAAAGwjIArDJV9ekVvt6mqnrBSl4TAFY7YRkAVpnu/omVulZVbUyyKYmwDACL+HeWAWCVqaod3X1YVZ2d5A1JbkpyWpI/S7I9yauSHJLkOd39N1V1UZLvJPnHSY5L8ivd/ZGqOjjJO5KcnuS++f7Lqmpzkn+Z5OAkhyZ5ZJIfTnJtkvck+WCSP5ofS5JXdPen5/W8PsmtSU5J8tkk/7q7u6qemuQt83PuTfLPk9yd5M1Jzk7yiCS/191/sI//ugBgv9g4dQEAwLKenFmQvS3J15O8s7vPqKpXJbkgyavn4zYl+WdJnpDksqo6KcnLk6S7f7SqnpTkY1X1xPn4f5rk1O6+bR6Cf7W7fzZJquqRSZ7Z3d+pqpOTvDezwJ0kT8kslH8zyeVJnlZVVyT5kyS/0N3bqurRSe5J8tIkt3f3U6vqEUkur6qPdfe1++HvCQD2KWEZAFa3bd19Q5JU1d8k+dh8//YkP7lo3Pu7e2eSr1bV15M8KcnTk7wtSbr7K1X1t0keCMuXdvdtS1zzoCS/W1WnJbl/0TlJckV3Xz+v5wuZhfTbk9zQ3dvm17pjfvxZSU6tqp+fn3t4kpMzW8EGgFVNWAaA1e3eRa93LtremQd/j4+/V9VJapn3vWuZY7+c2a3fT87s+SbfWaKe++c11G6un/n+C7r7kmWuBQCrkgd8AcD6cG5VfV9VPSHJDyW5JsmnkrwwSea3Xz9uvn90Z5JHLdo+PLOV4p1JXpRkwx6u/ZUkj53/3nKq6lHzB4ddkuTfVtVBD9RQVYcu8z4AsGpYWQaA9eGaJP83swd8/Zv57xu/PcnvV9X2zB7wtbm7763aZcH5yiT3VdUXk1yU5O1JPlBV5ya5LMuvQqe7v1tVv5DkbVV1SGa/r/yMJO/M7Dbtz9Xsorckec6++LAAsL95GjYArHHzp2F/pLv/dOpaAGC9cBs2AAAADKwsAwAAwMDKMgAAAAyEZQAAABgIywAAADAQlgEAAGAgLAMAAMDg/yiLkFEAAAAFSURBVAOTg5/Dsx7NVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'num_leaves': 128,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.2\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(\n",
    "    X,\n",
    "    X_test,\n",
    "    y,\n",
    "    params=params,\n",
    "    model_type='lgb',\n",
    "    plot_feature_importance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_model(input_dim=10):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(256, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(96, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #'rmsprop'\n",
    "    model.compile(optimizer=optimizer,loss='mae')\n",
    "    return model\n",
    "'''\n",
    "patience = 50\n",
    "call_ES = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=patience,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    #restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='rmsprop', loss='mae')\\n\\nhistory = model.fit_generator(train_gen,\\n                              steps_per_epoch=1000,\\n                              epochs=40,\\n                              verbose=2,\\n                              callbacks=cb,\\n                              validation_data=valid_gen,\\n                              validation_steps=200\\n                             )\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "def create_model(input_dim=10):\n",
    "\n",
    "    # The LSTM architecture\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with Dropout regularisation\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=(None, input_dim)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Second LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Fourth LSTM layer\n",
    "    model.add(CuDNNLSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    # The output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop', loss='mae')\n",
    "    return model\n",
    "\n",
    "    # Compile and fit model\n",
    "'''\n",
    "model.compile(optimizer='rmsprop', loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=40,\n",
    "                              verbose=2,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200\n",
    "                             )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "(20972, 14)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape[-1])\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97631842,  0.64285657,  1.39860834,  1.08713946, -0.33612565,\n",
       "        -0.35631039,  0.26153967,  0.2076963 ,  0.1436101 ,  1.73143967,\n",
       "         0.95573555, -0.18776256, -0.57443946,  0.15701396],\n",
       "       [ 0.02365161, -0.11287545, -0.55554979, -0.51437137, -0.47248075,\n",
       "        -0.45877415, -0.78084979, -0.05113359, -0.32919506,  1.15104347,\n",
       "         0.95573555,  0.290751  , -0.01152104, -0.14989879],\n",
       "       [ 0.21499489, -0.59747823, -0.55559324, -0.51450339, -0.47263474,\n",
       "        -0.459387  , -0.38667375, -0.09803637, -0.24054409,  2.15495302,\n",
       "         0.95573555,  0.13124648, -0.14226936, -0.11516405],\n",
       "       [-0.13488996, -0.01762173, -0.36869546, -0.32195906, -0.28285092,\n",
       "        -0.45761748, -0.70183658, -0.22272156, -0.32919506,  1.41591519,\n",
       "         0.95573555,  0.31353736, -0.15827081, -0.22646362],\n",
       "       [-0.63800453, -0.5923853 , -0.55576888, -0.47833191, -0.42470541,\n",
       "        -0.4577177 , -0.8652343 , -0.22702591, -0.43262118, -0.04011713,\n",
       "         0.95573555,  0.43506462, -0.07612292, -0.24675582]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 50)          13200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 74,451\n",
      "Trainable params: 74,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16777 samples, validate on 4195 samples\n",
      "Epoch 1/50\n",
      " - 9s - loss: 2.8249 - val_loss: 2.4333\n",
      "Epoch 2/50\n",
      " - 6s - loss: 2.4731 - val_loss: 2.3605\n",
      "Epoch 3/50\n",
      " - 6s - loss: 2.4418 - val_loss: 2.3548\n",
      "Epoch 4/50\n",
      " - 6s - loss: 2.4242 - val_loss: 2.3363\n",
      "Epoch 5/50\n",
      " - 6s - loss: 2.4213 - val_loss: 2.3838\n",
      "Epoch 6/50\n",
      " - 6s - loss: 2.4066 - val_loss: 2.3316\n",
      "Epoch 7/50\n",
      " - 6s - loss: 2.3947 - val_loss: 2.3912\n",
      "Epoch 8/50\n",
      " - 6s - loss: 2.3852 - val_loss: 2.3184\n",
      "Epoch 9/50\n",
      " - 6s - loss: 2.3861 - val_loss: 2.3154\n",
      "Epoch 10/50\n",
      " - 6s - loss: 2.3793 - val_loss: 2.3156\n",
      "Epoch 11/50\n",
      " - 6s - loss: 2.3777 - val_loss: 2.3168\n",
      "Epoch 12/50\n",
      " - 6s - loss: 2.3783 - val_loss: 2.3005\n",
      "Epoch 13/50\n",
      " - 6s - loss: 2.3731 - val_loss: 2.3140\n",
      "Epoch 14/50\n",
      " - 6s - loss: 2.3665 - val_loss: 2.2990\n",
      "Epoch 15/50\n",
      " - 6s - loss: 2.3685 - val_loss: 2.2971\n",
      "Epoch 16/50\n",
      " - 6s - loss: 2.3665 - val_loss: 2.2926\n",
      "Epoch 17/50\n",
      " - 6s - loss: 2.3669 - val_loss: 2.3413\n",
      "Epoch 18/50\n",
      " - 6s - loss: 2.3570 - val_loss: 2.2929\n",
      "Epoch 19/50\n",
      " - 6s - loss: 2.3537 - val_loss: 2.2767\n",
      "Epoch 20/50\n",
      " - 6s - loss: 2.3583 - val_loss: 2.2994\n",
      "Epoch 21/50\n",
      " - 6s - loss: 2.3520 - val_loss: 2.2773\n",
      "Epoch 22/50\n",
      " - 6s - loss: 2.3514 - val_loss: 2.2800\n",
      "Epoch 23/50\n",
      " - 6s - loss: 2.3460 - val_loss: 2.2744\n",
      "Epoch 24/50\n",
      " - 6s - loss: 2.3476 - val_loss: 2.3037\n",
      "Epoch 25/50\n",
      " - 6s - loss: 2.3477 - val_loss: 2.3050\n",
      "Epoch 26/50\n",
      " - 6s - loss: 2.3459 - val_loss: 2.3139\n",
      "Epoch 27/50\n",
      " - 6s - loss: 2.3331 - val_loss: 2.2849\n",
      "Epoch 28/50\n",
      " - 6s - loss: 2.3362 - val_loss: 2.2666\n",
      "Epoch 29/50\n",
      " - 6s - loss: 2.3333 - val_loss: 2.2889\n",
      "Epoch 30/50\n",
      " - 6s - loss: 2.3405 - val_loss: 2.2741\n",
      "Epoch 31/50\n",
      " - 6s - loss: 2.3326 - val_loss: 2.2652\n",
      "Epoch 32/50\n",
      " - 6s - loss: 2.3342 - val_loss: 2.2612\n",
      "Epoch 33/50\n",
      " - 6s - loss: 2.3273 - val_loss: 2.2744\n",
      "Epoch 34/50\n",
      " - 6s - loss: 2.3198 - val_loss: 2.2769\n",
      "Epoch 35/50\n",
      " - 6s - loss: 2.3255 - val_loss: 2.2605\n",
      "Epoch 36/50\n",
      " - 6s - loss: 2.3212 - val_loss: 2.2632\n",
      "Epoch 37/50\n",
      " - 6s - loss: 2.3210 - val_loss: 2.2633\n",
      "Epoch 38/50\n",
      " - 6s - loss: 2.3167 - val_loss: 2.2527\n",
      "Epoch 39/50\n",
      " - 6s - loss: 2.3191 - val_loss: 2.2611\n",
      "Epoch 40/50\n",
      " - 6s - loss: 2.3163 - val_loss: 2.2483\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.3096 - val_loss: 2.2526\n",
      "Epoch 42/50\n",
      " - 6s - loss: 2.3083 - val_loss: 2.2527\n",
      "Epoch 43/50\n",
      " - 6s - loss: 2.3103 - val_loss: 2.2798\n",
      "Epoch 44/50\n",
      " - 6s - loss: 2.3139 - val_loss: 2.2560\n",
      "Epoch 45/50\n",
      " - 6s - loss: 2.3024 - val_loss: 2.2355\n",
      "Epoch 46/50\n",
      " - 6s - loss: 2.3089 - val_loss: 2.2653\n",
      "Epoch 47/50\n",
      " - 6s - loss: 2.3053 - val_loss: 2.2507\n",
      "Epoch 48/50\n",
      " - 6s - loss: 2.3018 - val_loss: 2.2443\n",
      "Epoch 49/50\n",
      " - 6s - loss: 2.3012 - val_loss: 2.2348\n",
      "Epoch 50/50\n",
      " - 6s - loss: 2.2995 - val_loss: 2.2425\n",
      "loss: 2.825 | val_loss: 2.433 | diff: -0.392\n",
      "fold 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, None, 50)          13200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 74,451\n",
      "Trainable params: 74,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16777 samples, validate on 4195 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.8262 - val_loss: 2.5734\n",
      "Epoch 2/50\n",
      " - 6s - loss: 2.4644 - val_loss: 2.4415\n",
      "Epoch 3/50\n",
      " - 6s - loss: 2.4256 - val_loss: 2.4239\n",
      "Epoch 4/50\n",
      " - 6s - loss: 2.4111 - val_loss: 2.4254\n",
      "Epoch 5/50\n",
      " - 6s - loss: 2.3927 - val_loss: 2.4096\n",
      "Epoch 6/50\n",
      " - 6s - loss: 2.3883 - val_loss: 2.4150\n",
      "Epoch 7/50\n",
      " - 6s - loss: 2.3754 - val_loss: 2.4054\n",
      "Epoch 8/50\n",
      " - 6s - loss: 2.3700 - val_loss: 2.4125\n",
      "Epoch 9/50\n",
      " - 6s - loss: 2.3718 - val_loss: 2.4122\n",
      "Epoch 10/50\n",
      " - 6s - loss: 2.3513 - val_loss: 2.3849\n",
      "Epoch 11/50\n",
      " - 6s - loss: 2.3560 - val_loss: 2.3862\n",
      "Epoch 12/50\n",
      " - 6s - loss: 2.3537 - val_loss: 2.3798\n",
      "Epoch 13/50\n",
      " - 6s - loss: 2.3503 - val_loss: 2.3970\n",
      "Epoch 14/50\n",
      " - 6s - loss: 2.3461 - val_loss: 2.3803\n",
      "Epoch 15/50\n",
      " - 6s - loss: 2.3369 - val_loss: 2.3811\n",
      "Epoch 16/50\n",
      " - 6s - loss: 2.3333 - val_loss: 2.3759\n",
      "Epoch 17/50\n",
      " - 6s - loss: 2.3311 - val_loss: 2.3855\n",
      "Epoch 18/50\n",
      " - 6s - loss: 2.3323 - val_loss: 2.3805\n",
      "Epoch 19/50\n",
      " - 6s - loss: 2.3306 - val_loss: 2.3763\n",
      "Epoch 20/50\n",
      " - 6s - loss: 2.3285 - val_loss: 2.3705\n",
      "Epoch 21/50\n",
      " - 6s - loss: 2.3226 - val_loss: 2.3646\n",
      "Epoch 22/50\n",
      " - 6s - loss: 2.3223 - val_loss: 2.3597\n",
      "Epoch 23/50\n",
      " - 6s - loss: 2.3228 - val_loss: 2.3617\n",
      "Epoch 24/50\n",
      " - 6s - loss: 2.3259 - val_loss: 2.3632\n",
      "Epoch 25/50\n",
      " - 6s - loss: 2.3150 - val_loss: 2.3583\n",
      "Epoch 26/50\n",
      " - 6s - loss: 2.3235 - val_loss: 2.3711\n",
      "Epoch 27/50\n",
      " - 6s - loss: 2.3125 - val_loss: 2.3599\n",
      "Epoch 28/50\n",
      " - 6s - loss: 2.3062 - val_loss: 2.3724\n",
      "Epoch 29/50\n",
      " - 6s - loss: 2.3059 - val_loss: 2.3892\n",
      "Epoch 30/50\n",
      " - 6s - loss: 2.3092 - val_loss: 2.3511\n",
      "Epoch 31/50\n",
      " - 6s - loss: 2.3057 - val_loss: 2.3468\n",
      "Epoch 32/50\n",
      " - 6s - loss: 2.3059 - val_loss: 2.3567\n",
      "Epoch 33/50\n",
      " - 6s - loss: 2.3009 - val_loss: 2.3801\n",
      "Epoch 34/50\n",
      " - 6s - loss: 2.2964 - val_loss: 2.3624\n",
      "Epoch 35/50\n",
      " - 6s - loss: 2.2967 - val_loss: 2.3405\n",
      "Epoch 36/50\n",
      " - 6s - loss: 2.3003 - val_loss: 2.3566\n",
      "Epoch 37/50\n",
      " - 6s - loss: 2.2975 - val_loss: 2.3501\n",
      "Epoch 38/50\n",
      " - 6s - loss: 2.2965 - val_loss: 2.3368\n",
      "Epoch 39/50\n",
      " - 6s - loss: 2.2940 - val_loss: 2.3364\n",
      "Epoch 40/50\n",
      " - 6s - loss: 2.2852 - val_loss: 2.3346\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.2874 - val_loss: 2.3429\n",
      "Epoch 42/50\n",
      " - 6s - loss: 2.2845 - val_loss: 2.3742\n",
      "Epoch 43/50\n",
      " - 6s - loss: 2.2881 - val_loss: 2.3324\n",
      "Epoch 44/50\n",
      " - 6s - loss: 2.2892 - val_loss: 2.3405\n",
      "Epoch 45/50\n",
      " - 6s - loss: 2.2820 - val_loss: 2.3746\n",
      "Epoch 46/50\n",
      " - 6s - loss: 2.2852 - val_loss: 2.3816\n",
      "Epoch 47/50\n",
      " - 6s - loss: 2.2862 - val_loss: 2.3392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      " - 6s - loss: 2.2828 - val_loss: 2.3325\n",
      "Epoch 49/50\n",
      " - 6s - loss: 2.2835 - val_loss: 2.3455\n",
      "Epoch 50/50\n",
      " - 6s - loss: 2.2779 - val_loss: 2.3663\n",
      "loss: 2.826 | val_loss: 2.573 | diff: -0.253\n",
      "fold 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_9 (CuDNNLSTM)     (None, None, 50)          13200     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_11 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_12 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 74,451\n",
      "Trainable params: 74,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16778 samples, validate on 4194 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.8451 - val_loss: 2.4406\n",
      "Epoch 2/50\n",
      " - 6s - loss: 2.4757 - val_loss: 2.3780\n",
      "Epoch 3/50\n",
      " - 6s - loss: 2.4413 - val_loss: 2.3306\n",
      "Epoch 4/50\n",
      " - 6s - loss: 2.4180 - val_loss: 2.3323\n",
      "Epoch 5/50\n",
      " - 6s - loss: 2.4143 - val_loss: 2.3466\n",
      "Epoch 6/50\n",
      " - 6s - loss: 2.4076 - val_loss: 2.3235\n",
      "Epoch 7/50\n",
      " - 6s - loss: 2.3913 - val_loss: 2.3269\n",
      "Epoch 8/50\n",
      " - 6s - loss: 2.3998 - val_loss: 2.3276\n",
      "Epoch 9/50\n",
      " - 6s - loss: 2.3889 - val_loss: 2.3160\n",
      "Epoch 10/50\n",
      " - 6s - loss: 2.3787 - val_loss: 2.3176\n",
      "Epoch 11/50\n",
      " - 6s - loss: 2.3826 - val_loss: 2.3100\n",
      "Epoch 12/50\n",
      " - 6s - loss: 2.3769 - val_loss: 2.3170\n",
      "Epoch 13/50\n",
      " - 6s - loss: 2.3770 - val_loss: 2.3131\n",
      "Epoch 14/50\n",
      " - 6s - loss: 2.3614 - val_loss: 2.3360\n",
      "Epoch 15/50\n",
      " - 6s - loss: 2.3614 - val_loss: 2.3276\n",
      "Epoch 16/50\n",
      " - 6s - loss: 2.3583 - val_loss: 2.3104\n",
      "Epoch 17/50\n",
      " - 6s - loss: 2.3530 - val_loss: 2.3159\n",
      "Epoch 18/50\n",
      " - 6s - loss: 2.3535 - val_loss: 2.2951\n",
      "Epoch 19/50\n",
      " - 6s - loss: 2.3472 - val_loss: 2.2980\n",
      "Epoch 20/50\n",
      " - 6s - loss: 2.3543 - val_loss: 2.3206\n",
      "Epoch 21/50\n",
      " - 6s - loss: 2.3437 - val_loss: 2.3062\n",
      "Epoch 22/50\n",
      " - 6s - loss: 2.3416 - val_loss: 2.3208\n",
      "Epoch 23/50\n",
      " - 6s - loss: 2.3352 - val_loss: 2.2873\n",
      "Epoch 24/50\n",
      " - 6s - loss: 2.3378 - val_loss: 2.3096\n",
      "Epoch 25/50\n",
      " - 6s - loss: 2.3345 - val_loss: 2.2994\n",
      "Epoch 26/50\n",
      " - 6s - loss: 2.3306 - val_loss: 2.2775\n",
      "Epoch 27/50\n",
      " - 6s - loss: 2.3335 - val_loss: 2.2785\n",
      "Epoch 28/50\n",
      " - 6s - loss: 2.3296 - val_loss: 2.2828\n",
      "Epoch 29/50\n",
      " - 6s - loss: 2.3282 - val_loss: 2.2899\n",
      "Epoch 30/50\n",
      " - 6s - loss: 2.3296 - val_loss: 2.2951\n",
      "Epoch 31/50\n",
      " - 6s - loss: 2.3234 - val_loss: 2.2747\n",
      "Epoch 32/50\n",
      " - 6s - loss: 2.3222 - val_loss: 2.2913\n",
      "Epoch 33/50\n",
      " - 6s - loss: 2.3238 - val_loss: 2.2771\n",
      "Epoch 34/50\n",
      " - 6s - loss: 2.3250 - val_loss: 2.2853\n",
      "Epoch 35/50\n",
      " - 6s - loss: 2.3113 - val_loss: 2.2773\n",
      "Epoch 36/50\n",
      " - 6s - loss: 2.3055 - val_loss: 2.2746\n",
      "Epoch 37/50\n",
      " - 6s - loss: 2.3091 - val_loss: 2.2649\n",
      "Epoch 38/50\n",
      " - 6s - loss: 2.3148 - val_loss: 2.2636\n",
      "Epoch 39/50\n",
      " - 6s - loss: 2.3068 - val_loss: 2.2645\n",
      "Epoch 40/50\n",
      " - 6s - loss: 2.3086 - val_loss: 2.2699\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.3094 - val_loss: 2.2734\n",
      "Epoch 42/50\n",
      " - 6s - loss: 2.3072 - val_loss: 2.2784\n",
      "Epoch 43/50\n",
      " - 6s - loss: 2.3030 - val_loss: 2.2568\n",
      "Epoch 44/50\n",
      " - 6s - loss: 2.2938 - val_loss: 2.2762\n",
      "Epoch 45/50\n",
      " - 6s - loss: 2.3035 - val_loss: 2.2634\n",
      "Epoch 46/50\n",
      " - 6s - loss: 2.3061 - val_loss: 2.2649\n",
      "Epoch 47/50\n",
      " - 6s - loss: 2.2910 - val_loss: 2.2522\n",
      "Epoch 48/50\n",
      " - 6s - loss: 2.2886 - val_loss: 2.2595\n",
      "Epoch 49/50\n",
      " - 6s - loss: 2.2922 - val_loss: 2.2507\n",
      "Epoch 50/50\n",
      " - 6s - loss: 2.2895 - val_loss: 2.2549\n",
      "loss: 2.845 | val_loss: 2.441 | diff: -0.404\n",
      "fold 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_13 (CuDNNLSTM)    (None, None, 50)          13200     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_14 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 74,451\n",
      "Trainable params: 74,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16778 samples, validate on 4194 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 2.8347 - val_loss: 2.4589\n",
      "Epoch 2/50\n",
      " - 6s - loss: 2.4623 - val_loss: 2.4219\n",
      "Epoch 3/50\n",
      " - 6s - loss: 2.4285 - val_loss: 2.4140\n",
      "Epoch 4/50\n",
      " - 6s - loss: 2.4122 - val_loss: 2.4184\n",
      "Epoch 5/50\n",
      " - 6s - loss: 2.3937 - val_loss: 2.3894\n",
      "Epoch 6/50\n",
      " - 6s - loss: 2.3776 - val_loss: 2.3830\n",
      "Epoch 7/50\n",
      " - 6s - loss: 2.3676 - val_loss: 2.3814\n",
      "Epoch 8/50\n",
      " - 6s - loss: 2.3750 - val_loss: 2.4017\n",
      "Epoch 9/50\n",
      " - 6s - loss: 2.3649 - val_loss: 2.3903\n",
      "Epoch 10/50\n",
      " - 6s - loss: 2.3602 - val_loss: 2.3655\n",
      "Epoch 11/50\n",
      " - 6s - loss: 2.3584 - val_loss: 2.3688\n",
      "Epoch 12/50\n",
      " - 6s - loss: 2.3517 - val_loss: 2.3814\n",
      "Epoch 13/50\n",
      " - 6s - loss: 2.3426 - val_loss: 2.3933\n",
      "Epoch 14/50\n",
      " - 6s - loss: 2.3417 - val_loss: 2.3793\n",
      "Epoch 15/50\n",
      " - 6s - loss: 2.3485 - val_loss: 2.3635\n",
      "Epoch 16/50\n",
      " - 6s - loss: 2.3296 - val_loss: 2.3821\n",
      "Epoch 17/50\n",
      " - 6s - loss: 2.3274 - val_loss: 2.3628\n",
      "Epoch 18/50\n",
      " - 6s - loss: 2.3289 - val_loss: 2.3556\n",
      "Epoch 19/50\n",
      " - 6s - loss: 2.3241 - val_loss: 2.4092\n",
      "Epoch 20/50\n",
      " - 6s - loss: 2.3268 - val_loss: 2.3551\n",
      "Epoch 21/50\n",
      " - 6s - loss: 2.3193 - val_loss: 2.3476\n",
      "Epoch 22/50\n",
      " - 6s - loss: 2.3164 - val_loss: 2.3521\n",
      "Epoch 23/50\n",
      " - 6s - loss: 2.3223 - val_loss: 2.3620\n",
      "Epoch 24/50\n",
      " - 6s - loss: 2.3225 - val_loss: 2.3640\n",
      "Epoch 25/50\n",
      " - 6s - loss: 2.3112 - val_loss: 2.3651\n",
      "Epoch 26/50\n",
      " - 6s - loss: 2.3153 - val_loss: 2.3538\n",
      "Epoch 27/50\n",
      " - 6s - loss: 2.3106 - val_loss: 2.3408\n",
      "Epoch 28/50\n",
      " - 6s - loss: 2.3106 - val_loss: 2.3575\n",
      "Epoch 29/50\n",
      " - 6s - loss: 2.3112 - val_loss: 2.3400\n",
      "Epoch 30/50\n",
      " - 6s - loss: 2.3035 - val_loss: 2.3436\n",
      "Epoch 31/50\n",
      " - 6s - loss: 2.3070 - val_loss: 2.3436\n",
      "Epoch 32/50\n",
      " - 6s - loss: 2.2998 - val_loss: 2.3536\n",
      "Epoch 33/50\n",
      " - 6s - loss: 2.2970 - val_loss: 2.3472\n",
      "Epoch 34/50\n",
      " - 6s - loss: 2.3043 - val_loss: 2.3435\n",
      "Epoch 35/50\n",
      " - 6s - loss: 2.2972 - val_loss: 2.3730\n",
      "Epoch 36/50\n",
      " - 6s - loss: 2.2990 - val_loss: 2.3266\n",
      "Epoch 37/50\n",
      " - 6s - loss: 2.2901 - val_loss: 2.3378\n",
      "Epoch 38/50\n",
      " - 6s - loss: 2.2872 - val_loss: 2.3449\n",
      "Epoch 39/50\n",
      " - 6s - loss: 2.2876 - val_loss: 2.3289\n",
      "Epoch 40/50\n",
      " - 6s - loss: 2.2877 - val_loss: 2.3417\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.2892 - val_loss: 2.3341\n",
      "Epoch 42/50\n",
      " - 6s - loss: 2.2884 - val_loss: 2.3510\n",
      "Epoch 43/50\n",
      " - 6s - loss: 2.2867 - val_loss: 2.3677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      " - 6s - loss: 2.2824 - val_loss: 2.3323\n",
      "Epoch 45/50\n",
      " - 6s - loss: 2.2928 - val_loss: 2.3329\n",
      "Epoch 46/50\n",
      " - 6s - loss: 2.2768 - val_loss: 2.3253\n",
      "Epoch 47/50\n",
      " - 6s - loss: 2.2752 - val_loss: 2.3508\n",
      "Epoch 48/50\n",
      " - 6s - loss: 2.2788 - val_loss: 2.3309\n",
      "Epoch 49/50\n",
      " - 6s - loss: 2.2838 - val_loss: 2.3295\n",
      "Epoch 50/50\n",
      " - 6s - loss: 2.2762 - val_loss: 2.3261\n",
      "loss: 2.835 | val_loss: 2.459 | diff: -0.376\n",
      "fold 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, None, 50)          13200     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)    (None, 50)                20400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 74,451\n",
      "Trainable params: 74,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16778 samples, validate on 4194 samples\n",
      "Epoch 1/50\n",
      " - 8s - loss: 2.8163 - val_loss: 2.4104\n",
      "Epoch 2/50\n",
      " - 6s - loss: 2.4675 - val_loss: 2.3649\n",
      "Epoch 3/50\n",
      " - 6s - loss: 2.4422 - val_loss: 2.3954\n",
      "Epoch 4/50\n",
      " - 6s - loss: 2.4273 - val_loss: 2.4036\n",
      "Epoch 5/50\n",
      " - 6s - loss: 2.4038 - val_loss: 2.3329\n",
      "Epoch 6/50\n",
      " - 6s - loss: 2.4066 - val_loss: 2.3305\n",
      "Epoch 7/50\n",
      " - 6s - loss: 2.3930 - val_loss: 2.3657\n",
      "Epoch 8/50\n",
      " - 6s - loss: 2.3897 - val_loss: 2.3173\n",
      "Epoch 9/50\n",
      " - 6s - loss: 2.3743 - val_loss: 2.3421\n",
      "Epoch 10/50\n",
      " - 6s - loss: 2.3722 - val_loss: 2.3317\n",
      "Epoch 11/50\n",
      " - 6s - loss: 2.3640 - val_loss: 2.3216\n",
      "Epoch 12/50\n",
      " - 6s - loss: 2.3636 - val_loss: 2.3179\n",
      "Epoch 13/50\n",
      " - 6s - loss: 2.3657 - val_loss: 2.3327\n",
      "Epoch 14/50\n",
      " - 6s - loss: 2.3542 - val_loss: 2.3091\n",
      "Epoch 15/50\n",
      " - 6s - loss: 2.3541 - val_loss: 2.3068\n",
      "Epoch 16/50\n",
      " - 6s - loss: 2.3565 - val_loss: 2.3007\n",
      "Epoch 17/50\n",
      " - 6s - loss: 2.3534 - val_loss: 2.2921\n",
      "Epoch 18/50\n",
      " - 6s - loss: 2.3474 - val_loss: 2.3023\n",
      "Epoch 19/50\n",
      " - 6s - loss: 2.3432 - val_loss: 2.3238\n",
      "Epoch 20/50\n",
      " - 6s - loss: 2.3472 - val_loss: 2.2978\n",
      "Epoch 21/50\n",
      " - 6s - loss: 2.3407 - val_loss: 2.3035\n",
      "Epoch 22/50\n",
      " - 6s - loss: 2.3410 - val_loss: 2.2796\n",
      "Epoch 23/50\n",
      " - 6s - loss: 2.3421 - val_loss: 2.3100\n",
      "Epoch 24/50\n",
      " - 6s - loss: 2.3374 - val_loss: 2.2847\n",
      "Epoch 25/50\n",
      " - 6s - loss: 2.3271 - val_loss: 2.2900\n",
      "Epoch 26/50\n",
      " - 6s - loss: 2.3198 - val_loss: 2.3078\n",
      "Epoch 27/50\n",
      " - 6s - loss: 2.3361 - val_loss: 2.3040\n",
      "Epoch 28/50\n",
      " - 6s - loss: 2.3270 - val_loss: 2.3013\n",
      "Epoch 29/50\n",
      " - 6s - loss: 2.3245 - val_loss: 2.2766\n",
      "Epoch 30/50\n",
      " - 6s - loss: 2.3192 - val_loss: 2.2844\n",
      "Epoch 31/50\n",
      " - 6s - loss: 2.3225 - val_loss: 2.2760\n",
      "Epoch 32/50\n",
      " - 6s - loss: 2.3193 - val_loss: 2.2817\n",
      "Epoch 33/50\n",
      " - 6s - loss: 2.3182 - val_loss: 2.2884\n",
      "Epoch 34/50\n",
      " - 6s - loss: 2.3190 - val_loss: 2.3133\n",
      "Epoch 35/50\n",
      " - 6s - loss: 2.3079 - val_loss: 2.2689\n",
      "Epoch 36/50\n",
      " - 6s - loss: 2.3145 - val_loss: 2.2812\n",
      "Epoch 37/50\n",
      " - 6s - loss: 2.2993 - val_loss: 2.2661\n",
      "Epoch 38/50\n",
      " - 6s - loss: 2.3082 - val_loss: 2.2588\n",
      "Epoch 39/50\n",
      " - 6s - loss: 2.3052 - val_loss: 2.2578\n",
      "Epoch 40/50\n",
      " - 6s - loss: 2.3101 - val_loss: 2.2942\n",
      "Epoch 41/50\n",
      " - 6s - loss: 2.3001 - val_loss: 2.2643\n",
      "Epoch 42/50\n",
      " - 6s - loss: 2.2981 - val_loss: 2.2923\n",
      "Epoch 43/50\n",
      " - 6s - loss: 2.3030 - val_loss: 2.2589\n",
      "Epoch 44/50\n",
      " - 6s - loss: 2.2928 - val_loss: 2.2632\n",
      "Epoch 45/50\n",
      " - 6s - loss: 2.2951 - val_loss: 2.2865\n",
      "Epoch 46/50\n",
      " - 6s - loss: 2.2999 - val_loss: 2.2716\n",
      "Epoch 47/50\n",
      " - 6s - loss: 2.2972 - val_loss: 2.2585\n",
      "Epoch 48/50\n",
      " - 6s - loss: 2.2991 - val_loss: 2.2590\n",
      "Epoch 49/50\n",
      " - 6s - loss: 2.2903 - val_loss: 2.2714\n",
      "Epoch 50/50\n",
      " - 6s - loss: 2.2905 - val_loss: 2.2694\n",
      "loss: 2.816 | val_loss: 2.410 | diff: -0.406\n",
      "After 5 test_CV = 2.292 | train_CV = 2.829 | -0.538 CPU times: user 29min 56s, sys: 2min 34s, total: 32min 30s\n",
      "Wall time: 25min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "NN_oof = np.zeros(len(train_X))\n",
    "train_score = []\n",
    "fold_idxs = []\n",
    "\n",
    "NN_predictions = np.zeros(len(test_X))\n",
    "\n",
    "num_of_features = train_X.shape[-1]\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    fold_idxs.append(val_idx)\n",
    "    \n",
    "    ## X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "    X_tr, X_val = train_X[trn_idx], train_X[val_idx]\n",
    "    X_tr = X_tr.reshape(len(X_tr), 1, num_of_features)\n",
    "    X_val = X_val.reshape(len(X_val), 1, num_of_features)\n",
    "    y_tr, y_val = train_y[trn_idx], train_y[val_idx]\n",
    "    model = create_model(num_of_features)\n",
    "    model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=2, callbacks=[call_ES,], validation_data=[X_val, y_val]) #\n",
    "    \n",
    "    NN_oof[val_idx] = model.predict(X_val)[:,0]\n",
    "    \n",
    "    #NN_predictions += model.predict(test_X[train_columns])[:,0] / folds.n_splits\n",
    "    test_X = test_X.reshape(len(test_X), 1, num_of_features)\n",
    "    NN_predictions += model.predict(test_X)[:,0] / folds.n_splits\n",
    "    history = model.history.history\n",
    "    tr_loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n",
    "    train_score.append(tr_loss[-patience])\n",
    "#     break\n",
    "    \n",
    "cv_score = mean_absolute_error(train_y, NN_oof)\n",
    "print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(NN_predictions) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(NN_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#today = str(datetime.date.today())\n",
    "#submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "#submission[\"time_to_failure\"] = NN_predictions\n",
    "#submission.to_csv(f'NN_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "#submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_parameters = {'tournament_size': 17, 'population_size': 4000, 'p_crossover': 0.8, 'generations': 18}\n",
    "# function_set = ('add', 'sub', 'mul', 'div', \"sqrt\", \"log\", \"max\", \"min\", \"sin\", \"cos\", \"tan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # n_fold = 5\n",
    "# folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# GPL_oof = np.zeros(len(train_X))\n",
    "# GPL_predictions = np.zeros(len(test_X))\n",
    "# train_score = []\n",
    "\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n",
    "#     strLog = \"fold {}\".format(fold_)\n",
    "#     print(strLog)\n",
    "    \n",
    "#     X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n",
    "#     y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "#     model = SymbolicRegressor(**best_parameters, stopping_criteria=0.0,const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', \n",
    "#                           function_set=function_set, metric='mean absolute error', parsimony_coefficient=0.001,\n",
    "#                           p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, \n",
    "#                           p_point_replace=0.05, max_samples=1.0, feature_names=None, \n",
    "#                           warm_start=False, low_memory=False, n_jobs=-1, verbose=1, random_state=42)\n",
    "    \n",
    "#     model.fit(X_tr, y_tr) #\n",
    "    \n",
    "#     GPL_oof[val_idx] = model.predict(X_val)\n",
    "#     GPL_predictions += model.predict(test_X[train_columns]) / folds.n_splits\n",
    "    \n",
    "#     train_score.append(model.run_details_[\"best_fitness\"][-1])\n",
    "# #     break\n",
    "    \n",
    "# cv_score = mean_absolute_error(train_y, GPL_oof)\n",
    "# print(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = str(datetime.date.today())\n",
    "# submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n",
    "\n",
    "# submission[\"time_to_failure\"] = GPL_predictions\n",
    "# submission.to_csv(f'GPL_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scirpus_prediction = pd.read_csv(\"../input/andrews-new-script-plus-a-genetic-program-model/gpI.csv\")\n",
    "#Scirpus_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(prediction_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_00030f</td>\n",
       "      <td>2.449471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_0012b5</td>\n",
       "      <td>5.942364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_00184e</td>\n",
       "      <td>6.689695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_003339</td>\n",
       "      <td>8.646187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_0042cc</td>\n",
       "      <td>5.622608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id  time_to_failure\n",
       "0  seg_00030f         2.449471\n",
       "1  seg_0012b5         5.942364\n",
       "2  seg_00184e         6.689695\n",
       "3  seg_003339         8.646187\n",
       "4  seg_0042cc         5.622608"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = str(datetime.date.today())\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "submission[\"time_to_failure\"] = (prediction_xgb + prediction_lgb + NN_predictions) / 3\n",
    "submission.to_csv(f'xgb_lgb_nn_2_{today}_submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
