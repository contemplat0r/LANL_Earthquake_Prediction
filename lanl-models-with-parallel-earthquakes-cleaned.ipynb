{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(\n",
    "        df,\n",
    "        first_index=None,\n",
    "        last_index=None,\n",
    "        sample_size=150000,\n",
    "        holdout_size=50000,\n",
    "        smootch_windows_size = (3, 5, 7)\n",
    "    ):\n",
    "    \n",
    "    if first_index == None or last_index == None:\n",
    "        first_index = df.index.min()\n",
    "        last_index = df.index.max()\n",
    "    sample_indexes = np.random.randint(first_index, last_index, sample_size)\n",
    "    sample_indexes.sort()\n",
    "    #print(\"sample_indexes.shape:\", sample_indexes.shape)    \n",
    "    smootch_feature_names = ['smootch_mean_ws_{}'.format(window_size) for window_size in smootch_windows_size]\n",
    "    #half_windows_size = [ws // 2 for ws in smootch_windows_size]\n",
    "\n",
    "    acoustic_data_series = df['acoustic_data']\n",
    "    \n",
    "    sample_df = df.iloc[sample_indexes]\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True) # There is need map sample_df.index -> sample_indexes\n",
    "                                                    # Должно быть установелнно соотвествие индексов в sample_df\n",
    "                                                    # (Где после .reset_index индексы - это\n",
    "                                                    # все целые числа от 0 до sample_df.shape[0]) и sample_indexes -\n",
    "                                                    # соответствующие индексы в df которые являються случайной\n",
    "                                                    # выборкой из чисел о first_index до last_index и длинной \n",
    "                                                    # равной sample_df.index.shape[0] которая длинна равно sample_size \n",
    "                                                    # Соответсвенно, по видимому, in_window_begin_indexes и\n",
    "                                                    # in_window_end_indexes надо вычислять как то по другому\n",
    "    #print(\"just after create sample_df, sample_df.shape[0]:\", sample_df.shape[0])\n",
    "    \n",
    "    #for feature_name in smootch_feature_names:\n",
    "    #    sample_df[feature_name] = 0\n",
    "    #begin_smootch_features_value = []\n",
    "    #end_smootch_features_value = []\n",
    "    \n",
    "    #sample_indexes_set = set(sample_indexes)\n",
    "    begin_indexes_set = set()\n",
    "    end_indexes_set = set()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #print(\"sample_df.shape[0] just before main loop:\", sample_df.shape[0])\n",
    "    sample_df_len = sample_df.shape[0]\n",
    "    for window_size, feature_name in zip(smootch_windows_size, smootch_feature_names):\n",
    "        \n",
    "        print(\"\\n\" * 2)\n",
    "        print(\"window_size:\", window_size)\n",
    "        print()\n",
    "        feature_values_list = list(range(sample_size))\n",
    "        print(\"sample_df.index.tolist()[:window_size]:\\n\", sample_df.index.tolist()[:window_size])\n",
    "        print(\"df.index.tolist()[:window_size]:\\n\", df.index.tolist()[:window_size])\n",
    "\n",
    "        print(\"sample_df.index.tolist()[-window_size:]:\\n\", sample_df.index.tolist()[-window_size:])\n",
    "        print(\"df.index.tolist()[-window_size:]:\\n\", df.index.tolist()[-window_size:])\n",
    "        \n",
    "        half_window_size = window_size // 2\n",
    "        ##>begin_indexes = sample_indexes[:half_window_size]  \n",
    "        ##>print(\"begin_indexes:\\n\", begin_indexes)\n",
    "        sample_begin_indexes = sample_indexes[:half_window_size] #? sample_df.index[:half_window_size]\n",
    "        print(\"sample_begin_indexes:\\n\", sample_begin_indexes)\n",
    "        #full_data_begin_indexes = df.index[:half_window_size].tolist() #df.index[sample_indexes[i]]\n",
    "        full_data_begin_indexes = set(df.index[sample_begin_indexes].tolist())\n",
    "        print(\"full_data_begin_indexes:\\n\", full_data_begin_indexes)\n",
    "        \n",
    "        #in_window_begin_indexes = [idx for idx in sample_begin_indexes if idx in full_data_begin_indexes]\n",
    "        min_begin_index = min(full_data_begin_indexes)\n",
    "        in_window_begin_indexes = full_data_begin_indexes.intersection(\n",
    "            set(range(min_begin_index, min_begin_index + half_window_size))\n",
    "        )\n",
    "        print(\"in_window_begin_indexes:\\n\", in_window_begin_indexes)\n",
    "        \n",
    "        sample_end_indexes = sample_indexes[-half_window_size:]\n",
    "        print(\"sample_end_indexes:\\n\", sample_end_indexes)\n",
    "\n",
    "        #full_data_end_indexes = df.index[-half_window_size:].tolist()\n",
    "        full_data_end_indexes = set(df.index[sample_end_indexes].tolist())\n",
    "        print(\"full_data_end_indexes:\", full_data_end_indexes)\n",
    "        \n",
    "        #in_window_end_indexes = [idx for idx in sample_end_indexes if idx in full_data_end_indexes]\n",
    "        max_end_index = max(full_data_end_indexes)\n",
    "        in_window_end_indexes = full_data_end_indexes.intersection(\n",
    "            set(range(max_end_index - half_window_size, max_end_index))\n",
    "        )\n",
    "        print(\"in_window_end_indexes:\\n\", in_window_end_indexes)\n",
    "        \n",
    "        ##>begin_indexes = begin_indexes[begin_indexes <= half_window_size]\n",
    "        ##>print(\"2 begin_indexes? :\\n\", begin_indexes)\n",
    "        \n",
    "        ##>end_indexes = sample_indexes[-half_window_size:]\n",
    "        ##>print(\"2 end_indexes? :\\n\", end_indexes)\n",
    "        \n",
    "        if in_window_begin_indexes:\n",
    "            begin_indexes_set.union(in_window_begin_indexes)\n",
    "            for i, b_idx in enumerate(sorted(tuple(in_window_begin_indexes))):\n",
    "                print(\"i: {}, b_idx {}:\".format(i, b_idx))\n",
    "                print(\"type(b_idx):\", type(b_idx))\n",
    "                print(\"df[:window_size]:\\n\", df[:window_size])\n",
    "                #sample_df[feature_name].iloc[b_idx] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "                value = df.iloc[b_idx]['acoustic_data']\n",
    "                value = value - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "                feature_values_list[i] = value\n",
    "        if in_window_end_indexes:\n",
    "            end_indexes_set.union(in_window_end_indexes)\n",
    "            for i, e_idx in enumerate(sorted(tuple(in_window_end_indexes))):\n",
    "                print(\"i: {}, e_idx {}:\".format(i, e_idx))\n",
    "                print(\"type(e_idx):\", type(e_idx))\n",
    "                print(\"df[-window_size:]:\\n\", df[-window_size:])\n",
    "                #sample_df[feature_name].iloc[e_idx] = df[e_idx]['acoustic_data'] - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "                value = df.iloc[e_idx]['acoustic_data']\n",
    "                value = value - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "                feature_values_list[-i] = value\n",
    "        #print(\"in main loop, sample_df.shape:\", sample_df.shape)\n",
    "        slice_begin = len(begin_indexes_set)\n",
    "        slice_end = sample_df_len - len(end_indexes_set)\n",
    "        for i in range(sample_df_len)[slice_begin:slice_end]:\n",
    "            sample_idx = sample_indexes[i]\n",
    "            feature_values_list[i] = acoustic_data_series.iloc[sample_idx - half_window_size:sample_idx + half_window_size].mean()\n",
    "        #print(\"sample_df.shape[0] before assign feature_values_list:\", sample_df.shape[0])\n",
    "        #print(\"len(feature_values_list):\", len(feature_values_list))\n",
    "        sample_df[feature_name] = feature_values_list\n",
    "        \n",
    "    #sample_df_indexes_set = set(sample_df.index)\n",
    "    train_indexes = sample_df_indexes_set\n",
    "    holdout_df = None\n",
    "    if holdout_size > 0:\n",
    "        holdout_indexes = np.random.randint(0, sample_df.shape[0], holdout_size)\n",
    "        #print(\"sample_df.index:\\n\", sample_df.index.tolist())\n",
    "        #print(\"holdout_indexes:\\n\", holdout_indexes)\n",
    "        #train_indexes = np.array(sample_df.index)\n",
    "        holdout_df = sample_df.iloc[holdout_indexes]\n",
    "        holdout_df.reset_index(inplace=True)\n",
    "        holdout_df.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "        #train_indexes = sorted(tuple(sample_df_indexes_set.difference(set(holdout_indexes))))\n",
    "        train_indexes = sorted(tuple(set(sample_df.index).difference(set(holdout_indexes))))\n",
    "        #print(\"train_indexes:\\n\", train_indexes)\n",
    "        sample_df = sample_df.iloc[train_indexes]\n",
    "        sample_df.reset_index(inplace=True)\n",
    "        sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"Full calculation feature value time (with slicing) {} min:\".format((time.time() - start_time) / 60))\n",
    "    return sample_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(\n",
    "        df,\n",
    "        first_index=None,\n",
    "        last_index=None,\n",
    "        sample_size=150000,\n",
    "        holdout_size=50000,\n",
    "        smootch_windows_size = (3, 5, 7)\n",
    "    ):\n",
    "    \n",
    "    if first_index == None or last_index == None:\n",
    "        first_index = df.index.min()\n",
    "        last_index = df.index.max()\n",
    "    sample_indexes = np.random.randint(first_index, last_index, sample_size)\n",
    "    sample_indexes.sort()\n",
    "    #print(\"sample_indexes.shape:\", sample_indexes.shape)    \n",
    "    smootch_feature_names = ['smootch_mean_ws_{}'.format(window_size) for window_size in smootch_windows_size]\n",
    "    #half_windows_size = [ws // 2 for ws in smootch_windows_size]\n",
    "\n",
    "    acoustic_data_series = df['acoustic_data']\n",
    "    \n",
    "    sample_df = df.iloc[sample_indexes]\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True) # There is need map sample_df.index -> sample_indexes\n",
    "                                                    # Должно быть установелнно соотвествие индексов в sample_df\n",
    "                                                    # (Где после .reset_index индексы - это\n",
    "                                                    # все целые числа от 0 до sample_df.shape[0]) и sample_indexes -\n",
    "                                                    # соответствующие индексы в df которые являються случайной\n",
    "                                                    # выборкой из чисел о first_index до last_index и длинной \n",
    "                                                    # равной sample_df.index.shape[0] которая длинна равно sample_size \n",
    "                                                    # Соответсвенно, по видимому, in_window_begin_indexes и\n",
    "                                                    # in_window_end_indexes надо вычислять как то по другому\n",
    "    #print(\"just after create sample_df, sample_df.shape[0]:\", sample_df.shape[0])\n",
    "    \n",
    "    #for feature_name in smootch_feature_names:\n",
    "    #    sample_df[feature_name] = 0\n",
    "    #begin_smootch_features_value = []\n",
    "    #end_smootch_features_value = []\n",
    "    \n",
    "    #sample_indexes_set = set(sample_indexes)\n",
    "    begin_indexes_set = set()\n",
    "    end_indexes_set = set()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #print(\"sample_df.shape[0] just before main loop:\", sample_df.shape[0])\n",
    "    sample_df_len = sample_df.shape[0]\n",
    "    for window_size, feature_name in zip(smootch_windows_size, smootch_feature_names):\n",
    "        \n",
    "        print(\"\\n\" * 2)\n",
    "        print(\"window_size:\", window_size)\n",
    "        print()\n",
    "        feature_values_list = list(range(sample_size))\n",
    "        print(\"sample_df.index.tolist()[:window_size]:\\n\", sample_df.index.tolist()[:window_size])\n",
    "        print(\"df.index.tolist()[:window_size]:\\n\", df.index.tolist()[:window_size])\n",
    "\n",
    "        print(\"sample_df.index.tolist()[-window_size:]:\\n\", sample_df.index.tolist()[-window_size:])\n",
    "        print(\"df.index.tolist()[-window_size:]:\\n\", df.index.tolist()[-window_size:])\n",
    "        \n",
    "        half_window_size = window_size // 2\n",
    "        ##>begin_indexes = sample_indexes[:half_window_size]  \n",
    "        ##>print(\"begin_indexes:\\n\", begin_indexes)\n",
    "        sample_begin_indexes = sample_indexes[:half_window_size] #? sample_df.index[:half_window_size]\n",
    "        print(\"sample_begin_indexes:\\n\", sample_begin_indexes)\n",
    "        #full_data_begin_indexes = df.index[:half_window_size].tolist() #df.index[sample_indexes[i]]\n",
    "        full_data_begin_indexes = df.index[sample_begin_indexes].tolist()\n",
    "        print(\"full_data_begin_indexes:\\n\", full_data_begin_indexes)\n",
    "        \n",
    "        in_window_begin_indexes = [idx for idx in sample_begin_indexes if idx in full_data_begin_indexes]\n",
    "        print(\"in_window_begin_indexes:\\n\", in_window_begin_indexes)\n",
    "        \n",
    "        sample_end_indexes = sample_indexes[-half_window_size:]\n",
    "        print(\"sample_end_indexes:\\n\", sample_end_indexes)\n",
    "\n",
    "        #full_data_end_indexes = df.index[-half_window_size:].tolist()\n",
    "        full_data_end_indexes = df.index[sample_end_indexes].tolist()\n",
    "        print(\"full_data_end_indexes:\", full_data_end_indexes)\n",
    "        \n",
    "        in_window_end_indexes = [idx for idx in sample_end_indexes if idx in full_data_end_indexes]\n",
    "        print(\"in_window_end_indexes:\\n\", in_window_end_indexes)\n",
    "        \n",
    "        ##>begin_indexes = begin_indexes[begin_indexes <= half_window_size]\n",
    "        ##>print(\"2 begin_indexes? :\\n\", begin_indexes)\n",
    "        \n",
    "        ##>end_indexes = sample_indexes[-half_window_size:]\n",
    "        ##>print(\"2 end_indexes? :\\n\", end_indexes)\n",
    "        \n",
    "        if in_window_begin_indexes:\n",
    "            begin_indexes_set.union(set(in_window_begin_indexes))\n",
    "            for i, b_idx in enumerate(in_window_begin_indexes):\n",
    "                print(\"i: {}, b_idx {}:\".format(i, b_idx))\n",
    "                print(\"type(b_idx):\", type(b_idx))\n",
    "                print(\"df[:window_size]:\\n\", df[:window_size])\n",
    "                #sample_df[feature_name].iloc[b_idx] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "                value = df.iloc[b_idx]['acoustic_data']\n",
    "                value = value - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "                feature_values_list[i] = value\n",
    "        if in_window_end_indexes:\n",
    "            end_indexes_set.union(set(in_window_end_indexes))\n",
    "            for i, e_idx in enumerate(in_window_end_indexes):\n",
    "                print(\"i: {}, e_idx {}:\".format(i, e_idx))\n",
    "                print(\"type(e_idx):\", type(e_idx))\n",
    "                print(\"df[-window_size:]:\\n\", df[-window_size:])\n",
    "                #sample_df[feature_name].iloc[e_idx] = df[e_idx]['acoustic_data'] - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "                value = df.iloc[e_idx]['acoustic_data']\n",
    "                value = value - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "                feature_values_list[-i] = value\n",
    "        #print(\"in main loop, sample_df.shape:\", sample_df.shape)\n",
    "        slice_begin = len(begin_indexes_set)\n",
    "        slice_end = sample_df_len - len(end_indexes_set)\n",
    "        for i in range(sample_df_len)[slice_begin:slice_end]:\n",
    "            sample_idx = sample_indexes[i]\n",
    "            feature_values_list[i] = acoustic_data_series.iloc[sample_idx - half_window_size:sample_idx + half_window_size].mean()\n",
    "        #print(\"sample_df.shape[0] before assign feature_values_list:\", sample_df.shape[0])\n",
    "        #print(\"len(feature_values_list):\", len(feature_values_list))\n",
    "        sample_df[feature_name] = feature_values_list\n",
    "        \n",
    "    #sample_df_indexes_set = set(sample_df.index)\n",
    "    #train_indexes = sample_df_indexes_set\n",
    "    holdout_df = None\n",
    "    if holdout_size > 0:\n",
    "        holdout_indexes = np.random.randint(0, sample_df.shape[0], holdout_size)\n",
    "        #print(\"sample_df.index:\\n\", sample_df.index.tolist())\n",
    "        #print(\"holdout_indexes:\\n\", holdout_indexes)\n",
    "        #train_indexes = np.array(sample_df.index)\n",
    "        holdout_df = sample_df.iloc[holdout_indexes]\n",
    "        holdout_df.reset_index(inplace=True)\n",
    "        holdout_df.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "        #train_indexes = sorted(tuple(sample_df_indexes_set.difference(set(holdout_indexes))))\n",
    "        train_indexes = sorted(tuple(set(sample_df.index).difference(set(holdout_indexes))))\n",
    "        #print(\"train_indexes:\\n\", train_indexes)\n",
    "        sample_df = sample_df.iloc[train_indexes]\n",
    "        sample_df.reset_index(inplace=True)\n",
    "        sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"Full calculation feature value time (with slicing) {} min:\".format((time.time() - start_time) / 60))\n",
    "    return sample_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_margin_indexes =[\n",
    "    5656573,\n",
    "    50085877,\n",
    "    104677355,\n",
    "    138772452,\n",
    "    187641819,\n",
    "    218652629,\n",
    "    245829584,\n",
    "    307838916,\n",
    "    338276286,\n",
    "    375377847,\n",
    "    419368879,\n",
    "    461811622,\n",
    "    495800224,\n",
    "    528777114,\n",
    "    585568143,\n",
    "    621985672\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length = [earthquake_margin_indexes[i + 1] - earthquake_margin_indexes[i] for i in range(len(earthquake_margin_indexes) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44429304,\n",
       " 54591478,\n",
       " 34095097,\n",
       " 48869367,\n",
       " 31010810,\n",
       " 27176955,\n",
       " 62009332,\n",
       " 30437370,\n",
       " 37101561,\n",
       " 43991032,\n",
       " 42442743,\n",
       " 33988602,\n",
       " 32976890,\n",
       " 56791029,\n",
       " 36417529]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length = earthquakes_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_earthquaces_length = complete_earthquaces_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44429304,\n",
       " 54591478,\n",
       " 34095097,\n",
       " 48869367,\n",
       " 31010810,\n",
       " 27176955,\n",
       " 62009332,\n",
       " 30437370,\n",
       " 37101561,\n",
       " 43991032,\n",
       " 42442743,\n",
       " 33988602,\n",
       " 32976890,\n",
       " 56791029]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "earthquake_1_df = pd.read_csv(\n",
    "    '../input/train/train.csv',\n",
    "    #nrows=100000000,\n",
    "    names=['acoustic_data', 'time_to_failure'],\n",
    "    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "    skiprows=earthquake_margin_indexes[0],\n",
    "    nrows=complete_earthquakes_length[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44429304 entries, 0 to 44429303\n",
      "Data columns (total 2 columns):\n",
      "acoustic_data      float32\n",
      "time_to_failure    float32\n",
      "dtypes: float32(2)\n",
      "memory usage: 339.0 MB\n"
     ]
    }
   ],
   "source": [
    "earthquake_1_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "earthquake_1_with_additional_features_df = features_maker(earthquake_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full calculation feature value time (with slicing) 23.202832746505738 min:\n",
      "CPU times: user 23min 11s, sys: 1.04 s, total: 23min 12s\n",
      "Wall time: 23min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "earthquake_1_with_additional_features_df, holdout_df = add_features(earthquake_1_df, sample_size=2000000, holdout_size=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acoustic_data  time_to_failure  smootch_mean_ws_3  smootch_mean_ws_5  \\\n",
      "0            5.0        11.540800                6.0               6.00   \n",
      "1            3.0        11.540800                2.5               4.50   \n",
      "2            8.0        11.540800                6.0               4.75   \n",
      "3            7.0        11.540800                7.5               6.25   \n",
      "4            4.0        11.540800                4.5               4.75   \n",
      "5            5.0        11.540800                6.0               5.75   \n",
      "6            2.0        11.540800                1.5               2.50   \n",
      "7            2.0        11.540799                2.5               4.25   \n",
      "8            5.0        11.540799                6.5               5.00   \n",
      "9            3.0        11.540799                5.0               4.25   \n",
      "\n",
      "   smootch_mean_ws_7  \n",
      "0           5.833333  \n",
      "1           4.500000  \n",
      "2           4.666667  \n",
      "3           6.000000  \n",
      "4           3.833333  \n",
      "5           6.000000  \n",
      "6           3.000000  \n",
      "7           4.833333  \n",
      "8           4.833333  \n",
      "9           3.333333  \n",
      "\n",
      "        acoustic_data  time_to_failure  smootch_mean_ws_3  smootch_mean_ws_5  \\\n",
      "818646            6.0         0.000696                2.0               1.50   \n",
      "818647            9.0         0.000696                8.5               8.25   \n",
      "818648            2.0         0.000696                2.5               2.75   \n",
      "818649            9.0         0.000696                8.0               6.50   \n",
      "818650            5.0         0.000696                6.5               6.75   \n",
      "818651            1.0         0.000696               -1.5              -1.25   \n",
      "818652            8.0         0.000696                7.5               6.50   \n",
      "818653            6.0         0.000696                6.5               4.75   \n",
      "818654            5.0         0.000696                4.5               6.00   \n",
      "818655            6.0         0.000696                5.0               3.50   \n",
      "\n",
      "        smootch_mean_ws_7  \n",
      "818646           2.166667  \n",
      "818647           7.666667  \n",
      "818648           3.500000  \n",
      "818649           5.833333  \n",
      "818650           5.666667  \n",
      "818651           1.000000  \n",
      "818652           6.166667  \n",
      "818653           4.333333  \n",
      "818654           6.500000  \n",
      "818655           4.833333  \n"
     ]
    }
   ],
   "source": [
    "print(earthquake_1_with_additional_features_df[:10])\n",
    "print()\n",
    "print(earthquake_1_with_additional_features_df[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = earthquake_1_with_additional_features_df[earthquake_1_with_additional_features_df.columns.drop('time_to_failure')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = earthquake_1_with_additional_features_df['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    #'num_leaves': 51,\n",
    "    'num_leaves': 27,\n",
    "    #'min_data_in_leaf': 10,\n",
    "    'min_data_in_leaf': 8,\n",
    "    'objective':'regression',\n",
    "    #'max_depth': -1,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.001,\n",
    "    'boosting': 'gbdt',\n",
    "    #'feature_fraction': 0.91,\n",
    "    #'bagging_freq': 1,\n",
    "    #'bagging_fraction': 0.91,\n",
    "    #'bagging_seed': 42,\n",
    "    'metric': 'mae',\n",
    "    #'lambda_l1': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'nthread': 10,\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(**params, n_estimators = 20000, n_jobs = 10, num_iterations=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uldo/miniconda3/envs/DS-New/lib/python3.6/site-packages/lightgbm/engine.py:102: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[1000]\ttraining's l1: 2.85464\tvalid_1's l1: 2.85243\n",
      "[2000]\ttraining's l1: 2.84816\tvalid_1's l1: 2.84615\n",
      "[3000]\ttraining's l1: 2.84633\tvalid_1's l1: 2.84449\n",
      "[4000]\ttraining's l1: 2.84554\tvalid_1's l1: 2.84388\n",
      "[5000]\ttraining's l1: 2.84511\tvalid_1's l1: 2.84362\n",
      "[6000]\ttraining's l1: 2.84486\tvalid_1's l1: 2.8435\n",
      "[7000]\ttraining's l1: 2.84468\tvalid_1's l1: 2.84344\n",
      "[8000]\ttraining's l1: 2.84451\tvalid_1's l1: 2.84338\n",
      "[9000]\ttraining's l1: 2.84434\tvalid_1's l1: 2.84333\n",
      "[10000]\ttraining's l1: 2.84417\tvalid_1's l1: 2.84329\n",
      "[11000]\ttraining's l1: 2.84401\tvalid_1's l1: 2.84325\n",
      "[12000]\ttraining's l1: 2.84386\tvalid_1's l1: 2.84322\n",
      "[13000]\ttraining's l1: 2.84371\tvalid_1's l1: 2.8432\n",
      "[14000]\ttraining's l1: 2.84357\tvalid_1's l1: 2.84318\n",
      "[15000]\ttraining's l1: 2.84344\tvalid_1's l1: 2.84317\n",
      "[16000]\ttraining's l1: 2.84331\tvalid_1's l1: 2.84315\n",
      "[17000]\ttraining's l1: 2.84318\tvalid_1's l1: 2.84314\n",
      "[18000]\ttraining's l1: 2.84306\tvalid_1's l1: 2.84312\n",
      "[19000]\ttraining's l1: 2.84293\tvalid_1's l1: 2.84312\n",
      "[20000]\ttraining's l1: 2.8428\tvalid_1's l1: 2.84311\n",
      "[21000]\ttraining's l1: 2.84268\tvalid_1's l1: 2.84309\n",
      "[22000]\ttraining's l1: 2.84256\tvalid_1's l1: 2.84308\n",
      "[23000]\ttraining's l1: 2.84244\tvalid_1's l1: 2.84306\n",
      "[24000]\ttraining's l1: 2.84233\tvalid_1's l1: 2.84305\n",
      "[25000]\ttraining's l1: 2.84221\tvalid_1's l1: 2.84304\n",
      "[26000]\ttraining's l1: 2.84209\tvalid_1's l1: 2.84304\n",
      "[27000]\ttraining's l1: 2.84199\tvalid_1's l1: 2.84304\n",
      "[28000]\ttraining's l1: 2.84189\tvalid_1's l1: 2.84304\n",
      "[29000]\ttraining's l1: 2.84179\tvalid_1's l1: 2.84304\n",
      "[30000]\ttraining's l1: 2.8417\tvalid_1's l1: 2.84305\n",
      "[31000]\ttraining's l1: 2.8416\tvalid_1's l1: 2.84305\n",
      "[32000]\ttraining's l1: 2.8415\tvalid_1's l1: 2.84305\n",
      "[33000]\ttraining's l1: 2.84141\tvalid_1's l1: 2.84306\n",
      "[34000]\ttraining's l1: 2.8413\tvalid_1's l1: 2.84306\n",
      "[35000]\ttraining's l1: 2.8412\tvalid_1's l1: 2.84307\n",
      "[36000]\ttraining's l1: 2.84111\tvalid_1's l1: 2.84308\n",
      "[37000]\ttraining's l1: 2.84102\tvalid_1's l1: 2.84309\n",
      "[38000]\ttraining's l1: 2.84093\tvalid_1's l1: 2.84309\n",
      "[39000]\ttraining's l1: 2.84084\tvalid_1's l1: 2.8431\n",
      "[40000]\ttraining's l1: 2.84074\tvalid_1's l1: 2.84311\n",
      "[41000]\ttraining's l1: 2.84065\tvalid_1's l1: 2.84312\n",
      "[42000]\ttraining's l1: 2.84055\tvalid_1's l1: 2.84313\n",
      "[43000]\ttraining's l1: 2.84046\tvalid_1's l1: 2.84313\n",
      "[44000]\ttraining's l1: 2.84037\tvalid_1's l1: 2.84313\n",
      "[45000]\ttraining's l1: 2.84027\tvalid_1's l1: 2.84313\n",
      "Early stopping, best iteration is:\n",
      "[25894]\ttraining's l1: 2.84211\tvalid_1's l1: 2.84304\n",
      "CPU times: user 1h 33min 30s, sys: 11.8 s, total: 1h 33min 42s\n",
      "Wall time: 9min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting='gbdt', boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, learning_rate=0.001, max_depth=5,\n",
       "       metric='mae', min_child_samples=20, min_child_weight=0.001,\n",
       "       min_data_in_leaf=8, min_split_gain=0.0, n_estimators=20000,\n",
       "       n_jobs=10, nthread=10, num_iterations=100000, num_leaves=27,\n",
       "       objective='regression', random_state=42, reg_alpha=0.0,\n",
       "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=0, verbosity=-1)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_metric='mae',\n",
    "    verbose=1000,\n",
    "    early_stopping_rounds=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = holdout_df[holdout_df.columns.drop('time_to_failure')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = holdout_df['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8447993698125047"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.843063059848607, 2.8431104124307693, 2.8429766961521667, 2.842999310177736, 2.842972294755831, 2.843032369729009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(\n",
    "        earthquake_margin_indexes,\n",
    "        complete_earthquakes_length,\n",
    "        params,\n",
    "        sample_size=None,\n",
    "        holdout_size=None,\n",
    "        not_seen_data_begin=1,\n",
    "        not_seen_data_end=5656572 #5656569\n",
    "    ):\n",
    "    not_seen_data_df = pd.read_csv(\n",
    "        '../input/train/train.csv',\n",
    "        #nrows=100000000,\n",
    "        names=['acoustic_data', 'time_to_failure'],\n",
    "        dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "        skiprows=not_seen_data_begin,\n",
    "        nrows=not_seen_data_end\n",
    "    )\n",
    "    \n",
    "    not_seen_data_df, _ = add_features(\n",
    "        not_seen_data_df,\n",
    "        sample_size=not_seen_data_df.shape[0],\n",
    "        holdout_size=0\n",
    "    )\n",
    "    for i in range(len(complete_earthquakes_length)):\n",
    "        earthquake_df = pd.read_csv(\n",
    "                '../input/train/train.csv',\n",
    "                #nrows=100000000,\n",
    "                names=['acoustic_data', 'time_to_failure'],\n",
    "                dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "                skiprows=earthquake_margin_indexes[i],\n",
    "                nrows=complete_earthquakes_length[i]\n",
    "            )\n",
    "        if not sample_size:\n",
    "            sample_size = complete_earthquakes_length[i] // 100\n",
    "        if not holdout_size:\n",
    "            holdout_size = complete_earthquakes_length[i] // 500\n",
    "        earthquake_add_features_df, holdout_add_features_df = add_features(\n",
    "                earthquake_df,\n",
    "                sample_size=sample_size,\n",
    "                holdout_size=holdout_size\n",
    "            )\n",
    "        X_all = earthquake_add_features_df[earthquake_add_features_df.columns.drop('time_to_failure')]\n",
    "        y_all = earthquake_add_features_df['time_to_failure']\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params, n_estimators = 20000, n_jobs = 10, num_iterations=40000)\n",
    "        model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                eval_metric='mae',\n",
    "                verbose=1000,\n",
    "                early_stopping_rounds=4000\n",
    "            )\n",
    "        X_holdout = holdout_df[holdout_df.columns.drop('time_to_failure')]\n",
    "        y_holdout = holdout_df['time_to_failure']\n",
    "        \n",
    "        y_holdout_predict = model.predict(X_holdout)\n",
    "        print(\"earthquake {} mae {}\".format(i, mean_absolute_error(y_holdout, y_holdout_predict)))\n",
    "\n",
    "        not_seen_data_predict = model.predict(not_seen_data_df)\n",
    "        not_seen_data_predict_df = pd.DataFrame({'time_to_failure': not_seen_data_predict})\n",
    "        not_seen_data_predict_df.to_csv('not_seend_data_earthquake_{}_model_predict.csv', index=False)\n",
    "\n",
    "        model.save_model('earthquake_{}_model.txt'.format(i))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5656569",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5656569",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-87c7323909b0>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(earthquake_margin_indexes, complete_earthquakes_length, params, sample_size, holdout_size, not_seen_data_begin, not_seen_data_end)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnot_seen_data_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_seen_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mholdout_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_earthquakes_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-849e6d20a51c>\u001b[0m in \u001b[0;36madd_features\u001b[0;34m(df, first_index, last_index, sample_size, holdout_size, smootch_windows_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_window_end_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#sample_df[feature_name].iloc[e_idx] = df[e_idx]['acoustic_data'] - df.iloc[last_index - window_size:]['acoustic_data'].mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mfeature_values_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#print(\"in main loop, sample_df.shape:\", sample_df.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5656569"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models(earthquake_margin_indexes, complete_earthquakes_length, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 56 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "not_seen_data_df = pd.read_csv(\n",
    "    '../input/train/train.csv',\n",
    "    #nrows=100000000,\n",
    "    names=['acoustic_data', 'time_to_failure'],\n",
    "    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "    skiprows=1,\n",
    "    nrows=5656572\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656572, 2)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_seen_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "window_size: 3\n",
      "\n",
      "sample_df.index.tolist()[:window_size]:\n",
      " [0, 1, 2]\n",
      "df.index.tolist()[:window_size]:\n",
      " [0, 1, 2]\n",
      "sample_df.index.tolist()[-window_size:]:\n",
      " [5656569, 5656570, 5656571]\n",
      "df.index.tolist()[-window_size:]:\n",
      " [5656569, 5656570, 5656571]\n",
      "sample_begin_indexes:\n",
      " [0]\n",
      "full_data_begin_indexes:\n",
      " {0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-63e660aa34f0>\u001b[0m in \u001b[0;36madd_features\u001b[0;34m(df, first_index, last_index, sample_size, holdout_size, smootch_windows_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#in_window_begin_indexes = [idx for idx in sample_begin_indexes if idx in full_data_begin_indexes]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mmin_begin_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_data_begin_indexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         in_window_begin_indexes = full_data_begin_indexes.intersection(\n\u001b[1;32m     70\u001b[0m             \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_begin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_begin_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhalf_window_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "featured_not_seen_data_df, _ = add_features(\n",
    "    not_seen_data_df,\n",
    "    sample_size=not_seen_data_df.shape[0],\n",
    "    holdout_size=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_indexes:\n",
    " [1]\n",
    "sample_begin_indexes:\n",
    " [1]\n",
    "full_data_begin_indexes:\n",
    " [0]\n",
    "in_window_begin_indexes:\n",
    " []\n",
    "sample_end_indexes:\n",
    " [5656570]\n",
    "full_data_end_indexes: [5656571]\n",
    "in_window_end_indexes:\n",
    " []\n",
    "2 begin_indexes? :\n",
    " [1]\n",
    "2 end_indexes? :\n",
    " [5656570]\n",
    "begin_indexes:\n",
    " [1 2]\n",
    "sample_begin_indexes:\n",
    " [1 2]\n",
    "full_data_begin_indexes:\n",
    " [0, 1]\n",
    "in_window_begin_indexes:\n",
    " [1]\n",
    "sample_end_indexes:\n",
    " [5656569 5656570]\n",
    "full_data_end_indexes: [5656570, 5656571]\n",
    "in_window_end_indexes:\n",
    " [5656570]\n",
    "2 begin_indexes? :\n",
    " [1 2]\n",
    "2 end_indexes? :\n",
    " [5656569 5656570]\n",
    "i: 0, b_idx 1:\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\n",
    "   3077             try:\n",
    "-> 3078                 return self._engine.get_loc(key)\n",
    "   3079             except KeyError:\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 1\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<timed exec> in <module>()\n",
    "\n",
    "<ipython-input-159-69232aa59e5d> in add_features(df, first_index, last_index, sample_size, holdout_size, smootch_windows_size)\n",
    "     71                 print(\"i: {}, b_idx {}:\".format(i, b_idx))\n",
    "     72                 #sample_df[feature_name].iloc[b_idx] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "---> 73                 value = df[b_idx]['acoustic_data']\n",
    "     74                 value = value - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "     75                 feature_values_list[i] = value\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key)\n",
    "   2686             return self._getitem_multilevel(key)\n",
    "   2687         else:\n",
    "-> 2688             return self._getitem_column(key)\n",
    "   2689 \n",
    "   2690     def _getitem_column(self, key):\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key)\n",
    "   2693         # get column\n",
    "   2694         if self.columns.is_unique:\n",
    "-> 2695             return self._get_item_cache(key)\n",
    "   2696 \n",
    "   2697         # duplicate columns & possible reduce dimensionality\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item)\n",
    "   2487         res = cache.get(item)\n",
    "   2488         if res is None:\n",
    "-> 2489             values = self._data.get(item)\n",
    "   2490             res = self._box_item_values(item, values)\n",
    "   2491             cache[item] = res\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath)\n",
    "   4113 \n",
    "   4114             if not isna(item):\n",
    "-> 4115                 loc = self.items.get_loc(item)\n",
    "   4116             else:\n",
    "   4117                 indexer = np.arange(len(self.items))[isna(self.items)]\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\n",
    "   3078                 return self._engine.get_loc(key)\n",
    "   3079             except KeyError:\n",
    "-> 3080                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n",
    "   3081 \n",
    "   3082         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "window_size: 3\n",
    "\n",
    "begin_indexes:\n",
    " [0]\n",
    "sample_begin_indexes:\n",
    " [0]\n",
    "full_data_begin_indexes:\n",
    " [0]\n",
    "in_window_begin_indexes:\n",
    " [0]\n",
    "sample_end_indexes:\n",
    " [5656570]\n",
    "full_data_end_indexes: [5656571]\n",
    "in_window_end_indexes:\n",
    " []\n",
    "2 begin_indexes? :\n",
    " [0]\n",
    "2 end_indexes? :\n",
    " [5656570]\n",
    "i: 0, b_idx 0:\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\n",
    "   3077             try:\n",
    "-> 3078                 return self._engine.get_loc(key)\n",
    "   3079             except KeyError:\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 0\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<timed exec> in <module>()\n",
    "\n",
    "<ipython-input-161-35773f4b493b> in add_features(df, first_index, last_index, sample_size, holdout_size, smootch_windows_size)\n",
    "     74                 print(\"i: {}, b_idx {}:\".format(i, b_idx))\n",
    "     75                 #sample_df[feature_name].iloc[b_idx] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "---> 76                 value = df[b_idx]['acoustic_data']\n",
    "     77                 value = value - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "     78                 feature_values_list[i] = value\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key)\n",
    "   2686             return self._getitem_multilevel(key)\n",
    "   2687         else:\n",
    "-> 2688             return self._getitem_column(key)\n",
    "   2689 \n",
    "   2690     def _getitem_column(self, key):\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key)\n",
    "   2693         # get column\n",
    "   2694         if self.columns.is_unique:\n",
    "-> 2695             return self._get_item_cache(key)\n",
    "   2696 \n",
    "   2697         # duplicate columns & possible reduce dimensionality\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item)\n",
    "   2487         res = cache.get(item)\n",
    "   2488         if res is None:\n",
    "-> 2489             values = self._data.get(item)\n",
    "   2490             res = self._box_item_values(item, values)\n",
    "   2491             cache[item] = res\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath)\n",
    "   4113 \n",
    "   4114             if not isna(item):\n",
    "-> 4115                 loc = self.items.get_loc(item)\n",
    "   4116             else:\n",
    "   4117                 indexer = np.arange(len(self.items))[isna(self.items)]\n",
    "\n",
    "~/miniconda3/envs/DS-New/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\n",
    "   3078                 return self._engine.get_loc(key)\n",
    "   3079             except KeyError:\n",
    "-> 3080                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n",
    "   3081 \n",
    "   3082         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
