{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(\n",
    "        df,\n",
    "        first_index=None,\n",
    "        last_index=None,\n",
    "        sample_size=150000,\n",
    "        holdout_size=50000,\n",
    "        smootch_windows_size = (3, 5, 7)\n",
    "    ):\n",
    "    \n",
    "    if first_index == None or last_index == None:\n",
    "        first_index = df.index.min()\n",
    "        last_index = df.index.max()\n",
    "    sample_indexes = np.random.randint(first_index, last_index, sample_size)\n",
    "    sample_indexes.sort()\n",
    "    print(\"sample_indexes.shape:\", sample_indexes.shape)    \n",
    "    smootch_feature_names = ['smootch_mean_ws_{}'.format(window_size) for window_size in smootch_windows_size]\n",
    "    #half_windows_size = [ws // 2 for ws in smootch_windows_size]\n",
    "\n",
    "    acoustic_data_series = df['acoustic_data']\n",
    "    \n",
    "    sample_df = df.iloc[sample_indexes]\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"just after create sample_df, sample_df.shape[0]:\", sample_df.shape[0])\n",
    "    \n",
    "    #for feature_name in smootch_feature_names:\n",
    "    #    sample_df[feature_name] = 0\n",
    "    begin_smootch_features_value = []\n",
    "    end_smootch_features_value = []\n",
    "    \n",
    "    #sample_indexes_set = set(sample_indexes)\n",
    "    begin_indexes_set = set()\n",
    "    end_indexes_set = set()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"sample_df.shape[0] just before main loop:\", sample_df.shape[0])\n",
    "    sample_df_len = sample_df.shape[0]\n",
    "    for window_size, feature_name in zip(smootch_windows_size, smootch_feature_names):\n",
    "        \n",
    "        feature_values_list = list(range(sample_size))\n",
    "        \n",
    "        half_window_size = window_size // 2\n",
    "        begin_indexes = sample_indexes[:half_window_size]        \n",
    "        sample_begin_indexes = sample_indexes[:half_window_size]\n",
    "        full_data_begin_indexes = df.index[:half_window_size].tolist()\n",
    "\n",
    "        in_window_begin_indexes = [idx for idx in sample_begin_indexes if idx in full_data_begin_indexes]\n",
    "        \n",
    "        sample_end_indexes = sample_indexes[-half_window_size:]\n",
    "\n",
    "        full_data_end_indexes = df.index[-half_window_size:].tolist()\n",
    "        \n",
    "        in_window_end_indexes = [idx for idx in sample_end_indexes if idx in full_data_end_indexes]\n",
    "        begin_indexes = begin_indexes[begin_indexes <= half_window_size]\n",
    "        \n",
    "        end_indexes = sample_indexes[-half_window_size:]\n",
    "        \n",
    "        if in_window_begin_indexes:\n",
    "            begin_indexes_set.union(set(in_window_begin_indexes))\n",
    "            for i, b_idx in enumerate(in_window_begin_indexes):\n",
    "                #sample_df[feature_name].iloc[b_idx] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "                feature_values_list[i] = df[b_idx]['acoustic_data'] - df.iloc[first_index:first_index + window_size]['acoustic_data'].mean()\n",
    "        if in_window_end_indexes:\n",
    "            end_indexes_set.union(set(in_window_end_indexes))\n",
    "            for i, e_idx in enumerat(in_window_end_indexes):\n",
    "                #sample_df[feature_name].iloc[e_idx] = df[e_idx]['acoustic_data'] - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "                feature_values_list[-i] = df[e_idx]['acoustic_data'] - df.iloc[last_index - window_size:]['acoustic_data'].mean()\n",
    "        print(\"in main loop, sample_df.shape:\", sample_df.shape)\n",
    "        slice_begin = len(begin_indexes_set)\n",
    "        slice_end = sample_df_len - len(end_indexes_set)\n",
    "        for i in range(sample_df_len)[slice_begin:slice_end]:\n",
    "            sample_idx = sample_indexes[i]\n",
    "            feature_values_list[i] = acoustic_data_series.iloc[sample_idx - half_window_size:sample_idx + half_window_size].mean()\n",
    "        print(\"sample_df.shape[0] before assign feature_values_list:\", sample_df.shape[0])\n",
    "        print(\"len(feature_values_list):\", len(feature_values_list))\n",
    "        sample_df[feature_name] = feature_values_list\n",
    "    holdout_indexes = np.random.randint(0, sample_df.shape[0], holdout_size)\n",
    "    print(\"sample_df.index:\\n\", sample_df.index.tolist())\n",
    "    print(\"holdout_indexes:\\n\", holdout_indexes)\n",
    "    #train_indexes = np.array(sample_df.index)\n",
    "    holdout_df = sample_df.iloc[holdout_indexes]\n",
    "    holdout_df.reset_index(inplace=True)\n",
    "    holdout_df.drop(columns=['index'], inplace=True)\n",
    "    sample_df_indexes_set = set(sample_df.index)\n",
    "    train_indexes = sorted(tuple(sample_df_indexes_set.difference(set(holdout_indexes))))\n",
    "    print(\"train_indexes:\\n\", train_indexes)\n",
    "    sample_df = sample_df.iloc[train_indexes]\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"Full calculation feature value time (with slicing) {} min:\".format((time.time() - start_time) / 60))\n",
    "    return sample_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_margin_indexes =[\n",
    "    5656573,\n",
    "    50085877,\n",
    "    104677355,\n",
    "    138772452,\n",
    "    187641819,\n",
    "    218652629,\n",
    "    245829584,\n",
    "    307838916,\n",
    "    338276286,\n",
    "    375377847,\n",
    "    419368879,\n",
    "    461811622,\n",
    "    495800224,\n",
    "    528777114,\n",
    "    585568143,\n",
    "    621985672\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length = [earthquake_margin_indexes[i + 1] - earthquake_margin_indexes[i] for i in range(len(earthquake_margin_indexes) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44429304,\n",
       " 54591478,\n",
       " 34095097,\n",
       " 48869367,\n",
       " 31010810,\n",
       " 27176955,\n",
       " 62009332,\n",
       " 30437370,\n",
       " 37101561,\n",
       " 43991032,\n",
       " 42442743,\n",
       " 33988602,\n",
       " 32976890,\n",
       " 56791029,\n",
       " 36417529]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length = earthquakes_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_earthquaces_length = complete_earthquaces_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44429304,\n",
       " 54591478,\n",
       " 34095097,\n",
       " 48869367,\n",
       " 31010810,\n",
       " 27176955,\n",
       " 62009332,\n",
       " 30437370,\n",
       " 37101561,\n",
       " 43991032,\n",
       " 42442743,\n",
       " 33988602,\n",
       " 32976890,\n",
       " 56791029]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "earthquake_1_df = pd.read_csv(\n",
    "    '../input/train/train.csv',\n",
    "    #nrows=100000000,\n",
    "    names=['acoustic_data', 'time_to_failure'],\n",
    "    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "    skiprows=earthquake_margin_indexes[0],\n",
    "    nrows=complete_earthquakes_length[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44429304 entries, 0 to 44429303\n",
      "Data columns (total 2 columns):\n",
      "acoustic_data      float32\n",
      "time_to_failure    float32\n",
      "dtypes: float32(2)\n",
      "memory usage: 339.0 MB\n"
     ]
    }
   ],
   "source": [
    "earthquake_1_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "earthquake_1_with_additional_features_df = features_maker(earthquake_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_indexes.shape: (1500000,)\n",
      "just after create sample_df, sample_df.shape[0]: 1500000\n",
      "sample_df.shape[0] just before main loop: 1500000\n",
      "in main loop, sample_df.shape: (1500000, 2)\n",
      "sample_df.shape[0] before assign feature_values_list: 1500000\n",
      "len(feature_values_list): 1500000\n",
      "in main loop, sample_df.shape: (1500000, 3)\n",
      "sample_df.shape[0] before assign feature_values_list: 1500000\n",
      "len(feature_values_list): 1500000\n",
      "in main loop, sample_df.shape: (1500000, 4)\n",
      "sample_df.shape[0] before assign feature_values_list: 1500000\n",
      "len(feature_values_list): 1500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full calculation feature value time (with slicing) 17.315324095884957 min:\n",
      "CPU times: user 17min 16s, sys: 2.45 s, total: 17min 19s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "earthquake_1_with_additional_features_df, holdout_df = add_features(earthquake_1_df, sample_size=1500000, holdout_size=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acoustic_data  time_to_failure  smootch_mean_ws_3  smootch_mean_ws_5  \\\n",
      "0            6.0          11.5408                5.5               4.50   \n",
      "1            3.0          11.5408                4.5               4.50   \n",
      "2            6.0          11.5408                6.0               4.00   \n",
      "3            6.0          11.5408                7.0               6.00   \n",
      "4            6.0          11.5408                6.0               4.75   \n",
      "5            4.0          11.5408                3.5               5.25   \n",
      "6            9.0          11.5408                9.0               7.00   \n",
      "7            7.0          11.5408                8.0               6.00   \n",
      "8            9.0          11.5408                6.0               6.25   \n",
      "9            2.0          11.5408                3.5               2.50   \n",
      "\n",
      "   smootch_mean_ws_7  \n",
      "0           4.333333  \n",
      "1           4.166667  \n",
      "2           3.666667  \n",
      "3           4.500000  \n",
      "4           4.833333  \n",
      "5           5.666667  \n",
      "6           6.500000  \n",
      "7           6.333333  \n",
      "8           6.500000  \n",
      "9           2.833333  \n",
      "\n",
      "         acoustic_data  time_to_failure  smootch_mean_ws_3  smootch_mean_ws_5  \\\n",
      "1228233           10.0         0.000696                9.0               8.00   \n",
      "1228234            4.0         0.000696                2.5               4.00   \n",
      "1228235            9.0         0.000696                6.0               5.00   \n",
      "1228236            6.0         0.000696                7.5               7.50   \n",
      "1228237            3.0         0.000696                4.0               4.25   \n",
      "1228238            4.0         0.000696                2.0               3.75   \n",
      "1228239            3.0         0.000696                4.5               3.50   \n",
      "1228240            8.0         0.000696                7.5               6.00   \n",
      "1228241            5.0         0.000696                6.5               7.00   \n",
      "1228242            7.0         0.000695                6.0               5.50   \n",
      "\n",
      "         smootch_mean_ws_7  \n",
      "1228233           6.666667  \n",
      "1228234           3.833333  \n",
      "1228235           4.666667  \n",
      "1228236           8.000000  \n",
      "1228237           4.666667  \n",
      "1228238           3.500000  \n",
      "1228239           3.666667  \n",
      "1228240           6.500000  \n",
      "1228241           6.000000  \n",
      "1228242           4.500000  \n"
     ]
    }
   ],
   "source": [
    "print(earthquake_1_with_additional_features_df[:10])\n",
    "print()\n",
    "print(earthquake_1_with_additional_features_df[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = earthquake_1_with_additional_features_df[earthquake_1_with_additional_features_df.columns.drop('time_to_failure')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = earthquake_1_with_additional_features_df['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 51,\n",
    "         'min_data_in_leaf': 10, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.001,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.91,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.91,\n",
    "         \"bagging_seed\": 42,\n",
    "         \"metric\": 'mae',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": -1,\n",
    "         \"random_state\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(**params, n_estimators = 20000, n_jobs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_metric='mae',\n",
    "    verbose=1000,\n",
    "    early_stopping_rounds=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
