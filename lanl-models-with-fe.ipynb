{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "import dask.multiprocessing\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler='processes')\n",
    "dask.set_options( pool=ThreadPool(10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_dask_parralel(\n",
    "#def add_features_parallel(\n",
    "        df,\n",
    "        input_first_index=None,\n",
    "        input_last_index=None,\n",
    "        sample_size=150000,\n",
    "        holdout_size=50000,\n",
    "        smootch_windows_size = (3, 5, 7)\n",
    "    ):\n",
    "    if input_first_index == None or input_last_index == None:\n",
    "        input_first_index = df.index.min()\n",
    "        input_last_index = df.index.max() + 1\n",
    "        \n",
    "    \n",
    "    sample_indexes = random.sample(range(input_first_index, input_last_index), sample_size)\n",
    "    sample_indexes.sort()\n",
    "    \n",
    "    smootch_feature_names = ['smootch_mean_ws_{}'.format(window_size) for window_size in smootch_windows_size]\n",
    "    acoustic_data_series = df['acoustic_data']\n",
    "    full_data_indexes = tuple(acoustic_data_series.index.tolist())\n",
    "\n",
    "    sample_df = df.iloc[sample_indexes]\n",
    "\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    output_first_index = 0\n",
    "    output_last_index = len(sample_df) - 1\n",
    "    \n",
    "    begin_indexes_set = set()\n",
    "    end_indexes_set = set()\n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    sample_df_len = sample_df.shape[0]\n",
    "    @dask.delayed\n",
    "    def create_features():\n",
    "        for window_size, feature_name in zip(smootch_windows_size, smootch_feature_names):\n",
    "\n",
    "            feature_values_list = list(range(sample_size))\n",
    "\n",
    "            half_window_size = window_size // 2\n",
    "\n",
    "            sample_begin_indexes = sample_indexes[:half_window_size]\n",
    "            full_data_begin_indexes = set(df.index[sample_begin_indexes].tolist())\n",
    "            min_full_data_index = min(full_data_indexes)\n",
    "        \n",
    "            in_window_full_data_begin_indexes = set(range(input_first_index, input_first_index + half_window_size))              \n",
    "            in_window_begin_indexes = full_data_begin_indexes.intersection(\n",
    "                in_window_full_data_begin_indexes\n",
    "            )\n",
    "        \n",
    "            sample_end_indexes = sample_indexes[-half_window_size:]\n",
    "            full_data_end_indexes = set(df.index[sample_end_indexes].tolist())\n",
    "            max_full_data_index = max(full_data_end_indexes) + 1\n",
    "        \n",
    "            in_window_full_data_end_indexes = set(range(input_last_index - half_window_size, input_last_index))        \n",
    "            in_window_end_indexes = full_data_end_indexes.intersection(\n",
    "                in_window_full_data_end_indexes\n",
    "            )\n",
    "            if in_window_begin_indexes:\n",
    "                begin_indexes_set = begin_indexes_set.union(in_window_begin_indexes)\n",
    "                for i, b_idx in enumerate(sorted(tuple(in_window_begin_indexes))):\n",
    "                    value = sample_df.iloc[i]['acoustic_data']\n",
    "                    temp = acoustic_data_series.iloc[input_first_index:input_first_index + window_size].mean()\n",
    "                    value = value - temp\n",
    "                    feature_values_list[output_first_index + i] = value\n",
    "                \n",
    "            if in_window_end_indexes:\n",
    "                end_indexes_set = end_indexes_set.union(in_window_end_indexes)\n",
    "                for i, e_idx in enumerate(sorted(tuple(in_window_end_indexes))):\n",
    "                    value = sample_df.iloc[output_last_index - i]['acoustic_data']\n",
    "                    temp = acoustic_data_series.iloc[input_last_index - window_size:].mean()\n",
    "                    value = value - temp\n",
    "                    feature_values_list[output_last_index - i] = value\n",
    "                \n",
    "            first_regular_idx = len(begin_indexes_set)\n",
    "            last_regular_idx = sample_df_len - len(end_indexes_set)\n",
    "            for i in range(first_regular_idx, last_regular_idx):\n",
    "                sample_idx = sample_indexes[i]\n",
    "                feature_values_list[i] = acoustic_data_series.iloc[\n",
    "                    sample_idx - half_window_size:sample_idx + half_window_size\n",
    "                ].mean()\n",
    "            sample_df[feature_name] = feature_values_list\n",
    "        return sample_df\n",
    "        \n",
    "    holdout_df = None\n",
    "    if holdout_size > 0:\n",
    "        holdout_indexes = np.random.randint(0, sample_df.shape[0], holdout_size)\n",
    "        holdout_df = sample_df.iloc[holdout_indexes]\n",
    "        holdout_df.reset_index(inplace=True)\n",
    "        holdout_df.drop(columns=['index'], inplace=True)\n",
    "        train_indexes = sorted(tuple(set(sample_df.index).difference(set(holdout_indexes))))\n",
    "        sample_df = sample_df.iloc[train_indexes]\n",
    "        sample_df.reset_index(inplace=True)\n",
    "        sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"Full calculation feature value time (with slicing) {} min:\".format((time.time() - start_time) / 60))\n",
    "    return sample_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = add_features_dask_parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(z):\n",
    "    print()\n",
    "    print(\"extract_features, z.shape:\", z.shape)\n",
    "    #print()\n",
    "    return np.c_[z.mean(axis=1),\n",
    "                 np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n",
    "                 z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"createX, x.shape:\", x.shape)\n",
    "    print(\"createX, last_index:\", last_index)\n",
    "    print(\"createX, n_steps:\", n_steps)\n",
    "    print(\"createX, step_length:\", step_length)\n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n",
    "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    # MY CHANGE: This doesn't fix things, I get the same errors\n",
    "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1).astype(np.float32) - 5 ) / 3\n",
    "    temp = x[(last_index - n_steps * step_length):last_index]\n",
    "    print(\"createX, temp.shape before reshape:\", temp.shape)\n",
    "    temp = temp.reshape(n_steps, -1).astype(np.float32)\n",
    "    print(\"create_X, temp.shape after reshape:\", temp.shape)\n",
    "    temp = (temp - 5) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations.\n",
    "    print(\"createX, extract_features(temp).shape:\", extract_features(temp).shape)\n",
    "    print(\"createX, extract_features(temp[:, -step_length // 10:]).shape\", extract_features(temp[:, -step_length // 10:]).shape)\n",
    "    print(\"createX, extract_features(temp[:, -step_length // 100:]).shape\", extract_features(temp[:, -step_length // 100:]).shape)\n",
    "    print()\n",
    "    #result = np.c_[extract_features(temp),\n",
    "    #             extract_features(temp[:, -step_length // 10:]),\n",
    "    #             extract_features(temp[:, -step_length // 100:])]\n",
    "    result = np.c_[\n",
    "        temp,\n",
    "        temp[:, -step_length // 10:],\n",
    "        temp[:, -step_length // 100:]\n",
    "    ]\n",
    "    print(\"createX, result shape:\", result.shape)\n",
    "    print()\n",
    "    '''\n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]\n",
    "    '''\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "#def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "def generator(data, y, rows, batch_size=16, n_steps=150, step_length=1000):\n",
    "    #if max_index is None:\n",
    "    #    max_index = len(data) - 1\n",
    "    print(\"\\n\")   \n",
    "    print(\"generator, data.shape:\", data.shape)\n",
    "    \n",
    "    '''\n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        #rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "        print(\"generator, type(rows):\", type(rows))\n",
    "        print(\"generator, rows.shape:\", rows.shape)\n",
    "        print(\"generator rows:\\n\", rows)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        print(\"generator, samples.shape:\", samples.shape)\n",
    "        print(\"generator, targets.shape:\", targets.shape)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            print(\"generator j: {}, row: {}\".format(j, row))\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            print(\"generator, samples[{}].shape:\".format(j), samples[j].shape)\n",
    "            print(\"generator, row - 1:\", row - 1)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "            print(\"generator, targets[{}].shape:\".format(j), targets[j].shape)\n",
    "            print()\n",
    "        yield samples, targets\n",
    "    '''\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data, last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            #targets[j] = data[row - 1, 1]\n",
    "            targets[j] = y[row - 1]\n",
    "            #temp = x[(last_index - n_steps * step_length):last_index]\n",
    "            #print(\"createX, temp.shape before reshape:\", temp.shape)\n",
    "            #temp = temp.reshape(n_steps, -1).astype(np.float32)\n",
    "            #print(\"create_X, temp.shape after reshape:\", temp.shape)\n",
    "            #temp = (temp - 5) / 3\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_margin_indexes =[\n",
    "    5656573,\n",
    "    50085877,\n",
    "    104677355,\n",
    "    138772452,\n",
    "    187641819,\n",
    "    218652629,\n",
    "    245829584,\n",
    "    307838916,\n",
    "    338276286,\n",
    "    375377847,\n",
    "    419368879,\n",
    "    461811622,\n",
    "    495800224,\n",
    "    528777114,\n",
    "    585568143,\n",
    "    621985672\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length = [earthquake_margin_indexes[i + 1] - earthquake_margin_indexes[i] for i in range(len(earthquake_margin_indexes) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length = earthquakes_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_earthquaces_length = complete_earthquaces_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#not_seen_data_df = pd.read_csv(\n",
    "#    '../input/train/train.csv',\n",
    "#    #nrows=100000000,\n",
    "#    names=['acoustic_data', 'time_to_failure'],\n",
    "#    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "#    skiprows=1,\n",
    "#    nrows=5656572\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#featured_not_seen_data_df, _ = add_features(\n",
    "#    not_seen_data_df,\n",
    "#    sample_size=not_seen_data_df.shape[0] // 20,\n",
    "#    holdout_size=0\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_not_seen_data = featured_not_seen_data_df['time_to_failure']\n",
    "#featured_not_seen_data_no_time_df = featured_not_seen_data_df[featured_not_seen_data_df.columns.drop('time_to_failure')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float_data = pd.read_csv(\"../input/train/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#        earthquake_df = pd.read_csv(\n",
    "#                '../input/train/train.csv',\n",
    "#                #nrows=100000000,\n",
    "#                names=['acoustic_data', 'time_to_failure'],\n",
    "#                dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "#                skiprows=earthquake_margin_indexes[i],\n",
    "#                nrows=complete_earthquakes_length[i]\n",
    "#            )\n",
    "\n",
    "train_df = pd.read_csv('../input/train/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        if not sample_size:\n",
    "#            sample_size = complete_earthquakes_length[i] // 10\n",
    "#        if not holdout_size:\n",
    "#            holdout_size = complete_earthquakes_length[i] // 50\n",
    "sample_size = train_df.shape[0] // 10\n",
    "holdout_size = train_df.shape[0] // 50\n",
    "%time\n",
    "featured_train_df, featured_holdout_df = add_features(\n",
    "                train_df,\n",
    "                sample_size=sample_size,\n",
    "                holdout_size=holdout_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = featured_train_df[featured_train_df.columns.drop('time_to_failure')]\n",
    "y_all = featured_train_df['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "#n_features = create_X(float_data[0:150000]).shape[1]\n",
    "n_features = create_X(train_df[0:150000]).shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "#float_data[second_earthquake, 1]\n",
    "train_df[second_earthquake, 1]\n",
    "\n",
    "# Initialize generators\n",
    "# train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "# train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "# valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)\n",
    "train_gen = generator(train_df, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(train_df, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
