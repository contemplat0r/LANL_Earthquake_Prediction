{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "import dask.multiprocessing\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uldo/miniconda3/envs/DS-New/lib/python3.6/site-packages/dask/context.py:23: UserWarning: The dask.set_options function has been deprecated. Please use dask.config.set instead\n",
      "  warnings.warn(\"The dask.set_options function has been deprecated. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7fb999120a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.set(scheduler='processes')\n",
    "dask.set_options( pool=ThreadPool(10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_dask_parralel(\n",
    "#def add_features_parallel(\n",
    "        df,\n",
    "        input_first_index=None,\n",
    "        input_last_index=None,\n",
    "        sample_size=150000,\n",
    "        holdout_size=50000,\n",
    "        smootch_windows_size = (3, 5, 7)\n",
    "    ):\n",
    "    if input_first_index == None or input_last_index == None:\n",
    "        input_first_index = df.index.min()\n",
    "        input_last_index = df.index.max() + 1\n",
    "        \n",
    "    \n",
    "    sample_indexes = random.sample(range(input_first_index, input_last_index), sample_size)\n",
    "    sample_indexes.sort()\n",
    "    \n",
    "    smootch_feature_names = ['smootch_mean_ws_{}'.format(window_size) for window_size in smootch_windows_size]\n",
    "    acoustic_data_series = df['acoustic_data']\n",
    "    full_data_indexes = tuple(acoustic_data_series.index.tolist())\n",
    "\n",
    "    sample_df = df.iloc[sample_indexes]\n",
    "\n",
    "    sample_df.reset_index(inplace=True)\n",
    "    sample_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    output_first_index = 0\n",
    "    output_last_index = len(sample_df) - 1\n",
    "    \n",
    "    begin_indexes_set = set()\n",
    "    end_indexes_set = set()\n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    sample_df_len = sample_df.shape[0]\n",
    "    @dask.delayed\n",
    "    def create_features():\n",
    "        for window_size, feature_name in zip(smootch_windows_size, smootch_feature_names):\n",
    "\n",
    "            feature_values_list = list(range(sample_size))\n",
    "\n",
    "            half_window_size = window_size // 2\n",
    "\n",
    "            sample_begin_indexes = sample_indexes[:half_window_size]\n",
    "            full_data_begin_indexes = set(df.index[sample_begin_indexes].tolist())\n",
    "            min_full_data_index = min(full_data_indexes)\n",
    "        \n",
    "            in_window_full_data_begin_indexes = set(range(input_first_index, input_first_index + half_window_size))              \n",
    "            in_window_begin_indexes = full_data_begin_indexes.intersection(\n",
    "                in_window_full_data_begin_indexes\n",
    "            )\n",
    "        \n",
    "            sample_end_indexes = sample_indexes[-half_window_size:]\n",
    "            full_data_end_indexes = set(df.index[sample_end_indexes].tolist())\n",
    "            max_full_data_index = max(full_data_end_indexes) + 1\n",
    "        \n",
    "            in_window_full_data_end_indexes = set(range(input_last_index - half_window_size, input_last_index))        \n",
    "            in_window_end_indexes = full_data_end_indexes.intersection(\n",
    "                in_window_full_data_end_indexes\n",
    "            )\n",
    "            if in_window_begin_indexes:\n",
    "                begin_indexes_set = begin_indexes_set.union(in_window_begin_indexes)\n",
    "                for i, b_idx in enumerate(sorted(tuple(in_window_begin_indexes))):\n",
    "                    value = sample_df.iloc[i]['acoustic_data']\n",
    "                    temp = acoustic_data_series.iloc[input_first_index:input_first_index + window_size].mean()\n",
    "                    value = value - temp\n",
    "                    feature_values_list[output_first_index + i] = value\n",
    "                \n",
    "            if in_window_end_indexes:\n",
    "                end_indexes_set = end_indexes_set.union(in_window_end_indexes)\n",
    "                for i, e_idx in enumerate(sorted(tuple(in_window_end_indexes))):\n",
    "                    value = sample_df.iloc[output_last_index - i]['acoustic_data']\n",
    "                    temp = acoustic_data_series.iloc[input_last_index - window_size:].mean()\n",
    "                    value = value - temp\n",
    "                    feature_values_list[output_last_index - i] = value\n",
    "                \n",
    "            first_regular_idx = len(begin_indexes_set)\n",
    "            last_regular_idx = sample_df_len - len(end_indexes_set)\n",
    "            for i in range(first_regular_idx, last_regular_idx):\n",
    "                sample_idx = sample_indexes[i]\n",
    "                feature_values_list[i] = acoustic_data_series.iloc[\n",
    "                    sample_idx - half_window_size:sample_idx + half_window_size\n",
    "                ].mean()\n",
    "            sample_df[feature_name] = feature_values_list\n",
    "        return sample_df\n",
    "        \n",
    "    holdout_df = None\n",
    "    if holdout_size > 0:\n",
    "        holdout_indexes = np.random.randint(0, sample_df.shape[0], holdout_size)\n",
    "        holdout_df = sample_df.iloc[holdout_indexes]\n",
    "        holdout_df.reset_index(inplace=True)\n",
    "        holdout_df.drop(columns=['index'], inplace=True)\n",
    "        train_indexes = sorted(tuple(set(sample_df.index).difference(set(holdout_indexes))))\n",
    "        sample_df = sample_df.iloc[train_indexes]\n",
    "        sample_df.reset_index(inplace=True)\n",
    "        sample_df.drop(columns=['index'], inplace=True)\n",
    "    print(\"Full calculation feature value time (with slicing) {} min:\".format((time.time() - start_time) / 60))\n",
    "    return sample_df, holdout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = add_features_dask_parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(z):\n",
    "    print()\n",
    "    print(\"extract_features, z.shape:\", z.shape)\n",
    "    #print()\n",
    "    return np.c_[z.mean(axis=1),\n",
    "                 np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=1)),\n",
    "                 z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"createX, x.shape:\", x.shape)\n",
    "    print(\"createX, last_index:\", last_index)\n",
    "    print(\"createX, n_steps:\", n_steps)\n",
    "    print(\"createX, step_length:\", step_length)\n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n",
    "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    # MY CHANGE: This doesn't fix things, I get the same errors\n",
    "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1).astype(np.float32) - 5 ) / 3\n",
    "    temp = x[(last_index - n_steps * step_length):last_index]\n",
    "    print(\"createX, temp.shape before reshape:\", temp.shape)\n",
    "    temp = temp.reshape(n_steps, -1).astype(np.float32)\n",
    "    print(\"create_X, temp.shape after reshape:\", temp.shape)\n",
    "    temp = (temp - 5) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations.\n",
    "    print(\"createX, extract_features(temp).shape:\", extract_features(temp).shape)\n",
    "    print(\"createX, extract_features(temp[:, -step_length // 10:]).shape\", extract_features(temp[:, -step_length // 10:]).shape)\n",
    "    print(\"createX, extract_features(temp[:, -step_length // 100:]).shape\", extract_features(temp[:, -step_length // 100:]).shape)\n",
    "    print()\n",
    "    result = np.c_[\n",
    "        extract_features(temp),\n",
    "        extract_features(temp[:, -step_length // 10:]),\n",
    "        extract_features(temp[:, -step_length // 100:])\n",
    "    ]\n",
    "    '''               \n",
    "    result = np.c_[\n",
    "        temp,\n",
    "        temp[:, -step_length // 10:],\n",
    "        temp[:, -step_length // 100:]\n",
    "    ]\n",
    "    '''\n",
    "    print(\"createX, result shape:\", result.shape)\n",
    "    print()\n",
    "    '''\n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]\n",
    "    '''\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "#def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "def generator(data, y, rows, batch_size=16, n_steps=150, step_length=1000):\n",
    "    #if max_index is None:\n",
    "    #    max_index = len(data) - 1\n",
    "    print(\"\\n\")   \n",
    "    print(\"generator, data.shape:\", data.shape)\n",
    "    \n",
    "    '''\n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        #rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "        print(\"generator, type(rows):\", type(rows))\n",
    "        print(\"generator, rows.shape:\", rows.shape)\n",
    "        print(\"generator rows:\\n\", rows)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        print(\"generator, samples.shape:\", samples.shape)\n",
    "        print(\"generator, targets.shape:\", targets.shape)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            print(\"generator j: {}, row: {}\".format(j, row))\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            print(\"generator, samples[{}].shape:\".format(j), samples[j].shape)\n",
    "            print(\"generator, row - 1:\", row - 1)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "            print(\"generator, targets[{}].shape:\".format(j), targets[j].shape)\n",
    "            print()\n",
    "        yield samples, targets\n",
    "    '''\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data, last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            #targets[j] = data[row - 1, 1]\n",
    "            targets[j] = y[row - 1]\n",
    "            #temp = x[(last_index - n_steps * step_length):last_index]\n",
    "            #print(\"createX, temp.shape before reshape:\", temp.shape)\n",
    "            #temp = temp.reshape(n_steps, -1).astype(np.float32)\n",
    "            #print(\"create_X, temp.shape after reshape:\", temp.shape)\n",
    "            #temp = (temp - 5) / 3\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_margin_indexes =[\n",
    "    5656573,\n",
    "    50085877,\n",
    "    104677355,\n",
    "    138772452,\n",
    "    187641819,\n",
    "    218652629,\n",
    "    245829584,\n",
    "    307838916,\n",
    "    338276286,\n",
    "    375377847,\n",
    "    419368879,\n",
    "    461811622,\n",
    "    495800224,\n",
    "    528777114,\n",
    "    585568143,\n",
    "    621985672\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length = [earthquake_margin_indexes[i + 1] - earthquake_margin_indexes[i] for i in range(len(earthquake_margin_indexes) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length = earthquakes_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_earthquaces_length = complete_earthquaces_length[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_earthquakes_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#not_seen_data_df = pd.read_csv(\n",
    "#    '../input/train/train.csv',\n",
    "#    #nrows=100000000,\n",
    "#    names=['acoustic_data', 'time_to_failure'],\n",
    "#    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "#    skiprows=1,\n",
    "#    nrows=5656572\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#featured_not_seen_data_df, _ = add_features(\n",
    "#    not_seen_data_df,\n",
    "#    sample_size=not_seen_data_df.shape[0] // 20,\n",
    "#    holdout_size=0\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_not_seen_data = featured_not_seen_data_df['time_to_failure']\n",
    "#featured_not_seen_data_no_time_df = featured_not_seen_data_df[featured_not_seen_data_df.columns.drop('time_to_failure')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float_data = pd.read_csv(\"../input/train/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 11.6 s, total: 2min 38s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#        earthquake_df = pd.read_csv(\n",
    "#                '../input/train/train.csv',\n",
    "#                #nrows=100000000,\n",
    "#                names=['acoustic_data', 'time_to_failure'],\n",
    "#                dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32},\n",
    "#                skiprows=earthquake_margin_indexes[i],\n",
    "#                nrows=complete_earthquakes_length[i]\n",
    "#            )\n",
    "\n",
    "train_df = pd.read_csv('../input/train/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full calculation feature value time (with slicing) 0.08294108708699545 min:\n",
      "CPU times: user 1min, sys: 20.4 s, total: 1min 21s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#        if not sample_size:\n",
    "#            sample_size = complete_earthquakes_length[i] // 10\n",
    "#        if not holdout_size:\n",
    "#            holdout_size = complete_earthquakes_length[i] // 50\n",
    "#sample_size = train_df.shape[0] // 10\n",
    "#holdout_size = train_df.shape[0] // 50\n",
    "sample_size = 7500000\n",
    "holdout_size = 1500000\n",
    "featured_train_df, featured_holdout_df = add_features(\n",
    "                train_df,\n",
    "                sample_size=sample_size,\n",
    "                holdout_size=holdout_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = featured_train_df[featured_train_df.columns.drop('time_to_failure')]\n",
    "y_all = featured_train_df['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 150000, 300000, 450000, 600000, 750000, 900000, 1050000, 1200000, 1350000, 1500000, 1650000, 1800000, 1950000, 2100000, 2250000, 2400000, 2550000, 2700000, 2850000, 3000000, 3150000, 3300000, 3450000, 3600000, 3750000, 3900000, 4050000, 4200000, 4350000, 4500000, 4650000)\n",
      "(0, 150000, 300000, 450000, 600000, 750000, 900000, 1050000)\n"
     ]
    }
   ],
   "source": [
    "train_rows = tuple(range(0, 4800000, 150000))\n",
    "valid_rows = tuple(range(0, 1200000, 150000))\n",
    "print(train_rows)\n",
    "print(valid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "createX, x.shape: (150000, 1)\n",
      "createX, last_index: 150000\n",
      "createX, n_steps: 150\n",
      "createX, step_length: 1000\n",
      "createX, temp.shape before reshape: (150000, 1)\n",
      "create_X, temp.shape after reshape: (150, 1000)\n",
      "\n",
      "extract_features, z.shape: (150, 1000)\n",
      "createX, extract_features(temp).shape: (150, 6)\n",
      "\n",
      "extract_features, z.shape: (150, 100)\n",
      "createX, extract_features(temp[:, -step_length // 10:]).shape (150, 6)\n",
      "\n",
      "extract_features, z.shape: (150, 10)\n",
      "createX, extract_features(temp[:, -step_length // 100:]).shape (150, 6)\n",
      "\n",
      "\n",
      "extract_features, z.shape: (150, 1000)\n",
      "\n",
      "extract_features, z.shape: (150, 100)\n",
      "\n",
      "extract_features, z.shape: (150, 10)\n",
      "createX, result shape: (150, 18)\n",
      "\n",
      "Our RNN is based on 18 features\n"
     ]
    }
   ],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "#n_features = create_X(float_data[0:150000]).shape[1]\n",
    "n_features = create_X(X_train[0:150000].values).shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "#second_earthquake = 50085877\n",
    "#float_data[second_earthquake, 1]\n",
    "#X_train[second_earthquake, 1]\n",
    "\n",
    "# Initialize generators\n",
    "# train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "# train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "# valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)\n",
    "train_gen = generator(X_train.values, y_train.values, train_rows, batch_size=batch_size)\n",
    "valid_gen = generator(X_valid.values, y_valid.values, valid_rows, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
